import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Dict, List, Tuple, Optional

class NTLBGAttentionModule(nn.Module):
    """
    åŸºäºæ‚¨æœ¬ç§‘è®ºæ–‡çš„NTLBGç†è®ºçš„æ³¨æ„åŠ›æ¨¡å—
    æ ¸å¿ƒåˆ›æ–°ï¼šç»Ÿè®¡å­¦æŒ‡å¯¼çš„ä»£è¡¨ç‚¹é€‰æ‹©
    """
    
    def __init__(self, d_model: int, num_representatives: int = 6, temperature: float = 0.1):
        super().__init__()
        self.d_model = d_model
        self.num_representatives = num_representatives
        self.temperature = temperature
        
        # æŸ¥è¯¢æ¡ä»¶åˆ†å¸ƒå‚æ•°ä¼°è®¡å™¨
        self.query_encoder = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.GELU(),
            nn.Linear(d_model, d_model),
            nn.LayerNorm(d_model)
        )
        
        # ç»Ÿè®¡å‚æ•°é¢„æµ‹å™¨
        self.mu_predictor = nn.Linear(d_model, d_model)
        self.sigma_predictor = nn.Sequential(
            nn.Linear(d_model, d_model * d_model),
            nn.Tanh()  # ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
        )
        
        # ä»£è¡¨ç‚¹é‡è¦æ€§è¯„ä¼°å™¨
        self.importance_scorer = nn.Sequential(
            nn.Linear(d_model * 2, d_model),
            nn.GELU(),
            nn.Linear(d_model, 1),
            nn.Sigmoid()
        )
        
        # æ•°å€¼ç¨³å®šæ€§å‚æ•°
        self.eps = 1e-6
        
    def forward(self, video_features: torch.Tensor, query_embedding: torch.Tensor):
        """
        NTLBGä»£è¡¨ç‚¹é€‰æ‹©çš„æ ¸å¿ƒç®—æ³•
        Args:
            video_features: [B, T, D] è§†é¢‘å¸§ç‰¹å¾
            query_embedding: [B, D] æŸ¥è¯¢æ¡ä»¶ç¼–ç 
        Returns:
            dict: åŒ…å«ä»£è¡¨ç‚¹å’Œç»Ÿè®¡ä¿¡æ¯
        """
        B, T, D = video_features.shape
        
        # 1. æŸ¥è¯¢æ¡ä»¶ç¼–ç 
        q_encoded = self.query_encoder(query_embedding)  # [B, D]
        
        # 2. ä¼°è®¡æŸ¥è¯¢æ¡ä»¶ä¸‹çš„åˆ†å¸ƒå‚æ•°
        mu_q = self.mu_predictor(q_encoded)  # [B, D] æ¡ä»¶å‡å€¼
        sigma_flat = self.sigma_predictor(q_encoded)  # [B, D*D]
        
        # é‡æ„åæ–¹å·®çŸ©é˜µå¹¶ç¡®ä¿æ­£å®šæ€§
        sigma_q = sigma_flat.view(B, D, D)  # [B, D, D]
        sigma_q = torch.bmm(sigma_q, sigma_q.transpose(-1, -2))
        sigma_q = sigma_q + self.eps * torch.eye(D, device=video_features.device).unsqueeze(0)
        
        # 3. è®¡ç®—é©¬æ°è·ç¦»ï¼ˆNTLBGæ ¸å¿ƒï¼‰
        mahalanobis_distances = self._compute_mahalanobis_distance(
            video_features, mu_q, sigma_q
        )  # [B, T]
        
        # 4. NTLBGç­‰é«˜çº¿çº¦æŸé€‰æ‹©
        representative_indices, selection_weights = self._ntlbg_selection(
            video_features, q_encoded, mahalanobis_distances
        )
        
        # 5. æ„å»ºå¯Œä»£è¡¨ç‚¹ç‰¹å¾
        representative_features = self._construct_rich_representatives(
            video_features, representative_indices, selection_weights, q_encoded
        )
        
        # 6. è¿”å›å®Œæ•´ä¿¡æ¯ï¼ˆç”¨äºæŸå¤±è®¡ç®—å’Œåˆ†æï¼‰
        return {
            'representative_features': representative_features,
            'representative_indices': representative_indices,
            'selection_weights': selection_weights,
            'mahalanobis_distances': mahalanobis_distances,
            'mu_q': mu_q,
            'sigma_q': sigma_q,
            'query_encoded': q_encoded
        }
    
    def _compute_mahalanobis_distance(self, features, mu, sigma):
        """è®¡ç®—é©¬æ°è·ç¦»ï¼šæ ¸å¿ƒç»Ÿè®¡å­¦ç®—æ³•"""
        B, T, D = features.shape
        
        # ä¸­å¿ƒåŒ–
        centered_features = features - mu.unsqueeze(1)  # [B, T, D]
        
        # è®¡ç®—åæ–¹å·®çŸ©é˜µçš„é€†ï¼ˆæ•°å€¼ç¨³å®šç‰ˆæœ¬ï¼‰
        try:
            # ä½¿ç”¨Choleskyåˆ†è§£æé«˜æ•°å€¼ç¨³å®šæ€§
            L = torch.linalg.cholesky(sigma)  # [B, D, D]
            sigma_inv = torch.cholesky_inverse(L)  # [B, D, D]
        except:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šæ­£åˆ™åŒ–åæ±‚é€†
            regularized_sigma = sigma + 1e-4 * torch.eye(D, device=sigma.device).unsqueeze(0)
            sigma_inv = torch.linalg.inv(regularized_sigma)
        
        # é©¬æ°è·ç¦»è®¡ç®—ï¼šdÂ² = (x-Î¼)áµ€ Î£â»Â¹ (x-Î¼)
        distances = torch.einsum('btd,bde,bte->bt', centered_features, sigma_inv, centered_features)
        
        return torch.clamp(distances, min=0)  # ç¡®ä¿éè´Ÿ
    
    def _ntlbg_selection(self, video_features, query_encoded, distances):
        """
        åŸºäºNTLBGç†è®ºçš„ä»£è¡¨ç‚¹é€‰æ‹©
        æ ¸å¿ƒæ€æƒ³ï¼šé€‰æ‹©åœ¨åŒä¸€ç­‰é«˜æ¤­çƒé¢ä¸Šçš„ä»£è¡¨ç‚¹
        """
        B, T, D = video_features.shape
        K = min(self.num_representatives, T)
        
        if self.training:
            # è®­ç»ƒæ—¶ï¼šè½¯é€‰æ‹©ï¼ˆå¯å¾®åˆ†ï¼‰
            return self._soft_ntlbg_selection(video_features, query_encoded, distances, K)
        else:
            # æ¨ç†æ—¶ï¼šç¡¬é€‰æ‹©ï¼ˆæ›´ç²¾ç¡®ï¼‰
            return self._hard_ntlbg_selection(distances, K)
    
    def _soft_ntlbg_selection(self, video_features, query_encoded, distances, K):
        """è½¯é€‰æ‹©ï¼šåŸºäºGumbel-Softmaxçš„å¯å¾®åˆ†é€‰æ‹©"""
        B, T, D = video_features.shape
        
        # è®¡ç®—æ¯å¸§çš„é‡è¦æ€§åˆ†æ•°
        query_expanded = query_encoded.unsqueeze(1).expand(-1, T, -1)  # [B, T, D]
        combined_features = torch.cat([video_features, query_expanded], dim=-1)  # [B, T, 2D]
        
        importance_scores = self.importance_scorer(combined_features).squeeze(-1)  # [B, T]
        
        # ç»“åˆé©¬æ°è·ç¦»è°ƒæ•´åˆ†æ•°ï¼ˆè·ç¦»å°=é‡è¦æ€§é«˜ï¼‰
        distance_scores = torch.exp(-distances / self.temperature)
        final_scores = importance_scores + distance_scores
        
        # Gumbel-Softmaxé€‰æ‹©
        if self.training:
            # æ·»åŠ Gumbelå™ªå£°
            gumbel_noise = -torch.log(-torch.log(torch.rand_like(final_scores) + 1e-8) + 1e-8)
            noisy_scores = (final_scores + gumbel_noise) / self.temperature
        else:
            noisy_scores = final_scores / self.temperature
        
        # é€‰æ‹©top-K
        softmax_scores = F.softmax(noisy_scores, dim=1)
        top_weights, top_indices = torch.topk(softmax_scores, K, dim=1)
        
        # é‡æ–°å½’ä¸€åŒ–
        top_weights = top_weights / (top_weights.sum(dim=1, keepdim=True) + self.eps)
        
        return top_indices, top_weights
    
    def _hard_ntlbg_selection(self, distances, K):
        """ç¡¬é€‰æ‹©ï¼šåŸºäºç­‰é«˜çº¿çº¦æŸçš„ç¡®å®šæ€§é€‰æ‹©"""
        B, T = distances.shape
        
        # å¯»æ‰¾æœ€ä¼˜ç­‰é«˜çº¿å€¼
        # ç­–ç•¥ï¼šé€‰æ‹©ä½¿ä»£è¡¨ç‚¹æœ€å‡åŒ€åˆ†å¸ƒçš„ç­‰é«˜çº¿å€¼
        median_distance = torch.median(distances, dim=1, keepdim=True)[0]  # [B, 1]
        
        # è®¡ç®—æ¯ä¸ªç‚¹åˆ°ç›®æ ‡ç­‰é«˜çº¿çš„è·ç¦»
        contour_deviations = torch.abs(distances - median_distance)  # [B, T]
        
        # é€‰æ‹©æœ€æ¥è¿‘ç›®æ ‡ç­‰é«˜çº¿çš„Kä¸ªç‚¹
        _, selected_indices = torch.topk(contour_deviations, K, dim=1, largest=False)  # [B, K]
        
        # ç­‰æƒé‡ï¼ˆç¡¬é€‰æ‹©ï¼‰
        equal_weights = torch.ones(B, K, device=distances.device) / K
        
        return selected_indices, equal_weights
    
    def _construct_rich_representatives(self, video_features, indices, weights, query_encoded):
        """æ„å»ºå¯Œä»£è¡¨ç‚¹ï¼šä¸ä»…åŒ…å«è§†è§‰ç‰¹å¾ï¼Œè¿˜åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        B, T, D = video_features.shape
        K = indices.shape[1]
        
        # æ”¶é›†åŸºç¡€ä»£è¡¨ç‚¹ç‰¹å¾
        batch_indices = torch.arange(B, device=video_features.device).unsqueeze(1).expand(-1, K)
        base_features = video_features[batch_indices, indices]  # [B, K, D]
        
        # æ·»åŠ æƒé‡ä¿¡æ¯
        weighted_features = base_features * weights.unsqueeze(-1)  # [B, K, D]
        
        # æ·»åŠ æŸ¥è¯¢ç›¸å…³æ€§
        query_similarity = torch.bmm(
            base_features, query_encoded.unsqueeze(-1)
        ).squeeze(-1).unsqueeze(-1)  # [B, K, 1]
        
        # èåˆå¤šç»´ä¿¡æ¯
        rich_features = torch.cat([
            weighted_features,
            query_similarity.expand(-1, -1, D)
        ], dim=-1)  # [B, K, 2D]
        
        # æŠ•å½±å›åŸå§‹ç»´åº¦
        fusion_layer = nn.Linear(2 * D, D, device=video_features.device)
        rich_representatives = fusion_layer(rich_features)  # [B, K, D]
        
        return rich_representatives

class PaperNTLBGLLM(nn.Module):
    """
    è®ºæ–‡ç‰ˆæœ¬çš„å®Œæ•´NTLBG-LLMæ¨¡å‹
    é›†æˆç»Ÿè®¡å­¦ç†è®ºä¸å¤§è¯­è¨€æ¨¡å‹
    """
    
    def __init__(self, config: Dict):
        super().__init__()
        self.config = config
        
        # æ¨¡å‹ç»´åº¦
        self.d_model = config.get('d_model', 768)
        self.vocab_size = config.get('vocab_size', 50000)
        
        # è§†è§‰ç¼–ç å™¨
        self.video_projector = nn.Sequential(
            nn.Linear(config.get('video_feature_dim', 768), self.d_model),
            nn.LayerNorm(self.d_model),
            nn.GELU()
        )
        
        # NTLBGæ³¨æ„åŠ›æ¨¡å—ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
        self.ntlbg_attention = NTLBGAttentionModule(
            d_model=self.d_model,
            num_representatives=config.get('num_representatives', 6),
            temperature=config.get('temperature', 0.1)
        )
        
        # æ–‡æœ¬å¤„ç†
        self.text_embedding = nn.Embedding(self.vocab_size, self.d_model)
        self.positional_encoding = nn.Parameter(
            torch.randn(config.get('max_text_length', 512), self.d_model) * 0.02
        )
        
        # å¤šæ¨¡æ€èåˆ
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=self.d_model,
            num_heads=config.get('num_heads', 12),
            batch_first=True,
            dropout=0.1
        )
        
        # è¾“å‡ºå±‚
        self.output_norm = nn.LayerNorm(self.d_model)
        self.output_projection = nn.Linear(self.d_model, self.vocab_size)
        
        # Dropout
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, video_features, input_ids, attention_mask, labels=None):
        """å®Œæ•´çš„å‰å‘ä¼ æ’­"""
        B = video_features.size(0)
        
        # 1. è§†é¢‘ç‰¹å¾å¤„ç†
        video_projected = self.video_projector(video_features)  # [B, T, D]
        
        # 2. æ–‡æœ¬å¤„ç†
        text_embeddings = self.text_embedding(input_ids)  # [B, L, D]
        seq_length = text_embeddings.size(1)
        text_embeddings = text_embeddings + self.positional_encoding[:seq_length]
        
        # ç”ŸæˆæŸ¥è¯¢è¡¨ç¤º
        query_embedding = torch.mean(text_embeddings, dim=1)  # [B, D]
        
        # 3. NTLBGä»£è¡¨ç‚¹é€‰æ‹©ï¼ˆæ ¸å¿ƒç®—æ³•ï¼‰
        ntlbg_output = self.ntlbg_attention(video_projected, query_embedding)
        representative_features = ntlbg_output['representative_features']  # [B, K, D]
        
        # 4. å¤šæ¨¡æ€èåˆ
        fused_features, cross_attention_weights = self.cross_attention(
            query=text_embeddings,
            key=representative_features,
            value=representative_features
        )  # [B, L, D]
        
        # 5. è¾“å‡ºç”Ÿæˆ
        fused_features = self.dropout(fused_features)
        output_features = self.output_norm(fused_features)
        logits = self.output_projection(output_features)  # [B, L, vocab_size]
        
        # 6. æ„å»ºè¾“å‡º
        outputs = {
            'logits': logits,
            'representative_features': representative_features,
            'representative_indices': ntlbg_output['representative_indices'],
            'selection_weights': ntlbg_output['selection_weights'],
            'cross_attention_weights': cross_attention_weights,
            'ntlbg_stats': {
                'mahalanobis_distances': ntlbg_output['mahalanobis_distances'],
                'mu_q': ntlbg_output['mu_q'],
                'sigma_q': ntlbg_output['sigma_q']
            }
        }
        
        # 7. è®¡ç®—æŸå¤±ï¼ˆå¦‚æœæœ‰æ ‡ç­¾ï¼‰
        if labels is not None:
            # ä¸»ä»»åŠ¡æŸå¤±
            task_loss = F.cross_entropy(
                logits.view(-1, self.vocab_size),
                labels.view(-1),
                ignore_index=-100
            )
            
            # NTLBGç»Ÿè®¡çº¦æŸæŸå¤±
            ntlbg_loss = self._compute_ntlbg_constraint_loss(ntlbg_output)
            
            # ä»£è¡¨ç‚¹åˆ†å¸ƒå‡åŒ€æ€§æŸå¤±
            diversity_loss = self._compute_diversity_loss(representative_features)
            
            # æ€»æŸå¤±
            total_loss = (
                task_loss + 
                0.1 * ntlbg_loss + 
                0.05 * diversity_loss
            )
            
            outputs.update({
                'loss': total_loss,
                'task_loss': task_loss,
                'ntlbg_loss': ntlbg_loss,
                'diversity_loss': diversity_loss
            })
        
        return outputs
    
    def _compute_ntlbg_constraint_loss(self, ntlbg_output):
        """NTLBGç­‰é«˜çº¿çº¦æŸæŸå¤±"""
        distances = ntlbg_output['mahalanobis_distances']  # [B, T]
        indices = ntlbg_output['representative_indices']    # [B, K]
        
        # è·å–ä»£è¡¨ç‚¹çš„é©¬æ°è·ç¦»
        B, K = indices.shape
        batch_indices = torch.arange(B, device=distances.device).unsqueeze(1).expand(-1, K)
        representative_distances = distances[batch_indices, indices]  # [B, K]
        
        # ç­‰é«˜çº¿çº¦æŸï¼šæ‰€æœ‰ä»£è¡¨ç‚¹åº”æœ‰ç›¸åŒçš„é©¬æ°è·ç¦»
        target_distance = torch.median(representative_distances, dim=1, keepdim=True)[0]
        constraint_loss = torch.mean((representative_distances - target_distance) ** 2)
        
        return constraint_loss
    
    def _compute_diversity_loss(self, representative_features):
        """ä»£è¡¨ç‚¹å¤šæ ·æ€§æŸå¤±"""
        B, K, D = representative_features.shape
        
        # è®¡ç®—ä»£è¡¨ç‚¹é—´çš„ç›¸ä¼¼æ€§
        normalized_features = F.normalize(representative_features, dim=-1)  # [B, K, D]
        similarity_matrix = torch.bmm(
            normalized_features, normalized_features.transpose(-1, -2)
        )  # [B, K, K]
        
        # é™¤äº†å¯¹è§’çº¿ï¼Œå…¶ä»–ç›¸ä¼¼æ€§åº”è¯¥è¾ƒå°
        mask = torch.eye(K, device=similarity_matrix.device).unsqueeze(0).expand(B, -1, -1)
        off_diagonal_similarity = similarity_matrix * (1 - mask)
        
        # å¤šæ ·æ€§æŸå¤±ï¼šå‡å°‘ä»£è¡¨ç‚¹é—´çš„ç›¸ä¼¼æ€§
        diversity_loss = torch.mean(torch.sum(off_diagonal_similarity ** 2, dim=[1, 2]))
        
        return diversity_loss

def create_paper_model(config):
    """åˆ›å»ºè®ºæ–‡ç‰ˆæœ¬çš„NTLBG-LLMæ¨¡å‹"""
    return PaperNTLBGLLM(config)

if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å‹
    config = {
        'd_model': 768,
        'video_feature_dim': 768,
        'num_representatives': 6,
        'temperature': 0.1,
        'num_heads': 12,
        'vocab_size': 50000,
        'max_text_length': 512
    }
    
    model = create_paper_model(config)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 2
    video_features = torch.randn(batch_size, 100, 768)  # 100å¸§è§†é¢‘
    input_ids = torch.randint(1, 1000, (batch_size, 128))  # 128é•¿åº¦æ–‡æœ¬
    attention_mask = torch.ones(batch_size, 128)
    labels = torch.randint(1, 1000, (batch_size, 128))
    
    with torch.no_grad():
        outputs = model(video_features, input_ids, attention_mask, labels)
        
    print("âœ… è®ºæ–‡æ¨¡å‹æµ‹è¯•æˆåŠŸ!")
    print(f"ğŸ“Š ä»£è¡¨ç‚¹æ•°é‡: {outputs['representative_indices'].shape[1]}")
    print(f"ğŸ“Š æ€»æŸå¤±: {outputs['loss'].item():.4f}")
    print(f"ğŸ“Š NTLBGæŸå¤±: {outputs['ntlbg_loss'].item():.4f}")
