"""
çœŸæ­£çš„NTLBG-LLMå®ç°ï¼ŒåŸºäºå¤§æ¨¡å‹æ¶æ„
"""
import torch
import torch.nn as nn
from transformers import (
    AutoModel, AutoTokenizer, 
    CLIPVisionModel, CLIPImageProcessor,
    AutoModelForCausalLM
)
from .ntlbg_core import NTLBGAttention
import logging

logger = logging.getLogger(__name__)

class RealNTLBGLLM(nn.Module):
    """çœŸæ­£çš„NTLBG-LLMï¼šé›†æˆç»Ÿè®¡ç†è®ºçš„é•¿è§†é¢‘ç†è§£æ¨¡å‹"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # åŸºç¡€è¯­è¨€æ¨¡å‹é€‰æ‹©
        self.base_model_name = config.get('base_model_name', 'microsoft/DialoGPT-medium')
        
        # è§†è§‰ç¼–ç å™¨
        self.vision_encoder = CLIPVisionModel.from_pretrained(
            'openai/clip-vit-large-patch14'
        )
        self.vision_processor = CLIPImageProcessor.from_pretrained(
            'openai/clip-vit-large-patch14'
        )
        
        # è¯­è¨€æ¨¡å‹
        try:
            self.language_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True
            )
            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
        except Exception as e:
            logger.warning(f"åŠ è½½{self.base_model_name}å¤±è´¥ï¼Œä½¿ç”¨å¤‡é€‰æ¨¡å‹: {e}")
            # ä½¿ç”¨æ›´ç®€å•çš„æ¨¡å‹ä½œä¸ºå¤‡é€‰
            self.language_model = AutoModel.from_pretrained('microsoft/DialoGPT-medium')
            self.tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-medium')
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # è·å–æ¨¡å‹ç»´åº¦
        vision_dim = self.vision_encoder.config.hidden_size  # 1024
        try:
            lang_dim = self.language_model.config.hidden_size
        except:
            lang_dim = self.language_model.config.n_embd if hasattr(self.language_model.config, 'n_embd') else 768
        
        # **NTLBGæ ¸å¿ƒæ¨¡å—** - è¿™æ˜¯å…³é”®ï¼
        self.ntlbg_attention = NTLBGAttention(
            d_model=vision_dim,
            d_query=lang_dim,
            num_representatives=config.get('num_representatives', 6)
        )
        
        # æ¨¡æ€å¯¹é½å±‚
        self.vision_projection = nn.Linear(vision_dim, lang_dim)
        self.temporal_encoding = nn.Parameter(torch.randn(1000, lang_dim))  # æ”¯æŒ1000å¸§
        
        # å¤šæ¨¡æ€èåˆ
        self.multimodal_fusion = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=lang_dim,
                nhead=8,
                dim_feedforward=lang_dim * 4,
                batch_first=True
            ),
            num_layers=2
        )
        
        # ä»»åŠ¡å¤´
        try:
            vocab_size = self.language_model.config.vocab_size
        except:
            vocab_size = len(self.tokenizer)
            
        self.task_head = nn.Sequential(
            nn.Linear(lang_dim, lang_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(lang_dim, vocab_size)
        )
        
        # å†»ç»“éƒ¨åˆ†å‚æ•°ä»¥æé«˜è®­ç»ƒæ•ˆç‡
        self._freeze_base_models()
        
        logger.info(f"âœ… çœŸæ­£çš„NTLBG-LLMåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   è§†è§‰ç¼–ç å™¨: {vision_dim}D")
        logger.info(f"   è¯­è¨€æ¨¡å‹: {lang_dim}D")
        logger.info(f"   NTLBGä»£è¡¨ç‚¹: {config.get('num_representatives', 6)}ä¸ª")
    
    def _freeze_base_models(self):
        """å†»ç»“åŸºç¡€æ¨¡å‹çš„éƒ¨åˆ†å‚æ•°"""
        # å†»ç»“è§†è§‰ç¼–ç å™¨ï¼ˆé™¤äº†æœ€åä¸€å±‚ï¼‰
        for name, param in self.vision_encoder.named_parameters():
            if 'layer.23' not in name:  # åªè®­ç»ƒæœ€åä¸€å±‚
                param.requires_grad = False
        
        # å†»ç»“è¯­è¨€æ¨¡å‹çš„å¤§éƒ¨åˆ†å‚æ•°
        for name, param in self.language_model.named_parameters():
            # åªè®­ç»ƒæœ€åå‡ å±‚
            if not any(layer in name for layer in ['layer.23', 'layer.24', 'layer.25', 'lm_head', 'h.23', 'h.24']):
                param.requires_grad = False
        
        frozen_params = sum(p.numel() for p in self.parameters() if not p.requires_grad)
        total_params = sum(p.numel() for p in self.parameters())
        
        logger.info(f"ğŸ§Š å†»ç»“å‚æ•°æ¯”ä¾‹: {frozen_params/total_params:.1%}")
    
    def encode_video_frames(self, video_frames):
        """ç¼–ç è§†é¢‘å¸§ä¸ºç‰¹å¾åºåˆ—"""
        if not video_frames or len(video_frames) == 0:
            # è¿”å›ç©ºç‰¹å¾
            device = next(self.parameters()).device
            return torch.zeros(1, 1, self.vision_encoder.config.hidden_size, device=device)
        
        try:
            # é¢„å¤„ç†å›¾åƒ
            if hasattr(video_frames[0], 'size'):  # PIL Images
                inputs = self.vision_processor(video_frames, return_tensors="pt")
            else:  # å·²ç»æ˜¯tensor
                inputs = {'pixel_values': torch.stack(video_frames)}
            
            device = next(self.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ç¼–ç è§†é¢‘å¸§
            with torch.no_grad():
                vision_outputs = self.vision_encoder(**inputs)
            
            # è·å–ç‰¹å¾å¹¶æ·»åŠ æ—¶åºç¼–ç 
            frame_features = vision_outputs.last_hidden_state  # [T, seq_len, hidden_size]
            
            # ç®€åŒ–ï¼šå–CLS tokenæˆ–å¹³å‡æ± åŒ–
            if frame_features.dim() == 3:
                frame_features = frame_features.mean(dim=1)  # [T, hidden_size]
            
            # æ·»åŠ æ—¶åºä½ç½®ç¼–ç 
            T = frame_features.shape[0]
            temporal_pos = self.temporal_encoding[:T].to(device)  # [T, hidden_size]
            frame_features = frame_features + temporal_pos
            
            return frame_features.unsqueeze(0)  # [1, T, hidden_size]
            
        except Exception as e:
            logger.warning(f"è§†é¢‘ç¼–ç å¤±è´¥: {e}")
            device = next(self.parameters()).device
            return torch.zeros(1, 8, self.vision_encoder.config.hidden_size, device=device)
    
    def encode_text(self, text_input):
        """ç¼–ç æ–‡æœ¬è¾“å…¥"""
        if not text_input:
            device = next(self.parameters()).device
            try:
                hidden_size = self.language_model.config.hidden_size
            except:
                hidden_size = self.language_model.config.n_embd if hasattr(self.language_model.config, 'n_embd') else 768
            return torch.zeros(1, hidden_size, device=device)
        
        try:
            # æ ‡è®°åŒ–æ–‡æœ¬
            tokens = self.tokenizer(
                text_input,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            device = next(self.parameters()).device
            tokens = {k: v.to(device) for k, v in tokens.items()}
            
            # ç¼–ç æ–‡æœ¬
            with torch.no_grad():
                if hasattr(self.language_model, 'forward'):
                    outputs = self.language_model(**tokens, output_hidden_states=True)
                    if hasattr(outputs, 'hidden_states'):
                        text_features = outputs.hidden_states[-1].mean(dim=1)  # [1, hidden_size]
                    else:
                        text_features = outputs.last_hidden_state.mean(dim=1)
                else:
                    # ç®€åŒ–å¤„ç†
                    embeddings = self.language_model.get_input_embeddings()
                    text_features = embeddings(tokens['input_ids']).mean(dim=1)
            
            return text_features  # [1, hidden_size]
            
        except Exception as e:
            logger.warning(f"æ–‡æœ¬ç¼–ç å¤±è´¥: {e}")
            device = next(self.parameters()).device
            try:
                hidden_size = self.language_model.config.hidden_size
            except:
                hidden_size = 768
            return torch.zeros(1, hidden_size, device=device)
    
    def forward(self, video_frames=None, text_input=None, labels=None):
        """NTLBG-LLMå‰å‘ä¼ æ’­"""
        device = next(self.parameters()).device
        
        # 1. ç¼–ç è§†é¢‘å’Œæ–‡æœ¬
        video_features = self.encode_video_frames(video_frames)  # [1, T, vision_dim]
        text_features = self.encode_text(text_input)  # [1, lang_dim]
        
        # 2. æŠ•å½±è§†è§‰ç‰¹å¾åˆ°è¯­è¨€ç©ºé—´
        video_features_proj = self.vision_projection(video_features)  # [1, T, lang_dim]
        
        # 3. **NTLBGæ ¸å¿ƒå¤„ç†** - è¿™æ˜¯æˆ‘ä»¬çš„åˆ›æ–°ï¼
        ntlbg_results = self.ntlbg_attention(
            video_features=video_features_proj,
            query_embedding=text_features
        )
        
        # 4. è·å–NTLBGé€‰æ‹©çš„ä»£è¡¨ç‚¹ç‰¹å¾
        representative_features = ntlbg_results['representative_features']  # [1, K, lang_dim]
        attended_features = ntlbg_results['attended_features']  # [1, 1, lang_dim]
        
        # 5. å¤šæ¨¡æ€èåˆ
        # åˆå¹¶æ–‡æœ¬ã€ä»£è¡¨ç‚¹å’Œæ³¨æ„åŠ›ç‰¹å¾
        fused_input = torch.cat([
            attended_features,  # æŸ¥è¯¢-è§†é¢‘æ³¨æ„åŠ›ç‰¹å¾
            representative_features,  # NTLBGä»£è¡¨ç‚¹ç‰¹å¾
            text_features.unsqueeze(1)  # åŸå§‹æ–‡æœ¬ç‰¹å¾
        ], dim=1)  # [1, 1+K+1, lang_dim]
        
        fused_features = self.multimodal_fusion(fused_input)  # [1, 1+K+1, lang_dim]
        
        # 6. ä»»åŠ¡é¢„æµ‹
        # ä½¿ç”¨èåˆç‰¹å¾çš„å‡å€¼è¿›è¡Œé¢„æµ‹
        final_features = fused_features.mean(dim=1)  # [1, lang_dim]
        logits = self.task_head(final_features)  # [1, vocab_size]
        
        # 7. æ„å»ºè¾“å‡º
        outputs = {
            'logits': logits,
            'representative_features': representative_features,
            'representative_indices': ntlbg_results['representative_indices'],
            'mahalanobis_distances': ntlbg_results['mahalanobis_distances'],
            'attention_weights': ntlbg_results.get('attention_weights'),
            'ntlbg_constraint_loss': None
        }
        
        # 8. è®¡ç®—æŸå¤±
        if labels is not None:
            # ä¸»ä»»åŠ¡æŸå¤±
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            
            if labels.dim() == 1:
                # å•æ ‡ç­¾åˆ†ç±»
                task_loss = loss_fct(logits, labels)
            else:
                # åºåˆ—ç”Ÿæˆ
                task_loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))
            
            # NTLBGçº¦æŸæŸå¤±
            ntlbg_constraint_loss = self.ntlbg_attention.ntlbg_core.compute_ntlbg_constraint_loss(
                representative_features,
                ntlbg_results['mu_q'],
                ntlbg_results['sigma_q']
            )
            
            # æ€»æŸå¤±
            total_loss = task_loss + 0.5 * ntlbg_constraint_loss
            
            outputs.update({
                'loss': total_loss,
                'task_loss': task_loss,
                'ntlbg_constraint_loss': ntlbg_constraint_loss
            })
        
        return outputs


def create_real_ntlbg_llm(config):
    """åˆ›å»ºçœŸæ­£çš„NTLBG-LLMæ¨¡å‹"""
    try:
        model = RealNTLBGLLM(config)
        
        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        logger.info(f"ğŸ“Š NTLBG-LLMç»Ÿè®¡:")
        logger.info(f"   æ€»å‚æ•°: {total_params:,}")
        logger.info(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        logger.info(f"   è®­ç»ƒæ•ˆç‡: {trainable_params/total_params:.1%}")
        
        return model
        
    except Exception as e:
        logger.error(f"âŒ NTLBG-LLMåˆ›å»ºå¤±è´¥: {e}")
        raise
