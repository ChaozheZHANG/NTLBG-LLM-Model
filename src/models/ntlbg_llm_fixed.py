"""
ä¿®å¤ç‰ˆNTLBG-LLMä¸»æ¨¡å‹
"""
import torch
import torch.nn as nn
from transformers import (
    AutoModel, AutoTokenizer, 
    CLIPVisionModel, CLIPImageProcessor,
    AutoModelForCausalLM
)
from .ntlbg_core_fixed import FixedNTLBGAttention
import logging

logger = logging.getLogger(__name__)

class FixedNTLBGLLM(nn.Module):
    """ä¿®å¤ç‰ˆNTLBG-LLM"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # åŸºç¡€æ¨¡å‹é…ç½®
        self.base_model_name = config.get('base_model_name', 'microsoft/DialoGPT-medium')
        
        # è§†è§‰ç¼–ç å™¨
        self.vision_encoder = CLIPVisionModel.from_pretrained('openai/clip-vit-large-patch14')
        self.vision_processor = CLIPImageProcessor.from_pretrained('openai/clip-vit-large-patch14')
        
        # è¯­è¨€æ¨¡å‹
        self.language_model = AutoModel.from_pretrained(self.base_model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # è·å–ç»´åº¦
        vision_dim = self.vision_encoder.config.hidden_size  # 1024
        lang_dim = self.language_model.config.hidden_size    # 1024
        
        # **ä¿®å¤ç‰ˆNTLBGæ ¸å¿ƒ**
        self.ntlbg_attention = FixedNTLBGAttention(
            d_model=vision_dim,
            d_query=lang_dim,
            num_representatives=config.get('num_representatives', 6)
        )
        
        # æ¨¡æ€å¯¹é½
        self.vision_projection = nn.Sequential(
            nn.Linear(vision_dim, lang_dim),
            nn.LayerNorm(lang_dim),
            nn.GELU()
        )
        
        # å¤šæ¨¡æ€èåˆ
        self.multimodal_fusion = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=lang_dim,
                nhead=8,
                dim_feedforward=lang_dim * 4,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=2
        )
        
        # è¾“å‡ºå±‚
        vocab_size = len(self.tokenizer)
        self.output_projection = nn.Sequential(
            nn.Linear(lang_dim, lang_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(lang_dim, vocab_size)
        )
        
        # åˆ†ç±»å¤´ï¼ˆç”¨äºå¤šé€‰é¢˜ï¼‰
        self.classification_head = nn.Sequential(
            nn.Linear(lang_dim, lang_dim // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(lang_dim // 2, 4)  # 4é€‰æ‹©é¢˜
        )
        
        # å†»ç»“å¤§éƒ¨åˆ†å‚æ•°
        self._freeze_base_models()
        
        logger.info(f"âœ… ä¿®å¤ç‰ˆNTLBG-LLMåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   è§†è§‰ç¼–ç å™¨: {vision_dim}D -> {lang_dim}D")
        logger.info(f"   NTLBGä»£è¡¨ç‚¹: {config.get('num_representatives', 6)}ä¸ª")
    
    def _freeze_base_models(self):
        """æ™ºèƒ½å†»ç»“ç­–ç•¥"""
        # å†»ç»“è§†è§‰ç¼–ç å™¨çš„å‰é¢å‡ å±‚
        for name, param in self.vision_encoder.named_parameters():
            if not any(layer in name for layer in ['layer.22', 'layer.23', 'pooler']):
                param.requires_grad = False
        
        # å†»ç»“è¯­è¨€æ¨¡å‹çš„embeddingå’Œå‰é¢å‡ å±‚
        for name, param in self.language_model.named_parameters():
            if not any(layer in name for layer in ['layer.22', 'layer.23', 'pooler']):
                param.requires_grad = False
        
        frozen = sum(p.numel() for p in self.parameters() if not p.requires_grad)
        total = sum(p.numel() for p in self.parameters())
        logger.info(f"ğŸ§Š å†»ç»“å‚æ•°: {frozen/total:.1%}")
    
    def encode_video_frames(self, video_frames):
        """æ”¹è¿›çš„è§†é¢‘ç¼–ç """
        if not video_frames or len(video_frames) == 0:
            device = next(self.parameters()).device
            return torch.zeros(1, 1, self.vision_encoder.config.hidden_size, device=device)
        
        try:
            # é™åˆ¶å¸§æ•°ä»¥é¿å…å†…å­˜é—®é¢˜
            max_frames = 64
            if len(video_frames) > max_frames:
                # å‡åŒ€é‡‡æ ·
                indices = torch.linspace(0, len(video_frames)-1, max_frames, dtype=torch.long)
                video_frames = [video_frames[i] for i in indices]
            
            # é¢„å¤„ç†
            if hasattr(video_frames[0], 'size'):  # PIL Images
                inputs = self.vision_processor(video_frames, return_tensors="pt")
            else:
                inputs = {'pixel_values': torch.stack(video_frames)}
            
            device = next(self.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ç¼–ç ï¼ˆå…è®¸æ¢¯åº¦ä¼ æ’­ï¼‰
            vision_outputs = self.vision_encoder(**inputs)
            
            # è·å–ç‰¹å¾
            if hasattr(vision_outputs, 'pooler_output'):
                frame_features = vision_outputs.pooler_output  # [T, hidden_size]
            else:
                frame_features = vision_outputs.last_hidden_state.mean(dim=1)
            
            return frame_features.unsqueeze(0)  # [1, T, hidden_size]
            
        except Exception as e:
            logger.warning(f"è§†é¢‘ç¼–ç å¤±è´¥: {e}")
            device = next(self.parameters()).device
            return torch.randn(1, 8, self.vision_encoder.config.hidden_size, device=device)
    
    def encode_text(self, text_input):
        """æ”¹è¿›çš„æ–‡æœ¬ç¼–ç """
        if not text_input:
            device = next(self.parameters()).device
            return torch.zeros(1, self.language_model.config.hidden_size, device=device)
        
        try:
            tokens = self.tokenizer(
                text_input,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=256  # å‡å°‘é•¿åº¦
            )
            
            device = next(self.parameters()).device
            tokens = {k: v.to(device) for k, v in tokens.items()}
            
            # ç¼–ç ï¼ˆå…è®¸æ¢¯åº¦ä¼ æ’­ï¼‰
            outputs = self.language_model(**tokens)
            
            # è·å–ç‰¹å¾
            if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:
                text_features = outputs.pooler_output
            else:
                text_features = outputs.last_hidden_state.mean(dim=1)
            
            return text_features  # [1, hidden_size]
            
        except Exception as e:
            logger.warning(f"æ–‡æœ¬ç¼–ç å¤±è´¥: {e}")
            device = next(self.parameters()).device
            return torch.randn(1, self.language_model.config.hidden_size, device=device)
    
    def forward(self, video_frames=None, text_input=None, labels=None, return_loss=True):
        """ä¿®å¤ç‰ˆå‰å‘ä¼ æ’­"""
        device = next(self.parameters()).device
        
        # 1. ç¼–ç è¾“å…¥
        video_features = self.encode_video_frames(video_frames)  # [1, T, vision_dim]
        text_features = self.encode_text(text_input)  # [1, lang_dim]
        
        # 2. è§†è§‰ç‰¹å¾æŠ•å½±
        video_features_proj = self.vision_projection(video_features)  # [1, T, lang_dim]
        
        # 3. NTLBGå¤„ç†
        ntlbg_results = self.ntlbg_attention(
            video_features=video_features_proj,
            query_embedding=text_features
        )
        
        # 4. å¤šæ¨¡æ€èåˆ
        representative_features = ntlbg_results['representative_features']  # [1, K, lang_dim]
        attended_features = ntlbg_results['attended_features']  # [1, 1, lang_dim]
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        all_features = torch.cat([
            text_features.unsqueeze(1),  # åŸå§‹æ–‡æœ¬
            attended_features,           # æ³¨æ„åŠ›ç‰¹å¾
            representative_features      # ä»£è¡¨ç‚¹ç‰¹å¾
        ], dim=1)  # [1, 1+1+K, lang_dim]
        
        # å¤šæ¨¡æ€èåˆ
        fused_features = self.multimodal_fusion(all_features)  # [1, 1+1+K, lang_dim]
        
        # 5. è¾“å‡ºé¢„æµ‹
        pooled_features = fused_features.mean(dim=1)  # [1, lang_dim]
        
        # ç”Ÿæˆå¼è¾“å‡º
        generation_logits = self.output_projection(pooled_features)  # [1, vocab_size]
        
        # åˆ†ç±»è¾“å‡ºï¼ˆç”¨äºå¤šé€‰é¢˜ï¼‰
        classification_logits = self.classification_head(pooled_features)  # [1, 4]
        
        outputs = {
            'logits': generation_logits,
            'classification_logits': classification_logits,
            'representative_features': representative_features,
            'representative_indices': ntlbg_results['representative_indices'],
            'mahalanobis_distances': ntlbg_results['mahalanobis_distances'],
            'attention_weights': ntlbg_results.get('cross_attention_weights')
        }
        
        # 6. è®¡ç®—æŸå¤±
        if return_loss and labels is not None:
            # å¤„ç†ä¸åŒç±»å‹çš„æ ‡ç­¾
            if isinstance(labels, torch.Tensor) and labels.numel() == 1:
                # å•ä¸ªæ ‡ç­¾ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
                if labels.item() < 4:  # 4é€‰æ‹©é¢˜
                    loss = nn.CrossEntropyLoss()(classification_logits, labels.view(-1))
                else:  # ç”Ÿæˆä»»åŠ¡
                    loss = nn.CrossEntropyLoss()(generation_logits, labels.view(-1))
            else:
                # åºåˆ—æ ‡ç­¾æˆ–å…¶ä»–æ ¼å¼
                loss = nn.CrossEntropyLoss()(generation_logits, labels.view(-1))
            
            # æ·»åŠ NTLBGçº¦æŸæŸå¤±
            ntlbg_loss = self.ntlbg_attention.ntlbg_core.compute_ntlbg_constraint_loss(
                representative_features,
                ntlbg_results['mu_q'],
                ntlbg_results['sigma_q']
            )
            
            # æ€»æŸå¤±
            total_loss = loss + 0.1 * ntlbg_loss  # è°ƒæ•´æƒé‡
            
            outputs.update({
                'loss': total_loss,
                'task_loss': loss,
                'ntlbg_loss': ntlbg_loss
            })
        
        return outputs


def create_fixed_ntlbg_llm(config):
    """åˆ›å»ºä¿®å¤ç‰ˆNTLBG-LLM"""
    model = FixedNTLBGLLM(config)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    logger.info(f"ğŸ“Š ä¿®å¤ç‰ˆNTLBG-LLM:")
    logger.info(f"   æ€»å‚æ•°: {total_params:,}")
    logger.info(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    logger.info(f"   è®­ç»ƒæ•ˆç‡: {trainable_params/total_params:.1%}")
    
    return model
