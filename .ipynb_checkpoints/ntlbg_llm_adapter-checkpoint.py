"""
NTLBG-LLM: åŸºäºç»Ÿè®¡ç†è®ºçš„é•¿è§†é¢‘ç†è§£æ¨¡å‹
å°†NTLBGç®—æ³•é›†æˆåˆ°ç°æœ‰å¤§æ¨¡å‹ä¸­
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from transformers import (
    AutoModel, AutoTokenizer, AutoProcessor,
    AutoModel, AutoProcessor
)
import math

class NTLBGRepresentativeSelector(nn.Module):
    """NTLBGç»Ÿè®¡ç†è®ºçš„ä»£è¡¨ç‚¹é€‰æ‹©å™¨"""
    
    def __init__(self, d_model=1024, num_representatives=6, temperature=0.1):
        super().__init__()
        self.d_model = d_model
        self.num_representatives = num_representatives
        self.temperature = temperature
        
        # ç»Ÿè®¡å‚æ•°ä¼°è®¡ç½‘ç»œ
        self.mu_estimator = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.LayerNorm(d_model),
            nn.ReLU(),
            nn.Linear(d_model, d_model)
        )
        
        self.sigma_estimator = nn.Sequential(
            nn.Linear(d_model, d_model),
            nn.LayerNorm(d_model),  
            nn.ReLU(),
            nn.Linear(d_model, d_model),
            nn.Softplus()  # ç¡®ä¿æ–¹å·®ä¸ºæ­£
        )
        
        # æŸ¥è¯¢ä¾èµ–çš„é€‰æ‹©ç½‘ç»œ
        self.query_projector = nn.Linear(d_model, d_model)
        self.selection_head = nn.MultiheadAttention(d_model, 8, batch_first=True)
        
    def forward(self, **kwargs):
        """
        Args:
            video_features: [B, T, D] è§†é¢‘ç‰¹å¾åºåˆ—
            query_embedding: [B, D] æŸ¥è¯¢åµŒå…¥
        Returns:
            representative_features: [B, K, D] é€‰ä¸­çš„ä»£è¡¨ç‚¹ç‰¹å¾
            selection_info: åŒ…å«é€‰æ‹©ä¿¡æ¯çš„å­—å…¸
        """
        B, T, D = video_features.shape
        K = self.num_representatives
        
        # 1. åŸºäºæŸ¥è¯¢ä¼°è®¡ç»Ÿè®¡å‚æ•°
        mu_q = self.mu_estimator(query_embedding)  # [B, D]
        sigma_q = self.sigma_estimator(query_embedding) + 1e-6  # [B, D]
        
        # 2. è®¡ç®—é©¬æ°è·ç¦»
        centered_features = video_features - mu_q.unsqueeze(1)  # [B, T, D]
        mahalanobis_distances = torch.sum(
            (centered_features ** 2) / sigma_q.unsqueeze(1), dim=-1
        )  # [B, T]
        
        # 3. NTLBGä»£è¡¨ç‚¹é€‰æ‹©
        representative_indices, selection_weights = self._ntlbg_selection(
            mahalanobis_distances, video_features, mu_q, sigma_q
        )
        
        # 4. æå–ä»£è¡¨ç‚¹ç‰¹å¾
        representative_features = self._gather_features(
            video_features, representative_indices
        )
        
        # 5. è®¡ç®—NTLBGçº¦æŸæŸå¤±
        ntlbg_loss = self._compute_ntlbg_loss(
            representative_features, mu_q, sigma_q, mahalanobis_distances, representative_indices
        )
        
        return representative_features, {
            'representative_indices': representative_indices,
            'selection_weights': selection_weights,
            'mahalanobis_distances': mahalanobis_distances,
            'mu_q': mu_q,
            'sigma_q': sigma_q,
            'ntlbg_loss': ntlbg_loss
        }
    
    def _ntlbg_selection(self, distances, features, mu_q, sigma_q):
        """åŸºäºNTLBGç»Ÿè®¡ç†è®ºçš„ä»£è¡¨ç‚¹é€‰æ‹©"""
        B, T = distances.shape
        K = self.num_representatives
        
        # è®¡ç®—ç›®æ ‡ç­‰é«˜æ¤­çƒé¢çš„è·ç¦»
        median_distance = torch.median(distances, dim=1, keepdim=True)[0]  # [B, 1]
        
        # é€‰æ‹©æ¥è¿‘ç›®æ ‡è·ç¦»çš„å€™é€‰ç‚¹
        distance_weights = torch.exp(
            -torch.abs(distances - median_distance) / self.temperature
        )  # [B, T]
        
        # ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶è¿›è¡Œæœ€ç»ˆé€‰æ‹©
        query_expanded = mu_q.unsqueeze(1)  # [B, 1, D]
        attended_features, attention_weights = self.selection_head(
            query_expanded, features, features
        )  # [B, 1, D], [B, 1, T]
        
        # ç»“åˆè·ç¦»æƒé‡å’Œæ³¨æ„åŠ›æƒé‡
        combined_weights = distance_weights * attention_weights.squeeze(1)  # [B, T]
        
        # é€‰æ‹©top-Kä¸ªä»£è¡¨ç‚¹ï¼Œç¡®ä¿æ—¶åºå¤šæ ·æ€§
        representative_indices = self._diverse_top_k_selection(
            combined_weights, K, T
        )
        
        return representative_indices, combined_weights
    
    def _diverse_top_k_selection(self, weights, k, total_length):
        """æ—¶åºå¤šæ ·åŒ–çš„Top-Ké€‰æ‹©"""
        B = weights.shape[0]
        indices_list = []
        
        for b in range(B):
            w = weights[b]  # [T]
            
            if k >= total_length:
                indices = torch.arange(total_length, device=weights.device)
            else:
                # è´ªå¿ƒå¤šæ ·åŒ–é€‰æ‹©
                selected = []
                remaining = list(range(total_length))
                
                # é¦–å…ˆé€‰æ‹©æƒé‡æœ€é«˜çš„ç‚¹
                first_idx = torch.argmax(w).item()
                selected.append(first_idx)
                remaining.remove(first_idx)
                
                # è´ªå¿ƒé€‰æ‹©å‰©ä½™ç‚¹ï¼Œæœ€å¤§åŒ–æ—¶åºè·ç¦»
                for _ in range(k - 1):
                    if not remaining:
                        break
                    
                    best_idx = None
                    best_score = -1
                    
                    for candidate in remaining:
                        # è®¡ç®—æ—¶åºå¤šæ ·æ€§åˆ†æ•°
                        min_distance = min(abs(candidate - sel) for sel in selected)
                        diversity_score = min_distance * w[candidate].item()
                        
                        if diversity_score > best_score:
                            best_score = diversity_score
                            best_idx = candidate
                    
                    if best_idx is not None:
                        selected.append(best_idx)
                        remaining.remove(best_idx)
                
                indices = torch.tensor(selected, device=weights.device)
            
            indices_list.append(indices)
        
        # å¡«å……åˆ°ç›¸åŒé•¿åº¦
        max_len = max(len(idx) for idx in indices_list)
        padded_indices = torch.full((B, max_len), 0, device=weights.device)
        
        for b, indices in enumerate(indices_list):
            padded_indices[b, :len(indices)] = indices
        
        return padded_indices[:, :k]  # [B, K]
    
    def _gather_features(self, features, indices):
        """æ ¹æ®ç´¢å¼•æå–ç‰¹å¾"""
        B, T, D = features.shape
        K = indices.shape[1]
        
        # æ‰©å±•ç´¢å¼•ç»´åº¦
        expanded_indices = indices.unsqueeze(-1).expand(-1, -1, D)  # [B, K, D]
        
        # æå–ç‰¹å¾
        representative_features = torch.gather(features, 1, expanded_indices)  # [B, K, D]
        
        return representative_features
    
    def _compute_ntlbg_loss(self, rep_features, mu_q, sigma_q, distances, indices):
        """è®¡ç®—NTLBGçº¦æŸæŸå¤±"""
        B, K, D = rep_features.shape
        
        # 1. ç­‰é«˜æ¤­çƒé¢çº¦æŸï¼šä»£è¡¨ç‚¹åº”è¯¥åœ¨ç›¸ä¼¼çš„é©¬æ°è·ç¦»ä¸Š
        rep_distances = torch.gather(distances, 1, indices)  # [B, K]
        target_distance = rep_distances.median(dim=1, keepdim=True)[0]  # [B, 1]
        ellipsoid_loss = F.mse_loss(rep_distances, target_distance.expand_as(rep_distances))
        
        # 2. ç»Ÿè®¡ä¸€è‡´æ€§çº¦æŸï¼šä»£è¡¨ç‚¹åº”è¯¥ç¬¦åˆä¼°è®¡çš„åˆ†å¸ƒ
        centered_rep = rep_features - mu_q.unsqueeze(1)  # [B, K, D]
        consistency_loss = torch.mean(torch.sum(
            (centered_rep ** 2) / sigma_q.unsqueeze(1), dim=-1
        ))
        
        # 3. å¤šæ ·æ€§çº¦æŸï¼šä»£è¡¨ç‚¹ä¹‹é—´åº”è¯¥æœ‰è¶³å¤Ÿçš„å·®å¼‚
        pairwise_sim = torch.matmul(rep_features, rep_features.transpose(-1, -2))  # [B, K, K]
        diversity_loss = torch.mean(torch.triu(pairwise_sim, diagonal=1) ** 2)
        
        total_loss = ellipsoid_loss + 0.1 * consistency_loss + 0.05 * diversity_loss
        
        return total_loss


# class NTLBGLLaVAAdapter  # æš‚æ—¶ç¦ç”¨
class _NTLBGLLaVAAdapter(nn.Module):
    """åŸºäºLLaVAçš„NTLBG-LLMé€‚é…å™¨"""
    
    def __init__(self, base_model_name="llava-hf/LLaVA-NeXT-Video-7B-hf"):
        super().__init__()
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        print(f"ğŸ”„ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_name}")
        self.base_model = LlavaNextVideoForConditionalGeneration.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        self.processor = LlavaNextVideoProcessor.from_pretrained(
            base_model_name,
            trust_remote_code=True
        )
        
        # è·å–æ¨¡å‹é…ç½®
        self.d_model = self.base_model.config.text_config.hidden_size
        
        # é›†æˆNTLBGé€‰æ‹©å™¨
        self.ntlbg_selector = NTLBGRepresentativeSelector(
            d_model=self.d_model,
            num_representatives=6
        )
        
        # è§†é¢‘ç‰¹å¾é€‚é…å±‚
        self.video_adapter = nn.Sequential(
            nn.Linear(self.d_model, self.d_model),
            nn.LayerNorm(self.d_model),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.d_model, self.d_model)
        )
        
        # å†»ç»“åŸºç¡€æ¨¡å‹çš„éƒ¨åˆ†å‚æ•°
        self._freeze_base_model()
        
        print("âœ… NTLBG-LLaVAé€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _freeze_base_model(self):
        """å†»ç»“åŸºç¡€æ¨¡å‹çš„å¤§éƒ¨åˆ†å‚æ•°"""
        total_params = 0
        frozen_params = 0
        
        for name, param in self.base_model.named_parameters():
            total_params += param.numel()
            
            # åªå¾®è°ƒæœ€åå‡ å±‚å’Œè§†è§‰æŠ•å½±å±‚
            if any(keyword in name for keyword in [
                'language_model.model.layers.31',  # æœ€åä¸€å±‚
                'language_model.model.layers.30',  # å€’æ•°ç¬¬äºŒå±‚  
                'multi_modal_projector',           # å¤šæ¨¡æ€æŠ•å½±
                'language_model.lm_head'           # è¾“å‡ºå¤´
            ]):
                param.requires_grad = True
            else:
                param.requires_grad = False
                frozen_params += param.numel()
        
        print(f"ğŸ§Š å†»ç»“å‚æ•°æ¯”ä¾‹: {frozen_params/total_params:.1%}")
    
    def forward(self, **kwargs):
        """å‰å‘ä¼ æ’­"""
        B = input_ids.shape[0]
        
        # 1. æå–è§†é¢‘ç‰¹å¾
        with torch.no_grad():
            vision_outputs = self.base_model.vision_tower(
                pixel_values_videos.to(self.base_model.vision_tower.dtype)
            )
            video_features = vision_outputs.last_hidden_state  # [B, T, D]
        
        # 2. è·å–æ–‡æœ¬æŸ¥è¯¢åµŒå…¥
        with torch.no_grad():
            text_embeds = self.base_model.language_model.model.embed_tokens(input_ids)
            query_embedding = text_embeds.mean(dim=1)  # [B, D]
        
        # 3. NTLBGä»£è¡¨ç‚¹é€‰æ‹©
        representative_features, selection_info = self.ntlbg_selector(
            video_features, query_embedding
        )
        
        # 4. é€‚é…å™¨å¤„ç†
        adapted_features = self.video_adapter(representative_features)
        
        # 5. æ›¿æ¢è§†é¢‘ç‰¹å¾è¿›è¡Œæ¨ç†
        # è¿™é‡Œéœ€è¦é‡æ–°ç»„ç»‡è¾“å…¥ä»¥ä½¿ç”¨é€‰æ‹©çš„ä»£è¡¨ç‚¹
        # ç®€åŒ–å¤„ç†ï¼šç›´æ¥ä½¿ç”¨åŸºç¡€æ¨¡å‹
                # ç¡®ä¿æ‰€æœ‰è¾“å…¥éƒ½åœ¨æ¨¡å‹è®¾å¤‡ä¸Š
        device = next(self.base_model.parameters()).device
        for key, value in kwargs.items():
            if torch.is_tensor(kwargs[key]):
                kwargs[key] = value.to(device)
        
        
        outputs = self.base_model(**kwargs)
        
        # 6. æ·»åŠ NTLBGæŸå¤±
        if labels is not None:
            ntlbg_loss = selection_info['ntlbg_loss']
            outputs.loss = outputs.loss + 0.3 * ntlbg_loss
        
        # æ·»åŠ é€‰æ‹©ä¿¡æ¯åˆ°è¾“å‡º
        outputs.selection_info = selection_info
        outputs.representative_features = adapted_features
        
        return outputs


class NTLBGQwen2VLAdapter(nn.Module):
    """NTLBGå¢å¼ºçš„Qwen2-VLé€‚é…å™¨"""
    
    def __init__(self, base_model_name="microsoft/DialoGPT-medium", num_representatives=6):
        super().__init__()
        
        print(f"ğŸ”„ åŠ è½½Qwen2-VLåŸºç¡€æ¨¡å‹: {base_model_name}")
        
        # 1. åŠ è½½åŸºç¡€æ¨¡å‹
        self.base_model = AutoModel.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        self.processor = AutoProcessor.from_pretrained(
            base_model_name,
            trust_remote_code=True
        )
        
        # ä¿®å¤pad_token
        if hasattr(self.processor, 'pad_token') and self.processor.pad_token is None:
            self.processor.pad_token = self.processor.eos_token
            print("âœ… è®¾ç½®pad_token")
        
        self.tokenizer = self.processor
        
        # è·å–æ¨¡å‹é…ç½®
        self.hidden_size = self.base_model.config.hidden_size
        self.num_representatives = num_representatives
        
        # 2. å†»ç»“åŸºç¡€æ¨¡å‹å‚æ•°
        total_params = 0
        frozen_params = 0
        for param in self.base_model.parameters():
            total_params += param.numel()
            param.requires_grad = False
            frozen_params += param.numel()
        
        frozen_ratio = frozen_params / total_params
        print(f"ğŸ§Š å†»ç»“å‚æ•°æ¯”ä¾‹: {frozen_ratio:.1%}")
        
        # 3. åˆå§‹åŒ–NTLBGç»„ä»¶
        self.ntlbg_selector = NTLBGSelector(
            input_dim=self.hidden_size,
            num_representatives=num_representatives
        )
        
        # 4. æ·»åŠ é€‚é…å±‚
        self.adaptation_layer = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(self.hidden_size, self.hidden_size)
        )
        
        print("âœ… NTLBG-Qwen2VLé€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def forward(self, **kwargs):
        """å‰å‘ä¼ æ’­"""
        # ä»kwargsä¸­æå–å‚æ•°
        input_ids = kwargs.get('input_ids')
        attention_mask = kwargs.get('attention_mask') 
        pixel_values = kwargs.get('pixel_values')
        labels = kwargs.get('labels')
        
        # ç¡®ä¿tensoråœ¨æ­£ç¡®è®¾å¤‡ä¸Š
        device = next(self.base_model.parameters()).device
        if input_ids is not None:
            input_ids = input_ids.to(device)
        if attention_mask is not None:
            attention_mask = attention_mask.to(device)
        if pixel_values is not None:
            pixel_values = pixel_values.to(device)
        if labels is not None:
            labels = labels.to(device)
        
        # å‡†å¤‡è¾“å…¥
        model_inputs = {}
        if input_ids is not None:
            model_inputs['input_ids'] = input_ids
        if attention_mask is not None:
            model_inputs['attention_mask'] = attention_mask
        if pixel_values is not None:
            model_inputs['pixel_values'] = pixel_values
        if labels is not None:
            model_inputs['labels'] = labels
            
        # è°ƒç”¨åŸºç¡€æ¨¡å‹
        outputs = self.base_model(**model_inputs)
        
        return outputs


        
def create_ntlbg_adapter(base_model_type="qwen2vl"):
    """åˆ›å»ºNTLBGé€‚é…å™¨"""
    if base_model_type.lower() == "qwen2vl":
        return NTLBGQwen2VLAdapter()
    else:
        print(f"âš ï¸ æš‚æ—¶åªæ”¯æŒQwen2VLï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return NTLBGQwen2VLAdapter()

