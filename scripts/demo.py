#!/usr/bin/env python3
"""
NTLBG-LLMæ¼”ç¤ºè„šæœ¬
å±•ç¤ºæ¨¡å‹çš„æ ¸å¿ƒåŠŸèƒ½å’Œä»£è¡¨ç‚¹é€‰æ‹©æ•ˆæœ
"""

import os
import sys
import json
import torch
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.ntlbg_llm import create_ntlbg_llm
from src.models.ntlbg_attention import NTLBGAttention
from src.models.rich_points import RichRepresentativePointConstructor
from src.data.video_loader import VideoLoader


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="NTLBG-LLMæ¨¡å‹æ¼”ç¤º")
    parser.add_argument(
        "--config", 
        type=str, 
        default="configs/ntlbg_base_config.json",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--save_plots", 
        action="store_true",
        help="ä¿å­˜å¯è§†åŒ–å›¾ç‰‡"
    )
    return parser.parse_args()


def demo_ntlbg_attention():
    """æ¼”ç¤ºNTLBGæ³¨æ„åŠ›æœºåˆ¶"""
    print("ğŸ¯ æ¼”ç¤ºNTLBGæ³¨æ„åŠ›æœºåˆ¶")
    print("=" * 50)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    batch_size, T, d_model = 2, 50, 768
    video_features = torch.randn(batch_size, T, d_model)
    query_embedding = torch.randn(batch_size, d_model)
    
    # åˆå§‹åŒ–NTLBGæ³¨æ„åŠ›
    ntlbg_attention = NTLBGAttention(
        d_model=d_model,
        d_query=d_model,
        num_representatives=6
    )
    
    # å‰å‘ä¼ æ’­
    results = ntlbg_attention(video_features, query_embedding, return_stats=True)
    
    print(f"ğŸ“Š è¾“å…¥è§†é¢‘ç‰¹å¾å½¢çŠ¶: {video_features.shape}")
    print(f"ğŸ“Š æŸ¥è¯¢åµŒå…¥å½¢çŠ¶: {query_embedding.shape}")
    print(f"ğŸ“Š ä»£è¡¨ç‚¹ç´¢å¼•: {results['representative_indices'][0]}")
    print(f"ğŸ“Š ä»£è¡¨ç‚¹æƒé‡: {results['weights'][0]}")
    print(f"ğŸ“Š é€‰æ‹©çš„å¸§: {results['representative_indices'][0].tolist()}")
    
    # è®¡ç®—NTLBGçº¦æŸæŸå¤±
    constraint_loss = ntlbg_attention.compute_ntlbg_constraint_loss(
        results['representative_features'],
        results['mu_q'],
        results['sigma_q']
    )
    print(f"ğŸ“Š NTLBGçº¦æŸæŸå¤±: {constraint_loss.item():.6f}")
    
    return results


def demo_rich_points(ntlbg_results):
    """æ¼”ç¤ºå¯Œä»£è¡¨ç‚¹æ„é€ """
    print("\nğŸŒŸ æ¼”ç¤ºå¯Œä»£è¡¨ç‚¹æ„é€ ")
    print("=" * 50)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    batch_size, T, d_visual = 2, 50, 768
    video_features = torch.randn(batch_size, T, d_visual)
    representative_indices = ntlbg_results['representative_indices']
    
    # åˆå§‹åŒ–å¯Œä»£è¡¨ç‚¹æ„é€ å™¨
    rich_constructor = RichRepresentativePointConstructor(
        d_visual=d_visual,
        d_context=256,
        d_temporal=64
    )
    
    # æ„é€ å¯Œä»£è¡¨ç‚¹
    rich_results = rich_constructor(
        video_features=video_features,
        representative_indices=representative_indices
    )
    
    print(f"ğŸ“Š å¯Œä»£è¡¨ç‚¹ç‰¹å¾å½¢çŠ¶: {rich_results['rich_features'].shape}")
    print(f"ğŸ“Š ä¸Šä¸‹æ–‡ç‰¹å¾å½¢çŠ¶: {rich_results['context_features'].shape}")
    print(f"ğŸ“Š ä»£è¡¨æ€§æƒé‡: {rich_results['representativeness_weights'][0]}")
    print(f"ğŸ“Š è¦†ç›–èŒƒå›´: {rich_results['coverage_ranges'][0]}")
    
    # è®¡ç®—æŸå¤±
    info_loss = rich_constructor.compute_information_preservation_loss(
        video_features, rich_results['rich_features'], representative_indices
    )
    temporal_loss = rich_constructor.compute_temporal_coherence_loss(
        rich_results['rich_features'], representative_indices
    )
    
    print(f"ğŸ“Š ä¿¡æ¯ä¿æŒæŸå¤±: {info_loss.item():.6f}")
    print(f"ğŸ“Š æ—¶åºè¿è´¯æ€§æŸå¤±: {temporal_loss.item():.6f}")
    
    return rich_results


def demo_full_model(config):
    """æ¼”ç¤ºå®Œæ•´çš„NTLBG-LLMæ¨¡å‹"""
    print("\nğŸš€ æ¼”ç¤ºå®Œæ•´NTLBG-LLMæ¨¡å‹")
    print("=" * 50)
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    batch_size = 2
    T = 30  # è§†é¢‘å¸§æ•°
    seq_len = 20  # æ–‡æœ¬é•¿åº¦
    vocab_size = 32000
    
    # æ¨¡æ‹Ÿè§†é¢‘å¸§
    video_frames = torch.randn(batch_size, T, 3, 224, 224)
    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    attention_mask = torch.ones(batch_size, seq_len)
    labels = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    # åˆ›å»ºæ¨¡å‹
    model = create_ntlbg_llm(config['model_config'])
    
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
    print(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.1f}M")
    
    # å‰å‘ä¼ æ’­
    model.eval()
    with torch.no_grad():
        outputs = model(
            video_frames=video_frames,
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            return_ntlbg_stats=True
        )
    
    print(f"ğŸ“Š è¾“å‡ºlogitså½¢çŠ¶: {outputs['logits'].shape}")
    print(f"ğŸ“Š æ€»æŸå¤±: {outputs['loss'].item():.6f}")
    print(f"ğŸ“Š æŸå¤±ç»„ä»¶:")
    for name, loss in outputs['loss_components'].items():
        print(f"   {name}: {loss.item():.6f}")
    
    print(f"ğŸ“Š ä»£è¡¨ç‚¹ç´¢å¼•: {outputs['representative_indices'][0]}")
    print(f"ğŸ“Š ä»£è¡¨ç‚¹æƒé‡: {outputs['representative_weights'][0]}")
    
    # ç”Ÿæˆæµ‹è¯•
    print("\nğŸ¬ æµ‹è¯•ç”ŸæˆåŠŸèƒ½...")
    generation_outputs = model.generate(
        video_frames=video_frames,
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_new_tokens=5,
        do_sample=False
    )
    
    print(f"ğŸ“Š ç”Ÿæˆçš„tokenå½¢çŠ¶: {generation_outputs['generated_ids'].shape}")
    print(f"ğŸ“Š ç”Ÿæˆçš„tokens: {generation_outputs['generated_ids'][0]}")
    
    return outputs


def visualize_representative_selection(ntlbg_results, save_plots=False):
    """å¯è§†åŒ–ä»£è¡¨ç‚¹é€‰æ‹©ç»“æœ"""
    print("\nğŸ“Š å¯è§†åŒ–ä»£è¡¨ç‚¹é€‰æ‹©")
    print("=" * 50)
    
    # æå–ç¬¬ä¸€ä¸ªbatchçš„ç»“æœ
    indices = ntlbg_results['representative_indices'][0].cpu().numpy()
    weights = ntlbg_results['weights'][0].cpu().numpy()
    
    # åˆ›å»ºæ—¶åºå›¾
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
    
    # ç»˜åˆ¶ä»£è¡¨ç‚¹åˆ†å¸ƒ
    video_length = 50  # å‡è®¾è§†é¢‘é•¿åº¦
    frames = np.arange(video_length)
    
    # æ‰€æœ‰å¸§
    ax1.scatter(frames, np.ones_like(frames), alpha=0.3, s=20, label='All frames')
    
    # ä»£è¡¨ç‚¹
    ax1.scatter(indices, np.ones_like(indices), c='red', s=100, marker='*', label='Representative points')
    
    # æ·»åŠ æƒé‡ä¿¡æ¯
    for i, (idx, weight) in enumerate(zip(indices, weights)):
        ax1.annotate(f'{idx}\n({weight:.3f})', 
                    (idx, 1), 
                    xytext=(0, 20), 
                    textcoords='offset points', 
                    ha='center', 
                    fontsize=8)
    
    ax1.set_xlabel('Frame Index')
    ax1.set_ylabel('Selection')
    ax1.set_title('NTLBG Representative Point Selection')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # ç»˜åˆ¶æƒé‡åˆ†å¸ƒ
    ax2.bar(range(len(weights)), weights, alpha=0.7, color='skyblue')
    ax2.set_xlabel('Representative Point Index')
    ax2.set_ylabel('Weight')
    ax2.set_title('Representative Point Weights')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_plots:
        plt.savefig('representative_selection.png', dpi=300, bbox_inches='tight')
        print("ğŸ“Š å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ° representative_selection.png")
    else:
        plt.show()
    
    plt.close()


def performance_benchmark():
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("\nâš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 50)
    
    import time
    
    # æµ‹è¯•ä¸åŒè§†é¢‘é•¿åº¦çš„æ€§èƒ½
    video_lengths = [20, 50, 100, 200]
    batch_size = 2
    d_model = 768
    
    ntlbg_attention = NTLBGAttention(
        d_model=d_model,
        d_query=d_model,
        num_representatives=6
    )
    
    for T in video_lengths:
        video_features = torch.randn(batch_size, T, d_model)
        query_embedding = torch.randn(batch_size, d_model)
        
        # é¢„çƒ­
        for _ in range(5):
            _ = ntlbg_attention(video_features, query_embedding)
        
        # è®¡æ—¶
        start_time = time.time()
        for _ in range(10):
            _ = ntlbg_attention(video_features, query_embedding)
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 10
        print(f"ğŸ“Š è§†é¢‘é•¿åº¦ {T}: {avg_time:.4f}s æ¯æ¬¡å‰å‘ä¼ æ’­")
    
    print("ğŸ“Š æ€§èƒ½æµ‹è¯•å®Œæˆ!")


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r') as f:
        config = json.load(f)
    
    print("ğŸ¬ NTLBG-LLMæ¨¡å‹æ¼”ç¤º")
    print("=" * 60)
    print(f"ğŸ“‹ é…ç½®æ–‡ä»¶: {args.config}")
    print(f"ğŸ“‹ PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"ğŸ“‹ è®¾å¤‡: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    
    # 1. æ¼”ç¤ºNTLBGæ³¨æ„åŠ›æœºåˆ¶
    ntlbg_results = demo_ntlbg_attention()
    
    # 2. æ¼”ç¤ºå¯Œä»£è¡¨ç‚¹æ„é€ 
    rich_results = demo_rich_points(ntlbg_results)
    
    # 3. æ¼”ç¤ºå®Œæ•´æ¨¡å‹
    model_outputs = demo_full_model(config)
    
    # 4. å¯è§†åŒ–ç»“æœ
    visualize_representative_selection(ntlbg_results, save_plots=args.save_plots)
    
    # 5. æ€§èƒ½åŸºå‡†æµ‹è¯•
    performance_benchmark()
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")
    print("ğŸ¯ NTLBG-LLMæ¨¡å‹æ ¸å¿ƒåŠŸèƒ½æ¼”ç¤ºå®Œæ¯•")
    print("ğŸ“Š å…³é”®ç‰¹æ€§:")
    print("   - åŸºäºç»Ÿè®¡å­¦ç†è®ºçš„ä»£è¡¨ç‚¹é€‰æ‹©")
    print("   - å¯Œä»£è¡¨ç‚¹çš„æ—¶ç©ºä¸Šä¸‹æ–‡ä¿¡æ¯")
    print("   - å¤šä»»åŠ¡æŸå¤±å‡½æ•°ä¼˜åŒ–")
    print("   - ç«¯åˆ°ç«¯çš„è§†é¢‘é—®ç­”èƒ½åŠ›")


if __name__ == "__main__":
    main() 