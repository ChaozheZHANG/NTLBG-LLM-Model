"""
åŸºäºLLaVAåˆ›å»ºçœŸæ­£çš„NTLBG-LLM
"""
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer, CLIPVisionModel, CLIPImageProcessor
import os

class RealNTLBGLLM(nn.Module):
    """çœŸæ­£çš„NTLBG-LLMï¼ŒåŸºäºLLaVAæ¶æ„"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # è§†è§‰ç¼–ç å™¨ (CLIP)
        self.vision_tower = CLIPVisionModel.from_pretrained("openai/clip-vit-large-patch14")
        self.vision_processor = CLIPImageProcessor.from_pretrained("openai/clip-vit-large-patch14")
        
        # è¯­è¨€æ¨¡å‹ (é€‰æ‹©ä¸€ä¸ªå¼€æºæ¨¡å‹)
        model_choices = [
            "microsoft/DialoGPT-medium",
            "microsoft/DialoGPT-large", 
            "facebook/opt-1.3b",
            "EleutherAI/gpt-neo-1.3B"
        ]
        
        # å°è¯•åŠ è½½å¯ç”¨çš„æ¨¡å‹
        self.language_model = None
        self.tokenizer = None
        
        for model_name in model_choices:
            try:
                print(f"ğŸ”„ å°è¯•åŠ è½½: {model_name}")
                self.language_model = AutoModel.from_pretrained(model_name)
                self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                
                # æ·»åŠ pad_tokenå¦‚æœä¸å­˜åœ¨
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                    
                print(f"âœ… æˆåŠŸåŠ è½½: {model_name}")
                break
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥ {model_name}: {e}")
                continue
        
        if self.language_model is None:
            raise RuntimeError("âŒ æ— æ³•åŠ è½½ä»»ä½•è¯­è¨€æ¨¡å‹")
        
        # NTLBGæ ¸å¿ƒç»„ä»¶
        vision_dim = self.vision_tower.config.hidden_size
        lang_dim = self.language_model.config.hidden_size
        
        # ä»£è¡¨ç‚¹æ•°é‡
        self.num_representatives = config.get('num_representatives', 6)
        
        # è§†è§‰åˆ°è¯­è¨€çš„æŠ•å½±
        self.vision_proj = nn.Linear(vision_dim, lang_dim)
        
        # NTLBGä»£è¡¨ç‚¹å­¦ä¹ 
        self.representative_tokens = nn.Parameter(
            torch.randn(self.num_representatives, lang_dim)
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶ç”¨äºé€‰æ‹©ä»£è¡¨ç‚¹
        self.attention = nn.MultiheadAttention(lang_dim, num_heads=8, batch_first=True)
        
        # èåˆå±‚
        self.fusion_layer = nn.TransformerEncoderLayer(
            d_model=lang_dim, 
            nhead=8, 
            dim_feedforward=lang_dim*4,
            batch_first=True
        )
        
        # è¾“å‡ºå±‚
        self.output_proj = nn.Linear(lang_dim, self.language_model.config.vocab_size)
        
        print(f"âœ… NTLBG-LLM åˆå§‹åŒ–å®Œæˆ")
        print(f"   è§†è§‰ç¼–ç å™¨: {vision_dim}D")
        print(f"   è¯­è¨€æ¨¡å‹: {lang_dim}D")
        print(f"   ä»£è¡¨ç‚¹æ•°é‡: {self.num_representatives}")
    
    def encode_video_frames(self, frames):
        """ç¼–ç è§†é¢‘å¸§"""
        if len(frames) == 0:
            return torch.zeros(1, 768).to(next(self.parameters()).device)
        
        # å¤„ç†å›¾åƒ
        try:
            inputs = self.vision_processor(frames, return_tensors="pt")
            inputs = {k: v.to(next(self.parameters()).device) for k, v in inputs.items()}
            
            # è·å–è§†è§‰ç‰¹å¾
            vision_outputs = self.vision_tower(**inputs)
            vision_features = vision_outputs.last_hidden_state.mean(dim=1)  # [batch, dim]
            
            return vision_features
        except Exception as e:
            print(f"âŒ è§†é¢‘ç¼–ç å¤±è´¥: {e}")
            return torch.zeros(1, 768).to(next(self.parameters()).device)
    
    def ntlbg_representation_learning(self, features):
        """NTLBGä»£è¡¨ç‚¹å­¦ä¹ """
        batch_size = features.shape[0]
        
        # æ‰©å±•ä»£è¡¨ç‚¹åˆ°batch
        repr_tokens = self.representative_tokens.unsqueeze(0).expand(
            batch_size, -1, -1
        )  # [batch, num_repr, dim]
        
        # ä½¿ç”¨æ³¨æ„åŠ›é€‰æ‹©æœ€é‡è¦çš„ä»£è¡¨ç‚¹
        attended_repr, attention_weights = self.attention(
            repr_tokens, features.unsqueeze(1), features.unsqueeze(1)
        )
        
        # èåˆç‰¹å¾
        fused_features = self.fusion_layer(
            torch.cat([attended_repr, features.unsqueeze(1)], dim=1)
        )
        
        # å–å¹³å‡ä½œä¸ºæœ€ç»ˆè¡¨ç¤º
        final_repr = fused_features.mean(dim=1)  # [batch, dim]
        
        return final_repr, attention_weights
    
    def forward(self, video_frames, text_input, questions=None):
        """å‰å‘ä¼ æ’­"""
        device = next(self.parameters()).device
        
        # ç¼–ç è§†é¢‘
        if video_frames:
            vision_features = self.encode_video_frames(video_frames)
            vision_features = self.vision_proj(vision_features)
        else:
            vision_features = torch.zeros(1, self.language_model.config.hidden_size).to(device)
        
        # NTLBGè¡¨ç¤ºå­¦ä¹ 
        ntlbg_features, attention_weights = self.ntlbg_representation_learning(vision_features)
        
        # ç¼–ç æ–‡æœ¬
        if text_input:
            text_tokens = self.tokenizer(
                text_input, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=512
            )
            text_tokens = {k: v.to(device) for k, v in text_tokens.items()}
            
            # è·å–æ–‡æœ¬ç‰¹å¾
            lang_outputs = self.language_model(**text_tokens)
            text_features = lang_outputs.last_hidden_state.mean(dim=1)
        else:
            text_features = torch.zeros(1, self.language_model.config.hidden_size).to(device)
        
        # èåˆè§†è§‰å’Œæ–‡æœ¬ç‰¹å¾
        combined_features = ntlbg_features + text_features
        
        # ç”Ÿæˆè¾“å‡º
        output_logits = self.output_proj(combined_features)
        
        return {
            "logits": output_logits,
            "attention_weights": attention_weights,
            "ntlbg_features": ntlbg_features
        }

def create_model():
    """åˆ›å»ºNTLBG-LLMæ¨¡å‹"""
    config = {
        "num_representatives": 6,
        "vision_encoder": "openai/clip-vit-large-patch14"
    }
    
    try:
        model = RealNTLBGLLM(config)
        
        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"ğŸ“Š æ¨¡å‹ç»Ÿè®¡:")
        print(f"   æ€»å‚æ•°: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        return model
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    print("ğŸš€ åˆ›å»ºçœŸæ­£çš„NTLBG-LLM")
    model = create_model()
    
    if model:
        # ä¿å­˜æ¨¡å‹æ¶æ„
        torch.save(model.state_dict(), "/workspace/NTLBG-LLM/models/ntlbg_llm_base.pth")
        print("âœ… æ¨¡å‹æ¶æ„å·²ä¿å­˜")
    
    print("ğŸ¯ ä¸‹ä¸€æ­¥: ä½¿ç”¨è¿™ä¸ªçœŸæ­£çš„æ¨¡å‹è¿›è¡Œè®­ç»ƒ")
