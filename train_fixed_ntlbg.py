"""
ä¿®å¤ç‰ˆNTLBGè®­ç»ƒè„šæœ¬
"""
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset
import os
import sys
from tqdm import tqdm
import json
import numpy as np
from datetime import datetime
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ·»åŠ è·¯å¾„
sys.path.append('/workspace/NTLBG-LLM')
sys.path.append('/workspace/NTLBG-LLM/src')

# å¯¼å…¥ä¿®å¤ç‰ˆæ¨¡å‹
from src.models.ntlbg_llm_fixed import create_fixed_ntlbg_llm

# å¯¼å…¥å®˜æ–¹æ•°æ®åŠ è½½å™¨
try:
    from longvideobench import LongVideoBenchDataset
    HAS_OFFICIAL_LOADER = True
    logger.info("âœ… æˆåŠŸå¯¼å…¥å®˜æ–¹LongVideoBenchæ•°æ®åŠ è½½å™¨")
except ImportError:
    HAS_OFFICIAL_LOADER = False
    logger.warning("âš ï¸ æœªå®‰è£…å®˜æ–¹LongVideoBenchåŒ…ï¼Œä½¿ç”¨ç®€åŒ–æ•°æ®åŠ è½½å™¨")

class FixedTrainer:
    def __init__(self, config):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºä¿®å¤ç‰ˆæ¨¡å‹
        logger.info("ğŸ”¨ åˆ›å»ºä¿®å¤ç‰ˆNTLBG-LLM...")
        model_config = {
            'base_model_name': 'microsoft/DialoGPT-medium',
            'num_representatives': config.get('num_representatives', 6)
        }
        
        self.model = create_fixed_ntlbg_llm(model_config).to(self.device)
        
        # åˆ›å»ºæ•°æ®é›†
        self.train_dataset, self.val_dataset = self._create_datasets()
        
        # åªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        self.optimizer = torch.optim.AdamW(
            trainable_params,
            lr=config.get('learning_rate', 5e-5),  # ç¨é«˜çš„å­¦ä¹ ç‡
            weight_decay=config.get('weight_decay', 0.01),
            eps=1e-8
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer, 
            T_max=config.get('num_epochs', 5),
            eta_min=1e-7
        )
        
        logger.info(f"âœ… ä¿®å¤ç‰ˆè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   è®­ç»ƒæ ·æœ¬: {len(self.train_dataset) if self.train_dataset else 0}")
        logger.info(f"   éªŒè¯æ ·æœ¬: {len(self.val_dataset) if self.val_dataset else 0}")
        logger.info(f"   å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in trainable_params):,}")
    
    def _create_datasets(self):
        """åˆ›å»ºæ•°æ®é›†"""
        if HAS_OFFICIAL_LOADER:
            return self._create_official_datasets()
        else:
            return self._create_simple_datasets()
    
    def _create_official_datasets(self):
        """ä½¿ç”¨å®˜æ–¹æ•°æ®åŠ è½½å™¨"""
        try:
            data_path = "/workspace/NTLBG-LLM/data/longvideobench"
# ä¿®å¤NTLBGæ ¸å¿ƒæ¨¡å—




tree
cat > src/models/ntlbg_llm_fixed.py << 'EOF'
"""
ä¿®å¤ç‰ˆNTLBG-LLMä¸»æ¨¡å‹
"""
import torch
import torch.nn as nn
from transformers import (
    AutoModel, AutoTokenizer, 
    CLIPVisionModel, CLIPImageProcessor,
    AutoModelForCausalLM
)
from .ntlbg_core_fixed import FixedNTLBGAttention
import logging

logger = logging.getLogger(__name__)

class FixedNTLBGLLM(nn.Module):
    """ä¿®å¤ç‰ˆNTLBG-LLM"""
    
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # åŸºç¡€æ¨¡å‹é…ç½®
        self.base_model_name = config.get('base_model_name', 'microsoft/DialoGPT-medium')
        
        # è§†è§‰ç¼–ç å™¨
        self.vision_encoder = CLIPVisionModel.from_pretrained('openai/clip-vit-large-patch14')
        self.vision_processor = CLIPImageProcessor.from_pretrained('openai/clip-vit-large-patch14')
        
        # è¯­è¨€æ¨¡å‹
        self.language_model = AutoModel.from_pretrained(self.base_model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # è·å–ç»´åº¦
        vision_dim = self.vision_encoder.config.hidden_size  # 1024
        lang_dim = self.language_model.config.hidden_size    # 1024
        
        # **ä¿®å¤ç‰ˆNTLBGæ ¸å¿ƒ**
        self.ntlbg_attention = FixedNTLBGAttention(
            d_model=vision_dim,
            d_query=lang_dim,
            num_representatives=config.get('num_representatives', 6)
        )
        
        # æ¨¡æ€å¯¹é½
        self.vision_projection = nn.Sequential(
            nn.Linear(vision_dim, lang_dim),
            nn.LayerNorm(lang_dim),
            nn.GELU()
        )
        
        # å¤šæ¨¡æ€èåˆ
        self.multimodal_fusion = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(
                d_model=lang_dim,
                nhead=8,
                dim_feedforward=lang_dim * 4,
                dropout=0.1,
                batch_first=True
            ),
            num_layers=2
        )
        
        # è¾“å‡ºå±‚
        vocab_size = len(self.tokenizer)
        self.output_projection = nn.Sequential(
            nn.Linear(lang_dim, lang_dim),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(lang_dim, vocab_size)
        )
        
        # åˆ†ç±»å¤´ï¼ˆç”¨äºå¤šé€‰é¢˜ï¼‰
        self.classification_head = nn.Sequential(
            nn.Linear(lang_dim, lang_dim // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(lang_dim // 2, 4)  # 4é€‰æ‹©é¢˜
        )
        
        # å†»ç»“å¤§éƒ¨åˆ†å‚æ•°
        self._freeze_base_models()
        
        logger.info(f"âœ… ä¿®å¤ç‰ˆNTLBG-LLMåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   è§†è§‰ç¼–ç å™¨: {vision_dim}D -> {lang_dim}D")
        logger.info(f"   NTLBGä»£è¡¨ç‚¹: {config.get('num_representatives', 6)}ä¸ª")
    
    def _freeze_base_models(self):
        """æ™ºèƒ½å†»ç»“ç­–ç•¥"""
        # å†»ç»“è§†è§‰ç¼–ç å™¨çš„å‰é¢å‡ å±‚
        for name, param in self.vision_encoder.named_parameters():
            if not any(layer in name for layer in ['layer.22', 'layer.23', 'pooler']):
                param.requires_grad = False
        
        # å†»ç»“è¯­è¨€æ¨¡å‹çš„embeddingå’Œå‰é¢å‡ å±‚
        for name, param in self.language_model.named_parameters():
            if not any(layer in name for layer in ['layer.22', 'layer.23', 'pooler']):
                param.requires_grad = False
        
        frozen = sum(p.numel() for p in self.parameters() if not p.requires_grad)
        total = sum(p.numel() for p in self.parameters())
        logger.info(f"ğŸ§Š å†»ç»“å‚æ•°: {frozen/total:.1%}")
    
    def encode_video_frames(self, video_frames):
        """æ”¹è¿›çš„è§†é¢‘ç¼–ç """
        if not video_frames or len(video_frames) == 0:
            device = next(self.parameters()).device
            return torch.zeros(1, 1, self.vision_encoder.config.hidden_size, device=device)
        
        try:
            # é™åˆ¶å¸§æ•°ä»¥é¿å…å†…å­˜é—®é¢˜
            max_frames = 64
            if len(video_frames) > max_frames:
                # å‡åŒ€é‡‡æ ·
                indices = torch.linspace(0, len(video_frames)-1, max_frames, dtype=torch.long)
                video_frames = [video_frames[i] for i in indices]
            
            # é¢„å¤„ç†
            if hasattr(video_frames[0], 'size'):  # PIL Images
                inputs = self.vision_processor(video_frames, return_tensors="pt")
            else:
                inputs = {'pixel_values': torch.stack(video_frames)}
            
            device = next(self.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # ç¼–ç ï¼ˆå…è®¸æ¢¯åº¦ä¼ æ’­ï¼‰
            vision_outputs = self.vision_encoder(**inputs)
            
            # è·å–ç‰¹å¾
            if hasattr(vision_outputs, 'pooler_output'):
                frame_features = vision_outputs.pooler_output  # [T, hidden_size]
            else:
                frame_features = vision_outputs.last_hidden_state.mean(dim=1)
            
            return frame_features.unsqueeze(0)  # [1, T, hidden_size]
            
        except Exception as e:
            logger.warning(f"è§†é¢‘ç¼–ç å¤±è´¥: {e}")
            device = next(self.parameters()).device
            return torch.randn(1, 8, self.vision_encoder.config.hidden_size, device=device)
    
    def encode_text(self, text_input):
        """æ”¹è¿›çš„æ–‡æœ¬ç¼–ç """
        if not text_input:
            device = next(self.parameters()).device
            return torch.zeros(1, self.language_model.config.hidden_size, device=device)
        
        try:
            tokens = self.tokenizer(
                text_input,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=256  # å‡å°‘é•¿åº¦
            )
            
            device = next(self.parameters()).device
            tokens = {k: v.to(device) for k, v in tokens.items()}
            
            # ç¼–ç ï¼ˆå…è®¸æ¢¯åº¦ä¼ æ’­ï¼‰
            outputs = self.language_model(**tokens)
            
            # è·å–ç‰¹å¾
            if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:
                text_features = outputs.pooler_output
            else:
                text_features = outputs.last_hidden_state.mean(dim=1)
            
            return text_features  # [1, hidden_size]
            
        except Exception as e:
            logger.warning(f"æ–‡æœ¬ç¼–ç å¤±è´¥: {e}")
            device = next(self.parameters()).device
            return torch.randn(1, self.language_model.config.hidden_size, device=device)
    
    def forward(self, video_frames=None, text_input=None, labels=None, return_loss=True):
        """ä¿®å¤ç‰ˆå‰å‘ä¼ æ’­"""
        device = next(self.parameters()).device
        
        # 1. ç¼–ç è¾“å…¥
        video_features = self.encode_video_frames(video_frames)  # [1, T, vision_dim]
        text_features = self.encode_text(text_input)  # [1, lang_dim]
        
        # 2. è§†è§‰ç‰¹å¾æŠ•å½±
        video_features_proj = self.vision_projection(video_features)  # [1, T, lang_dim]
        
        # 3. NTLBGå¤„ç†
        ntlbg_results = self.ntlbg_attention(
            video_features=video_features_proj,
            query_embedding=text_features
        )
        
        # 4. å¤šæ¨¡æ€èåˆ
        representative_features = ntlbg_results['representative_features']  # [1, K, lang_dim]
        attended_features = ntlbg_results['attended_features']  # [1, 1, lang_dim]
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        all_features = torch.cat([
            text_features.unsqueeze(1),  # åŸå§‹æ–‡æœ¬
            attended_features,           # æ³¨æ„åŠ›ç‰¹å¾
            representative_features      # ä»£è¡¨ç‚¹ç‰¹å¾
        ], dim=1)  # [1, 1+1+K, lang_dim]
        
        # å¤šæ¨¡æ€èåˆ
        fused_features = self.multimodal_fusion(all_features)  # [1, 1+1+K, lang_dim]
        
        # 5. è¾“å‡ºé¢„æµ‹
        pooled_features = fused_features.mean(dim=1)  # [1, lang_dim]
        
        # ç”Ÿæˆå¼è¾“å‡º
        generation_logits = self.output_projection(pooled_features)  # [1, vocab_size]
        
        # åˆ†ç±»è¾“å‡ºï¼ˆç”¨äºå¤šé€‰é¢˜ï¼‰
        classification_logits = self.classification_head(pooled_features)  # [1, 4]
        
        outputs = {
            'logits': generation_logits,
            'classification_logits': classification_logits,
            'representative_features': representative_features,
            'representative_indices': ntlbg_results['representative_indices'],
            'mahalanobis_distances': ntlbg_results['mahalanobis_distances'],
            'attention_weights': ntlbg_results.get('cross_attention_weights')
        }
        
        # 6. è®¡ç®—æŸå¤±
        if return_loss and labels is not None:
            # å¤„ç†ä¸åŒç±»å‹çš„æ ‡ç­¾
            if isinstance(labels, torch.Tensor) and labels.numel() == 1:
                # å•ä¸ªæ ‡ç­¾ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰
                if labels.item() < 4:  # 4é€‰æ‹©é¢˜
                    loss = nn.CrossEntropyLoss()(classification_logits, labels.view(-1))
                else:  # ç”Ÿæˆä»»åŠ¡
                    loss = nn.CrossEntropyLoss()(generation_logits, labels.view(-1))
            else:
                # åºåˆ—æ ‡ç­¾æˆ–å…¶ä»–æ ¼å¼
                loss = nn.CrossEntropyLoss()(generation_logits, labels.view(-1))
            
            # æ·»åŠ NTLBGçº¦æŸæŸå¤±
            ntlbg_loss = self.ntlbg_attention.ntlbg_core.compute_ntlbg_constraint_loss(
                representative_features,
                ntlbg_results['mu_q'],
                ntlbg_results['sigma_q']
            )
            
            # æ€»æŸå¤±
            total_loss = loss + 0.1 * ntlbg_loss  # è°ƒæ•´æƒé‡
            
            outputs.update({
                'loss': total_loss,
                'task_loss': loss,
                'ntlbg_loss': ntlbg_loss
            })
        
        return outputs


def create_fixed_ntlbg_llm(config):
    """åˆ›å»ºä¿®å¤ç‰ˆNTLBG-LLM"""
    model = FixedNTLBGLLM(config)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    logger.info(f"ğŸ“Š ä¿®å¤ç‰ˆNTLBG-LLM:")
    logger.info(f"   æ€»å‚æ•°: {total_params:,}")
    logger.info(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    logger.info(f"   è®­ç»ƒæ•ˆç‡: {trainable_params/total_params:.1%}")
    
    return model
   def _create_official_datasets(self):
       """ä½¿ç”¨å®˜æ–¹æ•°æ®åŠ è½½å™¨"""
       try:
           data_path = "/workspace/NTLBG-LLM/data/longvideobench"
           
           # åŠ è½½éªŒè¯é›†æ•°æ®
           val_dataset = LongVideoBenchDataset(
               data_path, 
               "lvb_val.json", 
               max_num_frames=32
           )
           
           # å°†éªŒè¯é›†åˆ†ä¸ºè®­ç»ƒå’ŒéªŒè¯
           total_size = len(val_dataset)
           train_size = int(0.8 * total_size)
           val_size = total_size - train_size
           
           # éšæœºåˆ†å‰²
           indices = torch.randperm(total_size).tolist()
           train_indices = indices[:train_size]
           val_indices = indices[train_size:]
           
           train_dataset = Subset(val_dataset, train_indices)
           val_dataset_subset = Subset(val_dataset, val_indices)
           
           logger.info(f"âœ… ä½¿ç”¨å®˜æ–¹LongVideoBenchæ•°æ®")
           logger.info(f"   åŸå§‹æ•°æ®: {total_size} æ ·æœ¬")
           logger.info(f"   è®­ç»ƒ: {len(train_dataset)} æ ·æœ¬")
           logger.info(f"   éªŒè¯: {len(val_dataset_subset)} æ ·æœ¬")
           
           return train_dataset, val_dataset_subset
           
       except Exception as e:
           logger.error(f"âŒ å®˜æ–¹æ•°æ®åŠ è½½å¤±è´¥: {e}")
           return self._create_simple_datasets()
   
   def _create_simple_datasets(self):
       """ç®€åŒ–æ•°æ®é›†ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
       from src.data.fixed_dataset import FixedNTLBGDataset
       
       train_dataset = FixedNTLBGDataset(
           "/workspace/NTLBG-LLM/data",
           split="train",
           max_frames=32
       )
       
       val_dataset = FixedNTLBGDataset(
           "/workspace/NTLBG-LLM/data",
           split="val", 
           max_frames=32
       )
       
       return train_dataset, val_dataset
   
   def collate_fn(self, batch):
       """å¤„ç†æ‰¹æ¬¡æ•°æ®"""
       if HAS_OFFICIAL_LOADER and hasattr(batch[0], 'get'):
           # å®˜æ–¹æ•°æ®æ ¼å¼
           return self._collate_official(batch)
       else:
           # ç®€åŒ–æ•°æ®æ ¼å¼
           return self._collate_simple(batch)
   
   def _collate_official(self, batch):
       """å¤„ç†å®˜æ–¹LongVideoBenchæ•°æ®"""
       processed_batch = []
       
       for sample in batch:
           try:
               inputs = sample.get("inputs", [])
               
               # åˆ†ç¦»è§†é¢‘å¸§å’Œæ–‡æœ¬
               video_frames = []
               text_parts = []
               
               for item in inputs:
                   if hasattr(item, 'size'):  # PIL Image
                       video_frames.append(item)
                   elif isinstance(item, str):
                       text_parts.append(item)
               
               # æ„é€ æ–‡æœ¬
               combined_text = " ".join(text_parts)
               question = sample.get('question', '')
               if question:
                   combined_text += f" Question: {question}"
               
               # å¤„ç†ç­”æ¡ˆ
               answer = sample.get('answer', 0)
               if isinstance(answer, (list, tuple)):
                   answer = answer[0] if len(answer) > 0 else 0
               
               processed_batch.append({
                   'video_frames': video_frames,
                   'text': combined_text,
                   'answer': int(answer),
                   'question': question
               })
               
           except Exception as e:
               logger.warning(f"âš ï¸ å¤„ç†æ ·æœ¬å¤±è´¥: {e}")
               # æ·»åŠ ç©ºæ ·æœ¬
               processed_batch.append({
                   'video_frames': [],
                   'text': "empty sample",
                   'answer': 0,
                   'question': ""
               })
       
       return processed_batch
   
   def _collate_simple(self, batch):
       """å¤„ç†ç®€åŒ–æ•°æ®"""
       processed_batch = []
       for sample in batch:
           processed_batch.append({
               'video_frames': sample.get('video_frames', []),
               'text': f"{sample.get('text', '')} {sample.get('question', '')}",
               'answer': sample.get('answer', 0),
               'question': sample.get('question', '')
           })
       return processed_batch
   
   def train_epoch(self):
       """è®­ç»ƒä¸€ä¸ªepoch"""
       self.model.train()
       total_loss = 0
       total_task_loss = 0
       total_ntlbg_loss = 0
       num_batches = 0
       
       # åˆ›å»ºæ•°æ®åŠ è½½å™¨
       train_loader = DataLoader(
           self.train_dataset,
           batch_size=self.config.get('batch_size', 2),
           shuffle=True,
           num_workers=0,
           collate_fn=self.collate_fn
       )
       
       progress_bar = tqdm(train_loader, desc="è®­ç»ƒä¸­")
       
       for batch in progress_bar:
           self.optimizer.zero_grad()
           
           batch_loss = 0
           batch_task_loss = 0
           batch_ntlbg_loss = 0
           valid_samples = 0
           
           for sample in batch:
               try:
                   # å‡†å¤‡æ ‡ç­¾
                   answer = sample['answer']
                   labels = torch.tensor([answer], device=self.device, dtype=torch.long)
                   
                   # å‰å‘ä¼ æ’­
                   outputs = self.model(
                       video_frames=sample['video_frames'],
                       text_input=sample['text'],
                       labels=labels,
                       return_loss=True
                   )
                   
                   if 'loss' in outputs:
                       loss = outputs['loss']
                       task_loss = outputs.get('task_loss', loss)
                       ntlbg_loss = outputs.get('ntlbg_loss', torch.tensor(0.0))
                       
                       batch_loss += loss
                       batch_task_loss += task_loss
                       batch_ntlbg_loss += ntlbg_loss
                       valid_samples += 1
                   
               except Exception as e:
                   logger.warning(f"âš ï¸ è®­ç»ƒæ ·æœ¬å¤±è´¥: {e}")
                   continue
           
           # åªæœ‰å½“æœ‰æœ‰æ•ˆæ ·æœ¬æ—¶æ‰æ›´æ–°
           if valid_samples > 0:
               avg_loss = batch_loss / valid_samples
               avg_task_loss = batch_task_loss / valid_samples
               avg_ntlbg_loss = batch_ntlbg_loss / valid_samples
               
               # åå‘ä¼ æ’­
               avg_loss.backward()
               
               # æ¢¯åº¦è£å‰ª
               torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
               
               # ä¼˜åŒ–æ­¥éª¤
               self.optimizer.step()
               
               total_loss += avg_loss.item()
               total_task_loss += avg_task_loss.item()
               total_ntlbg_loss += avg_ntlbg_loss.item()
               num_batches += 1
               
               progress_bar.set_postfix({
                   'loss': f'{avg_loss.item():.4f}',
                   'task': f'{avg_task_loss.item():.4f}',
                   'ntlbg': f'{avg_ntlbg_loss.item():.4f}'
               })
       
       return {
           'total_loss': total_loss / max(num_batches, 1),
           'task_loss': total_task_loss / max(num_batches, 1),
           'ntlbg_loss': total_ntlbg_loss / max(num_batches, 1)
       }
   
   def evaluate(self):
       """è¯„ä¼°æ¨¡å‹"""
       self.model.eval()
       correct_predictions = 0
       total_predictions = 0
       
       # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨
       val_loader = DataLoader(
           self.val_dataset,
           batch_size=self.config.get('batch_size', 2),
           shuffle=False,
           num_workers=0,
           collate_fn=self.collate_fn
       )
       
       with torch.no_grad():
           for batch in tqdm(val_loader, desc="è¯„ä¼°ä¸­"):
               for sample in batch:
                   try:
                       # å‰å‘ä¼ æ’­
                       outputs = self.model(
                           video_frames=sample['video_frames'],
                           text_input=sample['text'],
                           return_loss=False
                       )
                       
                       # é¢„æµ‹ç­”æ¡ˆ
                       if 'classification_logits' in outputs:
                           pred = torch.argmax(outputs['classification_logits'], dim=-1).cpu().item()
                       else:
                           pred = torch.argmax(outputs['logits'], dim=-1).cpu().item()
                       
                       # è¯„ä¼°æ­£ç¡®æ€§
                       target = sample['answer']
                       if pred == target:
                           correct_predictions += 1
                       
                       total_predictions += 1
                       
                   except Exception as e:
                       logger.warning(f"âš ï¸ è¯„ä¼°æ ·æœ¬å¤±è´¥: {e}")
                       total_predictions += 1
       
       accuracy = correct_predictions / max(total_predictions, 1)
       return accuracy
   
   def train(self):
       """å®Œæ•´è®­ç»ƒæµç¨‹"""
       logger.info("ğŸš€ å¼€å§‹ä¿®å¤ç‰ˆNTLBG-LLMè®­ç»ƒ")
       logger.info("=" * 60)
       
       num_epochs = self.config.get('num_epochs', 5)
       best_accuracy = 0
       patience = 3
       patience_counter = 0
       
       results = {
           'train_losses': [],
           'task_losses': [],
           'ntlbg_losses': [],
           'val_accuracies': [],
           'best_accuracy': 0,
           'training_time': datetime.now().isoformat(),
           'config': self.config
       }
       
       for epoch in range(num_epochs):
           logger.info(f"\nğŸ“š Epoch {epoch+1}/{num_epochs}")
           logger.info(f"   å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.2e}")
           logger.info("-" * 40)
           
           # è®­ç»ƒ
           train_metrics = self.train_epoch()
           logger.info(f"   âœ… è®­ç»ƒæŸå¤±: {train_metrics['total_loss']:.4f}")
           logger.info(f"      ä»»åŠ¡æŸå¤±: {train_metrics['task_loss']:.4f}")
           logger.info(f"      NTLBGæŸå¤±: {train_metrics['ntlbg_loss']:.4f}")
           
           # è¯„ä¼°
           val_accuracy = self.evaluate()
           logger.info(f"   âœ… éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")
           
           # å­¦ä¹ ç‡è°ƒæ•´
           self.scheduler.step()
           
           # è®°å½•ç»“æœ
           results['train_losses'].append(train_metrics['total_loss'])
           results['task_losses'].append(train_metrics['task_loss'])
           results['ntlbg_losses'].append(train_metrics['ntlbg_loss'])
           results['val_accuracies'].append(val_accuracy)
           
           # æ—©åœå’Œæ¨¡å‹ä¿å­˜
           if val_accuracy > best_accuracy:
               best_accuracy = val_accuracy
               results['best_accuracy'] = best_accuracy
               patience_counter = 0
               
               # ä¿å­˜æœ€ä½³æ¨¡å‹
               os.makedirs("outputs/models", exist_ok=True)
               torch.save(self.model.state_dict(), "outputs/models/best_fixed_ntlbg_llm.pth")
               logger.info(f"   ğŸ¯ ä¿å­˜æœ€ä½³æ¨¡å‹ (å‡†ç¡®ç‡: {best_accuracy:.4f})")
           else:
               patience_counter += 1
               if patience_counter >= patience:
                   logger.info(f"   ğŸ›‘ æ—©åœï¼š{patience}ä¸ªepochæ²¡æœ‰æ”¹è¿›")
                   break
       
       # ä¿å­˜è®­ç»ƒç»“æœ
       os.makedirs("outputs", exist_ok=True)
       with open("outputs/fixed_training_results.json", "w") as f:
           json.dump(results, f, indent=2)
       
       logger.info(f"\nğŸ‰ ä¿®å¤ç‰ˆè®­ç»ƒå®Œæˆ!")
       logger.info(f"   ğŸ† æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.4f}")
       logger.info(f"   ğŸ“ æ¨¡å‹ä¿å­˜: outputs/models/best_fixed_ntlbg_llm.pth")
       
       return results


def main():
   """ä¸»å‡½æ•°"""
   logger.info("ğŸ¯ ä¿®å¤ç‰ˆNTLBG-LLMè®­ç»ƒå¼€å§‹")
   logger.info("=" * 80)
   
   config = {
       'batch_size': 2,  # é€‚ä¸­çš„æ‰¹æ¬¡å¤§å°
       'learning_rate': 5e-5,  # åˆé€‚çš„å­¦ä¹ ç‡
       'num_epochs': 8,
       'num_representatives': 6,
       'weight_decay': 0.01
   }
   
   logger.info("âš™ï¸ è®­ç»ƒé…ç½®:")
   for key, value in config.items():
       logger.info(f"   {key}: {value}")
   
   try:
       trainer = FixedTrainer(config)
       results = trainer.train()
       
       logger.info("\nğŸŠ æœ€ç»ˆç»“æœ:")
       logger.info(f"   ğŸ¯ æœ€ä½³å‡†ç¡®ç‡: {results['best_accuracy']:.4f}")
       logger.info(f"   ğŸ“ˆ è®­ç»ƒæŸå¤±è½¨è¿¹: {[f'{loss:.3f}' for loss in results['train_losses'][-3:]]}")
       logger.info(f"   ğŸ“Š éªŒè¯å‡†ç¡®ç‡è½¨è¿¹: {[f'{acc:.3f}' for acc in results['val_accuracies'][-3:]]}")
       
       # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›
       if results['best_accuracy'] > 0.3:
           logger.info(f"   âœ… æ€§èƒ½è‰¯å¥½ï¼è¶…è¿‡éšæœºçŒœæµ‹åŸºçº¿")
       elif results['best_accuracy'] > 0.1:
           logger.info(f"   ğŸ“ˆ æœ‰æ‰€æ”¹è¿›ï¼Œä½†ä»éœ€ä¼˜åŒ–")
       else:
           logger.info(f"   âš ï¸ æ€§èƒ½è¾ƒä½ï¼Œéœ€è¦è°ƒè¯•æ¨¡å‹æˆ–æ•°æ®")
       
       return results
       
   except Exception as e:
       logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
       import traceback
       traceback.print_exc()
       return None


if __name__ == "__main__":
   results = main()
   if results and results['best_accuracy'] > 0.2:
       print("\nğŸ‰ è®­ç»ƒæˆåŠŸï¼å¯ä»¥è¿›è¡Œè¯„ä¼°äº†")
   else:
       print("\nâš ï¸ è®­ç»ƒæ•ˆæœä¸ä½³ï¼Œå»ºè®®æ£€æŸ¥é…ç½®")
   def _create_official_datasets(self):
       """ä½¿ç”¨å®˜æ–¹æ•°æ®åŠ è½½å™¨"""
       try:
           data_path = "/workspace/NTLBG-LLM/data/longvideobench"
           
           # åŠ è½½éªŒè¯é›†æ•°æ®
           val_dataset = LongVideoBenchDataset(
               data_path, 
               "lvb_val.json", 
               max_num_frames=32
           )
           
           # å°†éªŒè¯é›†åˆ†ä¸ºè®­ç»ƒå’ŒéªŒè¯
           total_size = len(val_dataset)
           train_size = int(0.8 * total_size)
           val_size = total_size - train_size
           
           # éšæœºåˆ†å‰²
           indices = torch.randperm(total_size).tolist()
           train_indices = indices[:train_size]
           val_indices = indices[train_size:]
           
           train_dataset = Subset(val_dataset, train_indices)
           val_dataset_subset = Subset(val_dataset, val_indices)
           
           logger.info(f"âœ… ä½¿ç”¨å®˜æ–¹LongVideoBenchæ•°æ®")
           logger.info(f"   åŸå§‹æ•°æ®: {total_size} æ ·æœ¬")
           logger.info(f"   è®­ç»ƒ: {len(train_dataset)} æ ·æœ¬")
           logger.info(f"   éªŒè¯: {len(val_dataset_subset)} æ ·æœ¬")
           
           return train_dataset, val_dataset_subset
           
       except Exception as e:
           logger.error(f"âŒ å®˜æ–¹æ•°æ®åŠ è½½å¤±è´¥: {e}")
           return self._create_simple_datasets()
   
   def _create_simple_datasets(self):
       """ç®€åŒ–æ•°æ®é›†ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
       from src.data.fixed_dataset import FixedNTLBGDataset
       
       train_dataset = FixedNTLBGDataset(
           "/workspace/NTLBG-LLM/data",
           split="train",
           max_frames=32
       )
       
       val_dataset = FixedNTLBGDataset(
           "/workspace/NTLBG-LLM/data",
           split="val", 
           max_frames=32
       )
       
       return train_dataset, val_dataset
   
   def collate_fn(self, batch):
       """å¤„ç†æ‰¹æ¬¡æ•°æ®"""
       if HAS_OFFICIAL_LOADER and hasattr(batch[0], 'get'):
           # å®˜æ–¹æ•°æ®æ ¼å¼
           return self._collate_official(batch)
       else:
           # ç®€åŒ–æ•°æ®æ ¼å¼
           return self._collate_simple(batch)
   
   def _collate_official(self, batch):
       """å¤„ç†å®˜æ–¹LongVideoBenchæ•°æ®"""
       processed_batch = []
       
       for sample in batch:
           try:
               inputs = sample.get("inputs", [])
               
               # åˆ†ç¦»è§†é¢‘å¸§å’Œæ–‡æœ¬
               video_frames = []
               text_parts = []
               
               for item in inputs:
                   if hasattr(item, 'size'):  # PIL Image
                       video_frames.append(item)
                   elif isinstance(item, str):
                       text_parts.append(item)
               
               # æ„é€ æ–‡æœ¬
               combined_text = " ".join(text_parts)
               question = sample.get('question', '')
               if question:
                   combined_text += f" Question: {question}"
               
               # å¤„ç†ç­”æ¡ˆ
               answer = sample.get('answer', 0)
               if isinstance(answer, (list, tuple)):
                   answer = answer[0] if len(answer) > 0 else 0
               
               processed_batch.append({
                   'video_frames': video_frames,
                   'text': combined_text,
                   'answer': int(answer),
                   'question': question
               })
               
           except Exception as e:
               logger.warning(f"âš ï¸ å¤„ç†æ ·æœ¬å¤±è´¥: {e}")
               # æ·»åŠ ç©ºæ ·æœ¬
               processed_batch.append({
                   'video_frames': [],
                   'text': "empty sample",
                   'answer': 0,
                   'question': ""
               })
       
       return processed_batch
   
   def _collate_simple(self, batch):
       """å¤„ç†ç®€åŒ–æ•°æ®"""
       processed_batch = []
       for sample in batch:
           processed_batch.append({
               'video_frames': sample.get('video_frames', []),
               'text': f"{sample.get('text', '')} {sample.get('question', '')}",
               'answer': sample.get('answer', 0),
               'question': sample.get('question', '')
           })
       return processed_batch
   
   def train_epoch(self):
       """è®­ç»ƒä¸€ä¸ªepoch"""
       self.model.train()
       total_loss = 0
       total_task_loss = 0
       total_ntlbg_loss = 0
       num_batches = 0
       
       # åˆ›å»ºæ•°æ®åŠ è½½å™¨
       train_loader = DataLoader(
           self.train_dataset,
           batch_size=self.config.get('batch_size', 2),
           shuffle=True,
           num_workers=0,
           collate_fn=self.collate_fn
       )
       
       progress_bar = tqdm(train_loader, desc="è®­ç»ƒä¸­")
       
       for batch in progress_bar:
           self.optimizer.zero_grad()
           
           batch_loss = 0
           batch_task_loss = 0
           batch_ntlbg_loss = 0
           valid_samples = 0
           
           for sample in batch:
               try:
                   # å‡†å¤‡æ ‡ç­¾
                   answer = sample['answer']
                   labels = torch.tensor([answer], device=self.device, dtype=torch.long)
                   
                   # å‰å‘ä¼ æ’­
                   outputs = self.model(
                       video_frames=sample['video_frames'],
                       text_input=sample['text'],
                       labels=labels,
                       return_loss=True
                   )
                   
                   if 'loss' in outputs:
                       loss = outputs['loss']
                       task_loss = outputs.get('task_loss', loss)
                       ntlbg_loss = outputs.get('ntlbg_loss', torch.tensor(0.0))
                       
                       batch_loss += loss
                       batch_task_loss += task_loss
                       batch_ntlbg_loss += ntlbg_loss
                       valid_samples += 1
                   
               except Exception as e:
                   logger.warning(f"âš ï¸ è®­ç»ƒæ ·æœ¬å¤±è´¥: {e}")
                   continue
           
           # åªæœ‰å½“æœ‰æœ‰æ•ˆæ ·æœ¬æ—¶æ‰æ›´æ–°
           if valid_samples > 0:
               avg_loss = batch_loss / valid_samples
               avg_task_loss = batch_task_loss / valid_samples
               avg_ntlbg_loss = batch_ntlbg_loss / valid_samples
               
               # åå‘ä¼ æ’­
               avg_loss.backward()
               
               # æ¢¯åº¦è£å‰ª
               torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
               
               # ä¼˜åŒ–æ­¥éª¤
               self.optimizer.step()
               
               total_loss += avg_loss.item()
               total_task_loss += avg_task_loss.item()
               total_ntlbg_loss += avg_ntlbg_loss.item()
               num_batches += 1
               
               progress_bar.set_postfix({
                   'loss': f'{avg_loss.item():.4f}',
                   'task': f'{avg_task_loss.item():.4f}',
                   'ntlbg': f'{avg_ntlbg_loss.item():.4f}'
               })
       
       return {
           'total_loss': total_loss / max(num_batches, 1),
           'task_loss': total_task_loss / max(num_batches, 1),
           'ntlbg_loss': total_ntlbg_loss / max(num_batches, 1)
       }
   
   def evaluate(self):
       """è¯„ä¼°æ¨¡å‹"""
       self.model.eval()
       correct_predictions = 0
       total_predictions = 0
       
       # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨
       val_loader = DataLoader(
           self.val_dataset,
           batch_size=self.config.get('batch_size', 2),
           shuffle=False,
           num_workers=0,
           collate_fn=self.collate_fn
       )
       
       with torch.no_grad():
           for batch in tqdm(val_loader, desc="è¯„ä¼°ä¸­"):
               for sample in batch:
                   try:
                       # å‰å‘ä¼ æ’­
                       outputs = self.model(
                           video_frames=sample['video_frames'],
                           text_input=sample['text'],
                           return_loss=False
                       )
                       
                       # é¢„æµ‹ç­”æ¡ˆ
                       if 'classification_logits' in outputs:
                           pred = torch.argmax(outputs['classification_logits'], dim=-1).cpu().item()
                       else:
                           pred = torch.argmax(outputs['logits'], dim=-1).cpu().item()
                       
                       # è¯„ä¼°æ­£ç¡®æ€§
                       target = sample['answer']
                       if pred == target:
                           correct_predictions += 1
                       
                       total_predictions += 1
                       
                   except Exception as e:
                       logger.warning(f"âš ï¸ è¯„ä¼°æ ·æœ¬å¤±è´¥: {e}")
                       total_predictions += 1
       
       accuracy = correct_predictions / max(total_predictions, 1)
       return accuracy
   
   def train(self):
       """å®Œæ•´è®­ç»ƒæµç¨‹"""
       logger.info("ğŸš€ å¼€å§‹ä¿®å¤ç‰ˆNTLBG-LLMè®­ç»ƒ")
       logger.info("=" * 60)
       
       num_epochs = self.config.get('num_epochs', 5)
       best_accuracy = 0
       patience = 3
       patience_counter = 0
       
       results = {
           'train_losses': [],
           'task_losses': [],
           'ntlbg_losses': [],
           'val_accuracies': [],
           'best_accuracy': 0,
           'training_time': datetime.now().isoformat(),
           'config': self.config
       }
       
       for epoch in range(num_epochs):
           logger.info(f"\nğŸ“š Epoch {epoch+1}/{num_epochs}")
           logger.info(f"   å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.2e}")
           logger.info("-" * 40)
           
           # è®­ç»ƒ
           train_metrics = self.train_epoch()
           logger.info(f"   âœ… è®­ç»ƒæŸå¤±: {train_metrics['total_loss']:.4f}")
           logger.info(f"      ä»»åŠ¡æŸå¤±: {train_metrics['task_loss']:.4f}")
           logger.info(f"      NTLBGæŸå¤±: {train_metrics['ntlbg_loss']:.4f}")
           
           # è¯„ä¼°
           val_accuracy = self.evaluate()
           logger.info(f"   âœ… éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f}")
           
           # å­¦ä¹ ç‡è°ƒæ•´
           self.scheduler.step()
           
           # è®°å½•ç»“æœ
           results['train_losses'].append(train_metrics['total_loss'])
           results['task_losses'].append(train_metrics['task_loss'])
           results['ntlbg_losses'].append(train_metrics['ntlbg_loss'])
           results['val_accuracies'].append(val_accuracy)
           
           # æ—©åœå’Œæ¨¡å‹ä¿å­˜
           if val_accuracy > best_accuracy:
               best_accuracy = val_accuracy
               results['best_accuracy'] = best_accuracy
               patience_counter = 0
               
               # ä¿å­˜æœ€ä½³æ¨¡å‹
               os.makedirs("outputs/models", exist_ok=True)
               torch.save(self.model.state_dict(), "outputs/models/best_fixed_ntlbg_llm.pth")
               logger.info(f"   ğŸ¯ ä¿å­˜æœ€ä½³æ¨¡å‹ (å‡†ç¡®ç‡: {best_accuracy:.4f})")
           else:
               patience_counter += 1
               if patience_counter >= patience:
                   logger.info(f"   ğŸ›‘ æ—©åœï¼š{patience}ä¸ªepochæ²¡æœ‰æ”¹è¿›")
                   break
       
       # ä¿å­˜è®­ç»ƒç»“æœ
       os.makedirs("outputs", exist_ok=True)
       with open("outputs/fixed_training_results.json", "w") as f:
           json.dump(results, f, indent=2)
       
       logger.info(f"\nğŸ‰ ä¿®å¤ç‰ˆè®­ç»ƒå®Œæˆ!")
       logger.info(f"   ğŸ† æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.4f}")
       logger.info(f"   ğŸ“ æ¨¡å‹ä¿å­˜: outputs/models/best_fixed_ntlbg_llm.pth")
       
       return results


def main():
   """ä¸»å‡½æ•°"""
   logger.info("ğŸ¯ ä¿®å¤ç‰ˆNTLBG-LLMè®­ç»ƒå¼€å§‹")
   logger.info("=" * 80)
   
   config = {
       'batch_size': 2,  # é€‚ä¸­çš„æ‰¹æ¬¡å¤§å°
       'learning_rate': 5e-5,  # åˆé€‚çš„å­¦ä¹ ç‡
       'num_epochs': 8,
       'num_representatives': 6,
       'weight_decay': 0.01
   }
   
   logger.info("âš™ï¸ è®­ç»ƒé…ç½®:")
   for key, value in config.items():
       logger.info(f"   {key}: {value}")
   
   try:
       trainer = FixedTrainer(config)
       results = trainer.train()
       
       logger.info("\nğŸŠ æœ€ç»ˆç»“æœ:")
       logger.info(f"   ğŸ¯ æœ€ä½³å‡†ç¡®ç‡: {results['best_accuracy']:.4f}")
       logger.info(f"   ğŸ“ˆ è®­ç»ƒæŸå¤±è½¨è¿¹: {[f'{loss:.3f}' for loss in results['train_losses'][-3:]]}")
       logger.info(f"   ğŸ“Š éªŒè¯å‡†ç¡®ç‡è½¨è¿¹: {[f'{acc:.3f}' for acc in results['val_accuracies'][-3:]]}")
       
       # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹è¿›
       if results['best_accuracy'] > 0.3:
           logger.info(f"   âœ… æ€§èƒ½è‰¯å¥½ï¼è¶…è¿‡éšæœºçŒœæµ‹åŸºçº¿")
       elif results['best_accuracy'] > 0.1:
           logger.info(f"   ğŸ“ˆ æœ‰æ‰€æ”¹è¿›ï¼Œä½†ä»éœ€ä¼˜åŒ–")
       else:
           logger.info(f"   âš ï¸ æ€§èƒ½è¾ƒä½ï¼Œéœ€è¦è°ƒè¯•æ¨¡å‹æˆ–æ•°æ®")
       
       return results
       
   except Exception as e:
       logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
       import traceback
       traceback.print_exc()
       return None


if __name__ == "__main__":
   results = main()
   if results and results['best_accuracy'] > 0.2:
       print("\nğŸ‰ è®­ç»ƒæˆåŠŸï¼å¯ä»¥è¿›è¡Œè¯„ä¼°äº†")
   else:
       print("\nâš ï¸ è®­ç»ƒæ•ˆæœä¸ä½³ï¼Œå»ºè®®æ£€æŸ¥é…ç½®")
