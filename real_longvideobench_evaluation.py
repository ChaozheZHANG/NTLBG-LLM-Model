"""
çœŸæ­£çš„LongVideoBenchè¯„ä¼°è„šæœ¬ - å¯¹æ ‡å®˜æ–¹æ’è¡Œæ¦œ
"""
import torch
import torch.nn.functional as F
import os
import json
import sys
import numpy as np
from tqdm import tqdm
from datetime import datetime
import matplotlib.pyplot as plt
from collections import defaultdict

# æ·»åŠ è·¯å¾„
sys.path.append('/workspace/NTLBG-LLM')
sys.path.append('/workspace/NTLBG-LLM/LongVideoBench_official')

# å¯¼å…¥å®˜æ–¹æ•°æ®åŠ è½½å™¨
try:
    from longvideobench import LongVideoBenchDataset
    print("âœ… æˆåŠŸå¯¼å…¥å®˜æ–¹LongVideoBenchæ•°æ®åŠ è½½å™¨")
except ImportError as e:
    print(f"âŒ æ— æ³•å¯¼å…¥å®˜æ–¹æ•°æ®åŠ è½½å™¨: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…å®˜æ–¹LongVideoBenchåŒ…")
    sys.exit(1)

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å‹
from create_real_ntlbg_llm import RealNTLBGLLM

class LongVideoBenchEvaluator:
    def __init__(self, model_path=None):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆ›å»ºæ¨¡å‹
        config = {'num_representatives': 6}
        self.model = RealNTLBGLLM(config).to(self.device)
        
        # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
        if model_path and os.path.exists(model_path):
            print(f"ğŸ“¥ åŠ è½½æ¨¡å‹æƒé‡: {model_path}")
            self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        else:
            print("âš ï¸ ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹æƒé‡")
        
        self.model.eval()
        
        # åˆ›å»ºæ•°æ®é›†
        data_path = "/workspace/NTLBG-LLM/data/longvideobench"
        
        try:
            # éªŒè¯é›†
            self.val_dataset = LongVideoBenchDataset(
                data_path, 
                "lvb_val.json", 
                max_num_frames=32  # å¢åŠ å¸§æ•°ä»¥æ›´å¥½åœ°ç†è§£é•¿è§†é¢‘
            )
            print(f"âœ… éªŒè¯é›†åŠ è½½: {len(self.val_dataset)} æ ·æœ¬")
            
            # æµ‹è¯•é›†ï¼ˆæ²¡æœ‰ç­”æ¡ˆï¼‰
            self.test_dataset = LongVideoBenchDataset(
                data_path,
                "lvb_test_wo_gt.json",
                max_num_frames=32
            )
            print(f"âœ… æµ‹è¯•é›†åŠ è½½: {len(self.test_dataset)} æ ·æœ¬")
            
        except Exception as e:
            print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
            # åˆ›å»ºç©ºçš„å¤‡é€‰æ•°æ®é›†
            self.val_dataset = None
            self.test_dataset = None
    
    def process_sample(self, sample):
        """å¤„ç†å•ä¸ªæ ·æœ¬"""
        try:
            inputs = sample.get("inputs", [])
            
            # åˆ†ç¦»è§†é¢‘å¸§å’Œæ–‡æœ¬
            video_frames = []
            text_parts = []
            
            for item in inputs:
                if hasattr(item, 'size'):  # PIL Image
                    video_frames.append(item)
                elif isinstance(item, str):
                    text_parts.append(item)
            
            # ç»„åˆæ–‡æœ¬
            combined_text = " ".join(text_parts)
            
            return video_frames, combined_text
            
        except Exception as e:
            print(f"âŒ æ ·æœ¬å¤„ç†å¤±è´¥: {e}")
            return [], ""
    
    def predict_answer(self, video_frames, text_input):
        """é¢„æµ‹ç­”æ¡ˆ"""
        try:
            with torch.no_grad():
                outputs = self.model(
                    video_frames=video_frames,
                    text_input=text_input
                )
                
                logits = outputs['logits']
                
                # å¯¹äº4é€‰æ‹©é¢˜ï¼Œå–å‰4ä¸ªlogits
                if logits.shape[-1] >= 4:
                    choice_logits = logits[:, :4]
                    pred = torch.argmax(choice_logits, dim=-1).cpu().item()
                    confidence = torch.softmax(choice_logits, dim=-1).max().cpu().item()
                else:
                    # å¤‡é€‰ï¼šéšæœºé¢„æµ‹
                    pred = np.random.randint(0, 4)
                    confidence = 0.25
                
                return pred, confidence
                
        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            return np.random.randint(0, 4), 0.25
    
    def evaluate_validation_set(self, max_samples=None):
        """è¯„ä¼°éªŒè¯é›†"""
        print("ğŸ§ª è¯„ä¼°éªŒè¯é›†...")
        
        if self.val_dataset is None:
            print("âŒ éªŒè¯é›†ä¸å¯ç”¨")
            return {}
        
        # é™åˆ¶æ ·æœ¬æ•°é‡ä»¥åŠ å¿«è¯„ä¼°
        total_samples = len(self.val_dataset)
        if max_samples:
            total_samples = min(total_samples, max_samples)
        
        results = {
            'total': 0,
            'correct': 0,
            'by_duration': defaultdict(lambda: {'total': 0, 'correct': 0}),
            'predictions': [],
            'confidences': []
        }
        
        progress_bar = tqdm(range(total_samples), desc="è¯„ä¼°éªŒè¯é›†")
        
        for i in progress_bar:
            try:
                sample = self.val_dataset[i]
                
                # å¤„ç†æ ·æœ¬
                video_frames, text_input = self.process_sample(sample)
                
                # é¢„æµ‹
                pred, confidence = self.predict_answer(video_frames, text_input)
                
                # è·å–çœŸå®ç­”æ¡ˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                gt_answer = sample.get('answer', None)
                if gt_answer is not None:
                    if isinstance(gt_answer, (list, tuple)):
                        gt_answer = gt_answer[0] if len(gt_answer) > 0 else 0
                    
                    # ç»Ÿè®¡ç»“æœ
                    results['total'] += 1
                    if pred == gt_answer:
                        results['correct'] += 1
                    
                    # æŒ‰è§†é¢‘æ—¶é•¿åˆ†ç±»ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                    duration = sample.get('duration', 'unknown')
                    results['by_duration'][duration]['total'] += 1
                    if pred == gt_answer:
                        results['by_duration'][duration]['correct'] += 1
                
                results['predictions'].append(pred)
                results['confidences'].append(confidence)
                
                # æ›´æ–°è¿›åº¦æ¡
                if results['total'] > 0:
                    accuracy = results['correct'] / results['total']
                    progress_bar.set_postfix({
                        'accuracy': f'{accuracy:.4f}',
                        'samples': f"{results['total']}/{total_samples}"
                    })
                
            except Exception as e:
                print(f"âŒ æ ·æœ¬{i}è¯„ä¼°å¤±è´¥: {e}")
                continue
        
        # è®¡ç®—æœ€ç»ˆç»“æœ
        overall_accuracy = results['correct'] / max(results['total'], 1)
        
        print(f"\nğŸ“Š éªŒè¯é›†ç»“æœ:")
        print(f"   æ€»å‡†ç¡®ç‡: {overall_accuracy:.4f} ({results['correct']}/{results['total']})")
        
        # æŒ‰æ—¶é•¿åˆ†ç±»çš„ç»“æœ
        print(f"   æŒ‰è§†é¢‘æ—¶é•¿åˆ†ç±»:")
        for duration, stats in results['by_duration'].items():
            if stats['total'] > 0:
                acc = stats['correct'] / stats['total']
                print(f"     {duration}: {acc:.4f} ({stats['correct']}/{stats['total']})")
        
        return results
    
    def generate_test_predictions(self, output_file="test_predictions.json", max_samples=None):
        """ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹ï¼ˆç”¨äºæäº¤ï¼‰"""
        print("ğŸ”® ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹...")
        
        if self.test_dataset is None:
            print("âŒ æµ‹è¯•é›†ä¸å¯ç”¨")
            return
        
        total_samples = len(self.test_dataset)
        if max_samples:
            total_samples = min(total_samples, max_samples)
        
        predictions = []
        
        progress_bar = tqdm(range(total_samples), desc="é¢„æµ‹æµ‹è¯•é›†")
        
        for i in progress_bar:
            try:
                sample = self.test_dataset[i]
                
                # å¤„ç†æ ·æœ¬
                video_frames, text_input = self.process_sample(sample)
                
                # é¢„æµ‹
                pred, confidence = self.predict_answer(video_frames, text_input)
                
                # ä¿å­˜é¢„æµ‹ç»“æœ
                prediction = {
                    'sample_id': i,
                    'prediction': pred,
                    'confidence': confidence,
                    'question_id': sample.get('question_id', f'test_{i}')
                }
                predictions.append(prediction)
                
            except Exception as e:
                print(f"âŒ æµ‹è¯•æ ·æœ¬{i}é¢„æµ‹å¤±è´¥: {e}")
                # æ·»åŠ éšæœºé¢„æµ‹
                predictions.append({
                    'sample_id': i,
                    'prediction': np.random.randint(0, 4),
                    'confidence': 0.25,
                    'question_id': f'test_{i}'
                })
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        os.makedirs("outputs", exist_ok=True)
        with open(f"outputs/{output_file}", 'w') as f:
            json.dump(predictions, f, indent=2)
        
        print(f"âœ… æµ‹è¯•é›†é¢„æµ‹ä¿å­˜: outputs/{output_file}")
        return predictions
    
    def create_comparison_report(self, val_results):
        """åˆ›å»ºä¸SOTAæ¨¡å‹çš„å¯¹æ¯”æŠ¥å‘Š"""
        print("ğŸ“Š åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š...")
        
        # LongVideoBenchæ’è¡Œæ¦œæ•°æ®ï¼ˆä»æ‚¨æä¾›çš„æ–‡æ¡£ï¼‰
        sota_results = {
            'GPT-4o (0513)': 66.7,
            'Aria': 65.0,
            'LLaVA-Video-72B-Qwen2': 64.9,
            'Gemini-1.5-Pro': 64.4,
            'LLaVA-OneVision-QWen2-72B-OV': 63.2,
            'LLaVA-Video-7B-Qwen2': 62.7,
            'Gemini-1.5-Flash': 62.4,
            'GPT-4-Turbo': 60.7,
            'InternVL2-40B': 60.6,
            'GPT-4o-mini': 58.8,
            'Random Baseline': 25.0  # 4é€‰æ‹©é¢˜çš„éšæœºåŸºçº¿
        }
        
        # æˆ‘ä»¬çš„ç»“æœ
        our_accuracy = (val_results['correct'] / max(val_results['total'], 1)) * 100
        
        print(f"\nğŸ† LongVideoBenchæ’è¡Œæ¦œå¯¹æ¯”:")
        print(f"{'='*60}")
        print(f"{'æ¨¡å‹':<30} {'éªŒè¯é›†å‡†ç¡®ç‡ (%)':<15}")
        print(f"{'-'*60}")
        
        # æ·»åŠ æˆ‘ä»¬çš„æ¨¡å‹åˆ°æ’å
        all_results = sota_results.copy()
        all_results['NTLBG-LLM (Ours)'] = our_accuracy
        
        # æŒ‰å‡†ç¡®ç‡æ’åº
        sorted_results = sorted(all_results.items(), key=lambda x: x[1], reverse=True)
        
        our_rank = None
        for rank, (model, acc) in enumerate(sorted_results, 1):
            if model == 'NTLBG-LLM (Ours)':
                print(f"{model:<30} {acc:<15.1f} â­ (ç¬¬{rank}å)")
                our_rank = rank
            else:
                print(f"{model:<30} {acc:<15.1f}")
        
        print(f"{'='*60}")
        print(f"ğŸ¯ NTLBG-LLMæ’å: ç¬¬{our_rank}å / {len(sorted_results)}å")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report = {
            'evaluation_time': datetime.now().isoformat(),
            'our_model': {
                'name': 'NTLBG-LLM (Ours)',
                'accuracy': our_accuracy,
                'rank': our_rank,
                'total_samples': val_results['total'],
                'correct_predictions': val_results['correct']
            },
            'sota_comparison': sorted_results,
            'analysis': {
                'above_random': our_accuracy > 25.0,
                'competitive': our_accuracy > 40.0,
                'sota_level': our_accuracy > 60.0
            }
        }
        
        with open("outputs/longvideobench_comparison.json", "w") as f:
            json.dump(report, f, indent=2)
        
        return report

def main():
    print("ğŸ¯ LongVideoBenchçœŸå®è¯„ä¼°å¼€å§‹")
    print("=" * 60)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    model_path = "outputs/models/best_ntlbg_llm.pth"
    evaluator = LongVideoBenchEvaluator(model_path)
    
    # è¯„ä¼°éªŒè¯é›†ï¼ˆé™åˆ¶æ ·æœ¬æ•°é‡ä»¥åŠ å¿«è¯„ä¼°ï¼‰
    val_results = evaluator.evaluate_validation_set(max_samples=200)
    
    if val_results and val_results['total'] > 0:
        # åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š
        report = evaluator.create_comparison_report(val_results)
        
        print(f"\nğŸŠ è¯„ä¼°å®Œæˆ!")
        print(f"   ğŸ“Š å‡†ç¡®ç‡: {report['our_model']['accuracy']:.2f}%")
        print(f"   ğŸ† æ’å: ç¬¬{report['our_model']['rank']}å")
        
        if report['analysis']['sota_level']:
            print(f"   ğŸ”¥ è¾¾åˆ°SOTAæ°´å¹³ï¼")
        elif report['analysis']['competitive']:
            print(f"   âœ… å…·æœ‰ç«äº‰åŠ›ï¼")
        elif report['analysis']['above_random']:
            print(f"   ğŸ“ˆ è¶…è¿‡éšæœºåŸºçº¿ï¼")
        else:
            print(f"   âš ï¸ éœ€è¦æ”¹è¿›...")
    
    # ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹ï¼ˆå¯é€‰ï¼‰
    print(f"\nğŸ”® ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹...")
    evaluator.generate_test_predictions(max_samples=100)
    
    print(f"\nâœ… è¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜åœ¨ outputs/ ç›®å½•")

if __name__ == "__main__":
    main()
