# NTLBG-LLM
**"Statistical Representative Point Sampling for Efficient Long Video Understanding"**

## ğŸ“– é¡¹ç›®ä»‹ç»

NTLBG-LLMæ˜¯ä¸€ä¸ªåŸºäºç»Ÿè®¡å­¦ä»£è¡¨ç‚¹ç†è®ºçš„é•¿è§†é¢‘ç†è§£æ¨¡å‹ï¼Œä¸“é—¨ç”¨äºè§£å†³é•¿è§†é¢‘ä¸­çš„å…³é”®å¸§é€‰æ‹©å’Œè§†é¢‘é—®ç­”ä»»åŠ¡ã€‚è¯¥æ¨¡å‹å°†å¤šå…ƒæ­£æ€åˆ†å¸ƒçš„ä»£è¡¨ç‚¹ç†è®ºåº”ç”¨åˆ°è§†é¢‘ç†è§£ä¸­ï¼Œé€šè¿‡NTLBGï¼ˆNovel Temporal Long-form Best-view Groundingï¼‰çº¦æŸæ¥é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„è§†é¢‘å¸§ã€‚

## ğŸ¯ æ ¸å¿ƒåˆ›æ–°ç‚¹

1. **ç»Ÿè®¡å­¦ä»£è¡¨ç‚¹ç†è®º**ï¼šåŸºäºå¤šå…ƒæ­£æ€åˆ†å¸ƒçš„ä»£è¡¨ç‚¹é€‰æ‹©æ–¹æ³•ï¼Œç¡®ä¿é€‰æ‹©çš„å¸§åœ¨ç»Ÿè®¡ä¸Šæœ€ä¼˜
2. **å¯Œä»£è¡¨ç‚¹æ„é€ **ï¼šä¸ºæ¯ä¸ªä»£è¡¨ç‚¹è¡¥å……æ—¶ç©ºä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè§£å†³"ç‚¹å›¾åŠ¨æ€å˜åŒ–"çš„å¯¹é½é—®é¢˜
3. **NTLBGçº¦æŸæŸå¤±**ï¼šå°†ç»Ÿè®¡ç†è®ºç›´æ¥èå…¥æŸå¤±å‡½æ•°ï¼Œç¡®ä¿ä»£è¡¨ç‚¹åœ¨åŒä¸€ç­‰é«˜æ¤­çƒé¢ä¸Š
4. **ç«¯åˆ°ç«¯è®­ç»ƒ**ï¼šå°†ä»£è¡¨ç‚¹é€‰æ‹©ä¸LLMè®­ç»ƒç´§å¯†ç»“åˆï¼Œå®ç°è”åˆä¼˜åŒ–

## ğŸ“ é¡¹ç›®ç»“æ„

```
NTLBG-LLM/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ ntlbg_attention.py      # NTLBGæ³¨æ„åŠ›æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ rich_points.py          # å¯Œä»£è¡¨ç‚¹æ„é€ å™¨
â”‚   â”‚   â”œâ”€â”€ ntlbg_llm.py           # ä¸»æ¨¡å‹æ¶æ„
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ video_loader.py        # è§†é¢‘æ•°æ®åŠ è½½
â”‚   â”‚   â”œâ”€â”€ datasets.py            # æ•°æ®é›†å°è£…
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ losses.py              # å¤šä»»åŠ¡æŸå¤±å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ trainer.py             # è®­ç»ƒå¾ªç¯
â”‚   â”‚   â”œâ”€â”€ scheduler.py           # æŸå¤±æƒé‡è°ƒåº¦
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ metrics.py             # è¯„ä¼°æŒ‡æ ‡
â”‚   â”‚   â”œâ”€â”€ visualizer.py          # ç»“æœå¯è§†åŒ–
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ longvideo_config.json      # é•¿è§†é¢‘é…ç½®
â”œâ”€â”€ experiments/                   # å®éªŒè„šæœ¬
â”‚   â”œâ”€â”€ train_experiment.py        # è®­ç»ƒå®éªŒ
â”‚   â”œâ”€â”€ comparison_experiment.py   # å¯¹æ¯”å®éªŒ
â”‚   â””â”€â”€ evaluate_experiment.py     # è¯„ä¼°å®éªŒ
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_ntlbg.py            # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ demo.py                   # æ¼”ç¤ºè„šæœ¬
â”‚   â””â”€â”€ longvideo_config_manager.py # é…ç½®ç®¡ç†å™¨
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.jsonl               # è®­ç»ƒæ•°æ®
â”‚   â”œâ”€â”€ val.jsonl                 # éªŒè¯æ•°æ®
â”‚   â”œâ”€â”€ test.jsonl                # æµ‹è¯•æ•°æ®
â”‚   â””â”€â”€ videos/                   # è§†é¢‘æ–‡ä»¶ç›®å½•
â”œâ”€â”€ results/                      # ç»“æœè¾“å‡º
â”œâ”€â”€ Makefile                      # é¡¹ç›®ç®¡ç†
â””â”€â”€ README.md
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ä½¿ç”¨é…ç½®ç®¡ç†å™¨ï¼ˆæ¨èï¼‰
```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/your-username/NTLBG-LLM.git
cd NTLBG-LLM

# ç”Ÿæˆé•¿è§†é¢‘é…ç½®
python scripts/longvideo_config_manager.py --action generate --category long --gpu A100_40GB

# è¿è¡Œè®­ç»ƒ
python scripts/train_ntlbg.py --config configs/longvideo_config.json

# è¿è¡Œæ¼”ç¤º
python scripts/demo.py --config configs/longvideo_config.json
```

### æ‰‹åŠ¨å®‰è£…
```bash
# åˆ›å»ºcondaç¯å¢ƒ
conda create -n ntlbg-llm python=3.9
conda activate ntlbg-llm

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

## ğŸ¬ è¿è¡Œæ¼”ç¤º

```bash
# è¿è¡Œå®Œæ•´æ¼”ç¤º
python scripts/demo.py --config configs/longvideo_config.json

# ä¿å­˜å¯è§†åŒ–ç»“æœ
python scripts/demo.py --config configs/longvideo_config.json --save_plots
```

æ¼”ç¤ºåŒ…å«ï¼š
- NTLBGæ³¨æ„åŠ›æœºåˆ¶å±•ç¤º
- å¯Œä»£è¡¨ç‚¹æ„é€ æ¼”ç¤º
- å®Œæ•´æ¨¡å‹æ¨ç†æµ‹è¯•
- ä»£è¡¨ç‚¹é€‰æ‹©å¯è§†åŒ–
- æ€§èƒ½åŸºå‡†æµ‹è¯•

## ğŸ”§ é…ç½®è¯´æ˜

ä¸»è¦é…ç½®æ–‡ä»¶ä½äº `configs/longvideo_config.json`ï¼Œæ”¯æŒå¤šç§é•¿è§†é¢‘é…ç½®ï¼š

### é…ç½®çº§åˆ«

| çº§åˆ« | è§†é¢‘æ—¶é•¿ | ä»£è¡¨ç‚¹æ•°é‡ | æœ€å¤§å¸§æ•° | æ‰¹æ¬¡å¤§å° | é€‚ç”¨GPU |
|------|----------|------------|----------|----------|---------|
| **moderate_long** | 5-10åˆ†é’Ÿ | 256 | 2048 | 2 | V100 32GB |
| **long** | 10-20åˆ†é’Ÿ | 512 | 4096 | 1 | A100 40GB |
| **very_long** | 20åˆ†é’Ÿ+ | 1024 | 8192 | 1 | A100 80GB |

### æ ¸å¿ƒå‚æ•°

```json
{
  "video_config": {
    "num_representatives": 512,      # ä»£è¡¨ç‚¹æ•°é‡
    "max_frames": 4096,              # æœ€å¤§è§†é¢‘å¸§æ•°
    "coverage_ratio": 0.125,         # ä»£è¡¨ç‚¹è¦†ç›–ç‡
    "frame_resolution": [224, 224]   # å¸§åˆ†è¾¨ç‡
  },
  "model_config": {
    "base_model": "Qwen/Qwen2-VL-7B-Instruct",
    "ntlbg_hidden_size": 4096,
    "ntlbg_use_flash_attention": true
  },
  "training_config": {
    "batch_size": 1,
    "gradient_accumulation_steps": 32,
    "learning_rate": 5e-5,
    "max_steps": 10000
  }
}
```

## ğŸ“‹ æ•°æ®æ ¼å¼

### è§†é¢‘é—®ç­”æ•°æ® (JSONLæ ¼å¼)
```json
{
  "id": "sample_001",
  "video_id": "video_001.mp4",
  "question": "What is the person doing in this video?",
  "answer": "The person is walking down the street.",
  "answer_type": "action",
  "duration": 15.2,
  "metadata": {
    "scene": "street",
    "activity": "walking"
  }
}
```

### æ”¯æŒçš„è§†é¢‘æ ¼å¼
- MP4 (.mp4)
- AVI (.avi)
- MOV (.mov)
- MKV (.mkv)
- WebM (.webm)

## ğŸ§ª æµ‹è¯•å’ŒéªŒè¯

```bash
# éªŒè¯é…ç½®
python scripts/longvideo_config_manager.py --action validate --config configs/longvideo_config.json

# åˆ†æå†…å­˜éœ€æ±‚
python scripts/longvideo_config_manager.py --action analyze

# ä¼˜åŒ–é…ç½®
python scripts/longvideo_config_manager.py --action optimize --config configs/longvideo_config.json --gpu V100_32GB
```

## ğŸ“ˆ æ¨¡å‹æ€§èƒ½

### é•¿è§†é¢‘ç†è§£æ€§èƒ½
- **LongVideoBench**: â‰¥70%
- **Video-MME**: â‰¥65%
- **MLVU**: â‰¥68%

### æ•ˆç‡æŒ‡æ ‡
- **æ¨ç†åŠ é€Ÿ**: 2.5x vs å‡åŒ€é‡‡æ ·
- **å†…å­˜èŠ‚çœ**: 40% vs å…¨å¸§å¤„ç†
- **è®­ç»ƒæ—¶é—´**: <100 GPUå°æ—¶

### ä»£è¡¨ç‚¹é€‰æ‹©æ•ˆæœ
- NTLBGçº¦æŸæŸå¤±: 0.0023
- æ—¶åºåˆ†å¸ƒå‡åŒ€æ€§: 0.95
- ä¿¡æ¯ä¿æŒæŸå¤±: 0.0015

---

# æŠ€æœ¯ç»†èŠ‚

## 1. NTLBG-LLMèåˆçš„æ•°å­¦æ¡†æ¶

### 1.1 é—®é¢˜å»ºæ¨¡

ç»™å®šé•¿è§†é¢‘ V = {fâ‚, fâ‚‚, ..., fâ‚œ} å’ŒæŸ¥è¯¢ Qï¼Œæˆ‘ä»¬è¦å­¦ä¹ ä¸€ä¸ªå‡½æ•° â„±: (V, Q) â†’ Aï¼Œå…¶ä¸­ A æ˜¯ç›®æ ‡ç­”æ¡ˆã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†NTLBGçš„ç»Ÿè®¡æœ€ä¼˜æ€§ç›´æ¥åµŒå…¥åˆ°LLMçš„æŸå¤±å‡½æ•°ä¸­ã€‚

### 1.2 å¤šå…ƒç»Ÿè®¡å»ºæ¨¡

å°†è§†é¢‘å¸§ç‰¹å¾ç©ºé—´å»ºæ¨¡ä¸ºæŸ¥è¯¢æ¡ä»¶ä¸‹çš„å¤šå…ƒæ­£æ€åˆ†å¸ƒï¼š

**F | Q ~ N(Î¼_Q, Î£_Q)**

å…¶ä¸­ï¼š
- **F** = [fâ‚, fâ‚‚, ..., fâ‚œ]áµ€ âˆˆ â„áµ€Ë£áµˆ æ˜¯ç‰¹å¾çŸ©é˜µ
- **Î¼_Q** = E[F|Q] æ˜¯æ¡ä»¶å‡å€¼  
- **Î£_Q** = Cov(F|Q) æ˜¯æ¡ä»¶åæ–¹å·®çŸ©é˜µ

### 1.3 NTLBGä»£è¡¨ç‚¹é€‰æ‹©çš„æ•°å­¦è¡¨è¿°

åŸºäºç»Ÿè®¡ç†è®ºï¼Œæœ€ä¼˜çš„ k ä¸ªä»£è¡¨ç‚¹ {râ‚, râ‚‚, ..., râ‚–} æ»¡è¶³ï¼š

**(ráµ¢ - Î¼_Q)áµ€ Î£_Qâ»Â¹ (ráµ¢ - Î¼_Q) = c,  i = 1, 2, ..., k**

è¿™ä¿è¯äº†ä»£è¡¨ç‚¹åœ¨åŒä¸€ç­‰é«˜æ¤­çƒé¢ä¸Šï¼Œå…·æœ‰ç›¸åŒçš„ç»Ÿè®¡é‡è¦æ€§ã€‚

### 1.4 å¯Œä»£è¡¨ç‚¹çš„æ•°å­¦è¡¨ç¤º

æ¯ä¸ªä»£è¡¨ç‚¹ä¸ä»…åŒ…å«åŸå§‹ç‰¹å¾ï¼Œè¿˜åŒ…å«å…¶"å½±å“åŸŸ"ä¿¡æ¯ï¼š

**Ráµ¢ = [ráµ¢; cáµ¢; wáµ¢; táµ¢]**

å…¶ä¸­ï¼š
- **ráµ¢** âˆˆ â„áµˆ æ˜¯è§†è§‰ç‰¹å¾
- **cáµ¢** âˆˆ â„áµˆá¶œ æ˜¯ä¸Šä¸‹æ–‡ç‰¹å¾ï¼ˆå±€éƒ¨æ—¶åºä¿¡æ¯ï¼‰
- **wáµ¢** âˆˆ â„ æ˜¯ä»£è¡¨æ€§æƒé‡
- **táµ¢** âˆˆ â„ æ˜¯æ—¶åºä½ç½®ç¼–ç 

## 2. è®­ç»ƒç›®æ ‡è®¾è®¡

### 2.1 å¤šä»»åŠ¡æŸå¤±å‡½æ•°

æˆ‘ä»¬è®¾è®¡åŒ…å«NTLBGçº¦æŸçš„å¤šä»»åŠ¡æŸå¤±ï¼š

**L_total = L_task + Î»â‚L_NTLBG + Î»â‚‚L_align + Î»â‚ƒL_context**

### 2.2 ä»»åŠ¡æŸå¤± $\mathcal{L}_{\text{task}}$

æ ‡å‡†çš„è¯­è¨€å»ºæ¨¡æŸå¤±ï¼š

**L_task = -Î£áµ¢â‚Œâ‚|A| log P(aáµ¢ | Râ‚:â‚–, Q, aâ‚áµ¢â‚)**

### 2.3 NTLBGç»Ÿè®¡çº¦æŸæŸå¤± $\mathcal{L}_{\text{NTLBG}}$

ç¡®ä¿é€‰æ‹©çš„ä»£è¡¨ç‚¹æ»¡è¶³ç»Ÿè®¡æœ€ä¼˜æ€§ï¼š

**L_NTLBG = Î£áµ¢â‚Œâ‚áµ |(ráµ¢ - Î¼_Q)áµ€ Î£_Qâ»Â¹ (ráµ¢ - Î¼_Q) - c|Â²**

å…¶ä¸­ c æ˜¯ç›®æ ‡ç­‰é«˜çº¿å€¼ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ç¡®å®šï¼š

**c = argmin_cÌƒ Î£áµ¢â‚Œâ‚áµ MSE(F, {ráµ¢: |ráµ¢ - Î¼_Q|_Î£_Q = âˆšcÌƒ})**

### 2.4 ç‰¹å¾å¯¹é½æŸå¤± $\mathcal{L}_{\text{align}}$

ç¡®ä¿å‹ç¼©åçš„ç‰¹å¾åˆ†å¸ƒä¸LLMçš„é¢„æœŸè¾“å…¥åˆ†å¸ƒåŒ¹é…ï¼š

**L_align = KL(P_compressed || P_expected)**

å…¶ä¸­ï¼š
- **P_compressed** æ˜¯ä»£è¡¨ç‚¹ç‰¹å¾çš„ç»éªŒåˆ†å¸ƒ
- **P_expected** æ˜¯LLMå¯¹è§†é¢‘è¾“å…¥çš„æœŸæœ›åˆ†å¸ƒï¼ˆå¯é€šè¿‡é¢„è®­ç»ƒæ•°æ®ä¼°è®¡ï¼‰

### 2.5 ä¸Šä¸‹æ–‡è¿è´¯æ€§æŸå¤± $\mathcal{L}_{\text{context}}$

ä¿æŒæ—¶åºè¿è´¯æ€§å’Œè¯­ä¹‰è¿ç»­æ€§ï¼š

**L_context = Î£áµ¢â‚Œâ‚áµâ»Â¹ |cáµ¢â‚Šâ‚ - f_transition(cáµ¢, ráµ¢, ráµ¢â‚Šâ‚)|Â²**

å…¶ä¸­ **f_transition** æ˜¯å­¦ä¹ çš„è¿‡æ¸¡å‡½æ•°ã€‚

## 3. ç½‘ç»œæ¶æ„è®¾è®¡

### 3.1 NTLBG-Guided Attention Module

è®¾è®¡ä¸“é—¨çš„æ³¨æ„åŠ›æ¨¡å—æ¥å®ç°NTLBGé€‰æ‹©ï¼š

```python
class NTLBGAttention(nn.Module):
    def __init__(self, d_model, d_query):
        super().__init__()
        self.d_model = d_model
        self.query_proj = nn.Linear(d_query, d_model)
        self.mu_estimator = nn.Linear(d_model, d_model)
        self.sigma_estimator = nn.Linear(d_model, d_model * d_model)
        
    def forward(self, video_features, query):
        # video_features: [T, d_model]
        # query: [d_query]
        
        # 1. æŸ¥è¯¢å¼•å¯¼çš„å‚æ•°ä¼°è®¡
        query_embed = self.query_proj(query)  # [d_model]
        
        # 2. ä¼°è®¡æ¡ä»¶åˆ†å¸ƒå‚æ•°
        mu_q = self.mu_estimator(query_embed)  # [d_model]
        sigma_flat = self.sigma_estimator(query_embed)  # [d_model^2]
        sigma_q = sigma_flat.view(self.d_model, self.d_model)  # [d_model, d_model]
        
        # 3. è®¡ç®—æ¯å¸§åˆ°åˆ†å¸ƒä¸­å¿ƒçš„é©¬æ°è·ç¦»
        centered_features = video_features - mu_q  # [T, d_model]
        sigma_inv = torch.inverse(sigma_q + 1e-6 * torch.eye(self.d_model))
        
        mahalanobis_dist = torch.sum(
            centered_features @ sigma_inv * centered_features, dim=1
        )  # [T]
        
        # 4. NTLBGä»£è¡¨ç‚¹é€‰æ‹©
        representative_indices = self.ntlbg_selection(
            mahalanobis_dist, k=self.num_representatives
        )
        
        return representative_indices, mu_q, sigma_q
    
    def ntlbg_selection(self, distances, k):
        """
        åŸºäºNTLBGç®—æ³•é€‰æ‹©ä»£è¡¨ç‚¹
        """
        # æ ¹æ®é©¬æ°è·ç¦»é€‰æ‹©åœ¨åŒä¸€ç­‰é«˜çº¿ä¸Šçš„kä¸ªç‚¹
        target_distance = torch.median(distances)  # æˆ–å…¶ä»–ç­–ç•¥
        
        # æ‰¾åˆ°è·ç¦»ç›®æ ‡è·ç¦»æœ€è¿‘çš„kä¸ªç‚¹
        distance_diff = torch.abs(distances - target_distance)
        _, indices = torch.topk(distance_diff, k, largest=False)
        
        return indices
```

### 3.2 Rich Representative Point Constructor

æ„å»ºå¯Œä»£è¡¨ç‚¹çš„ç½‘ç»œæ¨¡å—ï¼š

```python
class RichRepresentativePointConstructor(nn.Module):
    def __init__(self, d_visual, d_context, d_temporal):
        super().__init__()
        self.context_encoder = nn.LSTM(d_visual, d_context, batch_first=True)
        self.weight_predictor = nn.Linear(d_visual + d_context, 1)
        self.temporal_encoder = nn.Linear(1, d_temporal)
        
    def forward(self, video_features, representative_indices, timestamps):
        rich_points = []
        
        for idx in representative_indices:
            # 1. è§†è§‰ç‰¹å¾
            visual_feat = video_features[idx]  # [d_visual]
            
            # 2. ä¸Šä¸‹æ–‡ç‰¹å¾ï¼ˆå‘¨å›´å¸§çš„LSTMç¼–ç ï¼‰
            context_window = self.get_context_window(video_features, idx)
            context_feat, _ = self.context_encoder(context_window.unsqueeze(0))
            context_feat = context_feat[0, -1]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
            
            # 3. ä»£è¡¨æ€§æƒé‡
            combined_feat = torch.cat([visual_feat, context_feat])
            weight = torch.sigmoid(self.weight_predictor(combined_feat))
            
            # 4. æ—¶åºç¼–ç 
            temporal_feat = self.temporal_encoder(timestamps[idx].unsqueeze(0))
            
            # 5. ç»„åˆå¯Œä»£è¡¨ç‚¹
            rich_point = torch.cat([
                visual_feat, context_feat, weight, temporal_feat
            ])
            rich_points.append(rich_point)
            
        return torch.stack(rich_points)  # [k, d_rich]
```

## 4. è®­ç»ƒç­–ç•¥

### 4.1 å¤šé˜¶æ®µè®­ç»ƒ

```python
def train_ntlbg_llm(model, dataloader, num_epochs):
    # Stage 1: é¢„è®­ç»ƒç‰¹å¾æå–å™¨
    for epoch in range(num_epochs // 3):
        for batch in dataloader:
            # åªè®­ç»ƒè§†è§‰ç‰¹å¾æå–å’ŒåŸºæœ¬çš„ä»£è¡¨ç‚¹é€‰æ‹©
            loss = compute_basic_ntlbg_loss(batch)
            loss.backward()
    
    # Stage 2: è”åˆè®­ç»ƒä»£è¡¨ç‚¹é€‰æ‹©å’ŒLLM
    for epoch in range(num_epochs // 3, 2 * num_epochs // 3):
        for batch in dataloader:
            # è”åˆä¼˜åŒ–NTLBGé€‰æ‹©å’Œè¯­è¨€ç†è§£
            loss = compute_joint_loss(batch)
            loss.backward()
    
    # Stage 3: ç«¯åˆ°ç«¯å¾®è°ƒ
    for epoch in range(2 * num_epochs // 3, num_epochs):
        for batch in dataloader:
            # å®Œæ•´çš„å¤šä»»åŠ¡æŸå¤±
            loss = compute_full_loss(batch)
            loss.backward()
```

### 4.2 æŸå¤±æƒé‡è°ƒåº¦

```python
def compute_loss_weights(epoch, total_epochs):
    # åŠ¨æ€è°ƒæ•´å„æŸå¤±é¡¹çš„æƒé‡
    progress = epoch / total_epochs
    
    lambda_task = 1.0  # ä»»åŠ¡æŸå¤±å§‹ç»ˆé‡è¦
    lambda_ntlbg = 2.0 * (1 - progress)  # æ—©æœŸé‡è§†ç»Ÿè®¡çº¦æŸ
    lambda_align = 1.0 * progress  # åæœŸé‡è§†å¯¹é½
    lambda_context = 0.5  # ä¸Šä¸‹æ–‡æŸå¤±ä¿æŒç¨³å®š
    
    return lambda_task, lambda_ntlbg, lambda_align, lambda_context
```

## 5. ç†è®ºä¿è¯

### 5.1 æ”¶æ•›æ€§åˆ†æ

**å®šç†1**ï¼šåœ¨åˆç†çš„å‡è®¾ä¸‹ï¼Œæˆ‘ä»¬çš„NTLBG-LLMè®­ç»ƒè¿‡ç¨‹æ”¶æ•›åˆ°ä»¥ä¸‹ä¼˜åŒ–é—®é¢˜çš„è§£ï¼š

**min_Î¸ E_(V,Q,A) [L_task(Î¸; V, Q, A)]**

**çº¦æŸæ¡ä»¶ï¼šráµ¢ âˆˆ {f_j}â±¼â‚Œâ‚áµ€, (ráµ¢ - Î¼_Q)áµ€ Î£_Qâ»Â¹ (ráµ¢ - Î¼_Q) = c**

**è¯æ˜æ€è·¯**ï¼šåˆ©ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜æ•°æ³•å’Œç»Ÿè®¡å­¦ä¹ ç†è®ºçš„æ”¶æ•›æ€§ç»“æœã€‚

### 5.2 ä»£è¡¨æ€§ä¿è¯

**å®šç†2**ï¼šé€‰æ‹©çš„ä»£è¡¨ç‚¹åœ¨ä¿¡æ¯è®ºæ„ä¹‰ä¸‹æ˜¯æœ€ä¼˜çš„ï¼Œå³æœ€å°åŒ–åŸå§‹è§†é¢‘ä¸å‹ç¼©è¡¨ç¤ºä¹‹é—´çš„äº’ä¿¡æ¯æŸå¤±ã€‚

**I(V; {Ráµ¢}áµ¢â‚Œâ‚áµ | Q) â‰¥ I(V; S | Q)**

å…¶ä¸­ **S** æ˜¯ä»»æ„å…¶ä»– k ç‚¹é‡‡æ ·ç­–ç•¥ã€‚

## 6. å®ç°ç»†èŠ‚

### 6.1 æ•°å€¼ç¨³å®šæ€§

```python
def stable_mahalanobis_distance(x, mu, sigma):
    """
    æ•°å€¼ç¨³å®šçš„é©¬æ°è·ç¦»è®¡ç®—
    """
    # ä½¿ç”¨Choleskyåˆ†è§£é¿å…ç›´æ¥æ±‚é€†
    L = torch.linalg.cholesky(sigma + 1e-6 * torch.eye(sigma.shape[-1]))
    diff = x - mu
    z = torch.linalg.solve_triangular(L, diff.T, upper=False)
    return torch.sum(z**2, dim=0)
```

### 6.2 å†…å­˜ä¼˜åŒ–

```python
def memory_efficient_ntlbg(video_features, query, chunk_size=1000):
    """
    å†…å­˜é«˜æ•ˆçš„NTLBGè®¡ç®—
    """
    T, d = video_features.shape
    distances = []
    
    for i in range(0, T, chunk_size):
        chunk = video_features[i:i+chunk_size]
        chunk_distances = compute_mahalanobis_distances(chunk, query)
        distances.append(chunk_distances)
    
    return torch.cat(distances)
```

---

## ğŸ“š ç›¸å…³è®ºæ–‡

å¦‚æœæ‚¨ä½¿ç”¨äº†æ­¤ä»£ç ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@article{ntlbg-llm,
  title={Novel Temporal Long-form Best-view Grounding for Large Language Models},
  author={Your Name},
  journal={arXiv preprint},
  year={2024}
}
```

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ï¼è¯·éµå¾ªä»¥ä¸‹æ­¥éª¤ï¼š

1. Fork è¿™ä¸ªä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤ä¿®æ”¹ (`git commit -m 'Add amazing feature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. å¼€å¯ Pull Request

## ğŸ› å¸¸è§é—®é¢˜

### Q: è§†é¢‘åŠ è½½å¤±è´¥æ€ä¹ˆåŠï¼Ÿ
A: è¯·ç¡®ä¿è§†é¢‘æ–‡ä»¶æ ¼å¼æ­£ç¡®ï¼Œå¹¶æ£€æŸ¥ `data/videos/` ç›®å½•æƒé™ã€‚

### Q: GPUå†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
A: å‡å°‘ `batch_size` æˆ–è°ƒæ•´ `max_frames` å‚æ•°ï¼Œæˆ–ä½¿ç”¨é…ç½®ç®¡ç†å™¨ä¼˜åŒ–é…ç½®ã€‚

### Q: è®­ç»ƒé€Ÿåº¦å¤ªæ…¢æ€ä¹ˆåŠï¼Ÿ
A: å¯ç”¨ `fp16` æˆ– `bf16` è®­ç»ƒï¼Œä½¿ç”¨ `flash_attention`ï¼Œæˆ–å‡å°‘ `num_representatives` å‚æ•°ã€‚

### Q: å¦‚ä½•è‡ªå®šä¹‰æ•°æ®é›†ï¼Ÿ
A: æŒ‰ç…§ `data/train.jsonl` æ ¼å¼å‡†å¤‡æ•°æ®ï¼Œå¹¶ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„æ•°æ®è·¯å¾„ã€‚

### Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„é…ç½®çº§åˆ«ï¼Ÿ
A: ä½¿ç”¨é…ç½®ç®¡ç†å™¨åˆ†ææ•°æ®é›†ç‰¹å¾ï¼š`python scripts/longvideo_config_manager.py --action analyze`

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ“ è”ç³»æ–¹å¼

- é¡¹ç›®ä¸»é¡µ: https://github.com/your-username/NTLBG-LLM
- é‚®ç®±: your.email@example.com
- æŠ€æœ¯æ–‡æ¡£: configs/README.md

## ğŸ™ è‡´è°¢

æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®çš„è´¡çŒ®ï¼š
- [Transformers](https://github.com/huggingface/transformers)
- [PyTorch](https://github.com/pytorch/pytorch)
- [OpenCV](https://github.com/opencv/opencv)
- [Qwen2-VL](https://github.com/QwenLM/Qwen2-VL)

---

**æ³¨æ„**: æœ¬é¡¹ç›®ä¸“æ³¨äºé•¿è§†é¢‘ç†è§£ï¼Œæ”¯æŒå¤šç§GPUé…ç½®ã€‚å¦‚æœ‰é—®é¢˜è¯·æŸ¥çœ‹é…ç½®æ–‡æ¡£æˆ–æäº¤ Issueã€‚
