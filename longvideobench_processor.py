"""
LongVideoBenchæ•°æ®å¤„ç†å™¨
æ”¯æŒçœŸå®è§†é¢‘æ•°æ®çš„åŠ è½½å’Œå¤„ç†
"""
import os
import json
import torch
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import cv2
import numpy as np
from PIL import Image
from typing import List, Dict, Any
import logging

try:
    from longvideobench import LongVideoBenchDataset as OfficialDataset
    OFFICIAL_LOADER_AVAILABLE = False  # ç®€åŒ–å¤„ç†
    print("âœ… æˆåŠŸå¯¼å…¥å®˜æ–¹LongVideoBenchæ•°æ®åŠ è½½å™¨")
except ImportError:
    OFFICIAL_LOADER_AVAILABLE = False
    print("âš ï¸ å®˜æ–¹æ•°æ®åŠ è½½å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨è‡ªå®šä¹‰å®ç°")

logger = logging.getLogger(__name__)

class LongVideoBenchProcessor:
    """LongVideoBenchæ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, data_root="/workspace/NTLBG-LLM/data/longvideobench", max_frames=64):
        self.data_root = Path(data_root)
        self.max_frames = max_frames
        self.video_dir = self.data_root / "videos"
        self.subtitle_dir = self.data_root / "subtitles"
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        self._check_data_integrity()
    
    def _check_data_integrity(self):
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§"""
        required_files = [
            self.data_root / "lvb_val.json",
            self.data_root / "lvb_test_wo_gt.json"
        ]
        
        for file_path in required_files:
            if not file_path.exists():
                logger.warning(f"âš ï¸ ç¼ºå°‘æ–‡ä»¶: {file_path}")
        
        if self.video_dir.exists():
            video_count = len(list(self.video_dir.glob("*.mp4")))
            logger.info(f"ğŸ“¹ æ‰¾åˆ° {video_count} ä¸ªè§†é¢‘æ–‡ä»¶")
        else:
            logger.warning(f"âš ï¸ è§†é¢‘ç›®å½•ä¸å­˜åœ¨: {self.video_dir}")
        
        if self.subtitle_dir.exists():
            subtitle_count = len(list(self.subtitle_dir.glob("*.srt")))
            logger.info(f"ğŸ“ æ‰¾åˆ° {subtitle_count} ä¸ªå­—å¹•æ–‡ä»¶")
    
    def load_video_frames(self, video_path: str) -> List[Image.Image]:
        """åŠ è½½è§†é¢‘å¸§"""
        if not os.path.exists(video_path):
            logger.warning(f"âš ï¸ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            return []
        
        try:
            cap = cv2.VideoCapture(video_path)
            frames = []
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            # å‡åŒ€é‡‡æ ·å¸§
            if frame_count <= self.max_frames:
                indices = list(range(frame_count))
            else:
                indices = np.linspace(0, frame_count-1, self.max_frames, dtype=int)
            
            for idx in indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()
                if ret:
                    # è½¬æ¢ä¸ºPIL Image
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    pil_frame = Image.fromarray(frame_rgb)
                    frames.append(pil_frame)
            
            cap.release()
            return frames
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½è§†é¢‘å¤±è´¥ {video_path}: {e}")
            return []
    
    def load_subtitle(self, subtitle_path: str) -> str:
        """åŠ è½½å­—å¹•"""
        if not os.path.exists(subtitle_path):
            return ""
        
        try:
            with open(subtitle_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ç®€å•çš„SRTè§£æ
            lines = content.split('\n')
            subtitle_text = []
            
            for line in lines:
                line = line.strip()
                if line and not line.isdigit() and '-->' not in line:
                    subtitle_text.append(line)
            
            return ' '.join(subtitle_text)
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å­—å¹•å¤±è´¥ {subtitle_path}: {e}")
            return ""


class LongVideoBenchDataset(Dataset):
    """LongVideoBenchæ•°æ®é›†"""
    
    def __init__(self, data_root, split="val", max_frames=64, max_samples=None):
        self.data_root = Path(data_root)
        self.split = split
        self.max_frames = max_frames
        self.processor = LongVideoBenchProcessor(data_root, max_frames)
        
        # åŠ è½½æ•°æ®
        self.data = self._load_data()
        
        if max_samples:
            self.data = self.data[:max_samples]
        
        logger.info(f"ğŸ“Š {split}æ•°æ®é›†åŠ è½½å®Œæˆ: {len(self.data)} ä¸ªæ ·æœ¬")
    
    def _load_data(self):
        """åŠ è½½æ•°æ®"""
        if OFFICIAL_LOADER_AVAILABLE:
            return self._load_with_official_loader()
        else:
            return self._load_with_custom_loader()
    
    def _load_with_official_loader(self):
        """ä½¿ç”¨å®˜æ–¹åŠ è½½å™¨"""
        try:
            json_file = f"lvb_{self.split}.json"
            official_dataset = OfficialDataset(
                str(self.data_root),
                json_file,
                max_num_frames=self.max_frames
            )
            
            data = []
            for i in range(len(official_dataset)):
                try:
                    sample = official_dataset[i]
                    
                    # åˆ†ç¦»è§†é¢‘å¸§å’Œæ–‡æœ¬
                    frames = []
                    texts = []
                    
                    for item in sample.get("inputs", []):
                        if hasattr(item, 'size'):  # PIL Image
                            frames.append(item)
                        elif isinstance(item, str):
                            texts.append(item)
                    
                    processed_sample = {
                        'video_id': sample.get('video_id', f'video_{i}'),
                        'frames': frames,
                        'subtitle': ' '.join(texts),
                        'question': sample.get('question', ''),
                        'options': sample.get('options', []),
                        'answer': sample.get('answer', 0)
                    }
                    
                    data.append(processed_sample)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ å®˜æ–¹æ•°æ®æ ·æœ¬{i}åŠ è½½å¤±è´¥: {e}")
                    continue
            
            return data
            
        except Exception as e:
            logger.error(f"âŒ å®˜æ–¹åŠ è½½å™¨å¤±è´¥: {e}")
            return self._load_with_custom_loader()
    
    def _load_with_custom_loader(self):
        """ä½¿ç”¨è‡ªå®šä¹‰åŠ è½½å™¨"""
        json_file = self.data_root / f"lvb_{self.split}.json"
        
        if not json_file.exists():
            logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
            return self._create_dummy_data()
        
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                raw_data = json.load(f)
            
            data = []
            for item in raw_data:
                try:
                    video_id = item.get('video_id', '')
                    video_path = self.data_root / "videos" / f"{video_id}.mp4"
                    subtitle_path = self.data_root / "subtitles" / f"{video_id}.srt"
                    
                    # åŠ è½½è§†é¢‘å¸§
                    frames = self.processor.load_video_frames(str(video_path))
                    
                    # åŠ è½½å­—å¹•
                    subtitle = self.processor.load_subtitle(str(subtitle_path))
                    
                    processed_sample = {
                        'video_id': video_id,
                        'frames': frames,
                        'subtitle': subtitle,
                        'question': item.get('question', ''),
                        'options': item.get('options', []),
                        'answer': item.get('answer', 0)
                    }
                    
                    data.append(processed_sample)
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ æ ·æœ¬å¤„ç†å¤±è´¥: {e}")
                    continue
            
            return data
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            return self._create_dummy_data()
    
    def _create_dummy_data(self):
        """åˆ›å»ºè™šæ‹Ÿæ•°æ®ç”¨äºæµ‹è¯•"""
        logger.info("ğŸ”§ åˆ›å»ºè™šæ‹Ÿæ•°æ®è¿›è¡Œæµ‹è¯•...")
        
        dummy_data = []
        questions = [
            "What is the main topic of this video?",
            "Who appears in this video?", 
            "What happens at the end of the video?",
            "What is the setting of this video?",
            "What is the speaker discussing?"
        ]
        
        for i in range(50):  # åˆ›å»º50ä¸ªè™šæ‹Ÿæ ·æœ¬
            # åˆ›å»ºè™šæ‹Ÿå¸§
            frames = []
            for j in range(self.max_frames):
                # åˆ›å»ºéšæœºé¢œè‰²çš„è™šæ‹Ÿå›¾åƒ
                img_array = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
                frame = Image.fromarray(img_array)
                frames.append(frame)
            
            dummy_sample = {
                'video_id': f'dummy_video_{i}',
                'frames': frames,
                'subtitle': f'This is a dummy subtitle for video {i}.',
                'question': questions[i % len(questions)],
                'options': ['Option A', 'Option B', 'Option C', 'Option D'],
                'answer': i % 4
            }
            
            dummy_data.append(dummy_sample)
        
        return dummy_data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        return self.data[idx]


def create_dataloader(data_root, split="val", batch_size=1, max_frames=64, max_samples=None):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    dataset = LongVideoBenchDataset(
        data_root=data_root,
        split=split,
        max_frames=max_frames,
        max_samples=max_samples
    )
    
    def collate_fn(batch):
        """æ‰¹å¤„ç†å‡½æ•°"""
        return {
            'video_ids': [item['video_id'] for item in batch],
            'frames': [item['frames'] for item in batch],
            'subtitles': [item['subtitle'] for item in batch],
            'questions': [item['question'] for item in batch],
            'options': [item['options'] for item in batch],
            'answers': torch.tensor([item['answer'] for item in batch])
        }
    
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=(split == "train"),
        collate_fn=collate_fn,
        num_workers=0  # H200ä¸Šè®¾ç½®ä¸º0é¿å…å¤šè¿›ç¨‹é—®é¢˜
    )

