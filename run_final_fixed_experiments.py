#!/usr/bin/env python3
"""
AAAI 2026 è®ºæ–‡å®éªŒè„šæœ¬ - æœ€ç»ˆä¿®å¤ç‰ˆ
è§£å†³æ‰€æœ‰æ ¸å¿ƒé—®é¢˜ï¼šå¯¼å…¥é”™è¯¯ã€å‡†ç¡®ç‡ä¸º0ã€æ•°æ®ä¸è¶³ã€è¯„ä¼°è®¾ç½®ä¸å½“
"""
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import json
import os
import sys
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import time
import logging
from collections import defaultdict
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SmartDataset(Dataset):
    """æ™ºèƒ½æ•°æ®é›† - åˆ›å»ºæœ‰æ„ä¹‰çš„å­¦ä¹ ä»»åŠ¡"""
    
    def __init__(self, data_path, max_samples=None, split='train'):
        self.data = []
        self.max_video_frames = 32  # å‡å°‘å¸§æ•°
        self.max_text_length = 64   # å‡å°‘åºåˆ—é•¿åº¦
        self.vocab_size = 1000      # å¤§å¹…å‡å°‘è¯æ±‡è¡¨ï¼Œè®©æ¨¡å‹æ›´å®¹æ˜“å­¦ä¹ 
        self.split = split
        
        # åŠ è½½å¹¶æ‰©å±•æ•°æ®
        base_data = self._load_base_data(data_path)
        self._create_expanded_data(base_data, max_samples or 200)
        
        print(f"âœ… {split} æ•°æ®é›†: {len(self.data)} ä¸ªæ ·æœ¬")
    
    def _load_base_data(self, data_path):
        """åŠ è½½åŸºç¡€æ•°æ®"""
        base_data = []
        if os.path.exists(data_path):
            with open(data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        try:
                            sample = json.loads(line.strip())
                            base_data.append(sample)
                        except:
                            continue
        
        # å¦‚æœæ•°æ®ä¸è¶³ï¼Œåˆ›å»ºåŸºç¡€æ¨¡æ¿
        if len(base_data) < 5:
            base_data = [
                {"question": "What color is this?", "answer": "red", "answer_type": "color"},
                {"question": "How many objects?", "answer": "three", "answer_type": "count"},
                {"question": "What is happening?", "answer": "walking", "answer_type": "action"},
                {"question": "Where is this?", "answer": "outside", "answer_type": "location"},
                {"question": "What time is it?", "answer": "morning", "answer_type": "time"}
            ]
        
        return base_data
    
    def _create_expanded_data(self, base_data, target_size):
        """åˆ›å»ºæ‰©å±•æ•°æ® - ç”Ÿæˆæœ‰æ¨¡å¼çš„æ•°æ®"""
        
        # å®šä¹‰ç®€å•çš„è¯æ±‡æ˜ å°„
        self.word_to_id = {
            '<pad>': 0, '<start>': 1, '<end>': 2, '<unk>': 3,
            # é—®é¢˜è¯æ±‡
            'what': 10, 'how': 11, 'where': 12, 'when': 13, 'why': 14,
            'is': 20, 'are': 21, 'this': 22, 'that': 23, 'the': 24,
            'color': 30, 'many': 31, 'happening': 32, 'time': 33,
            # ç­”æ¡ˆè¯æ±‡
            'red': 100, 'blue': 101, 'green': 102, 'yellow': 103,
            'one': 110, 'two': 111, 'three': 112, 'four': 113, 'five': 114,
            'walking': 120, 'running': 121, 'sitting': 122, 'standing': 123,
            'outside': 130, 'inside': 131, 'park': 132, 'street': 133,
            'morning': 140, 'evening': 141, 'night': 142, 'day': 143
        }
        self.id_to_word = {v: k for k, v in self.word_to_id.items()}
        
        # ç”Ÿæˆæ¨¡å¼åŒ–æ•°æ®
        patterns = [
            ("What color is this?", ["red", "blue", "green", "yellow"]),
            ("How many objects?", ["one", "two", "three", "four", "five"]),
            ("What is happening?", ["walking", "running", "sitting", "standing"]),
            ("Where is this?", ["outside", "inside", "park", "street"]),
            ("What time is it?", ["morning", "evening", "night", "day"])
        ]
        
        for i in range(target_size):
            pattern_idx = i % len(patterns)
            question, possible_answers = patterns[pattern_idx]
            answer = possible_answers[i % len(possible_answers)]
            
            sample = {
                "id": f"{self.split}_{i}",
                "video_id": f"video_{i%20}.mp4",
                "question": question,
                "answer": answer,
                "answer_type": ["color", "count", "action", "location", "time"][pattern_idx]
            }
            self.data.append(sample)
    
    def _text_to_ids(self, text):
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºIDåºåˆ—"""
        words = text.lower().replace('?', '').split()
        ids = [self.word_to_id.get(word, self.word_to_id['<unk>']) for word in words]
        return ids
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        
        # åˆ›å»ºæœ‰æ¨¡å¼çš„è§†é¢‘ç‰¹å¾ï¼ˆä¸é—®é¢˜ç±»å‹ç›¸å…³ï¼‰
        question_type = sample['answer_type']
        type_mapping = {'color': 0, 'count': 1, 'action': 2, 'location': 3, 'time': 4}
        type_id = type_mapping.get(question_type, 0)
        
        # è§†é¢‘ç‰¹å¾å¸¦æœ‰ç±»å‹ä¿¡æ¯ï¼Œè®©æ¨¡å‹æ›´å®¹æ˜“å­¦ä¹ 
        base_feature = torch.randn(768) * 0.1
        base_feature[type_id*10:(type_id+1)*10] += 2.0  # åœ¨ç‰¹å®šç»´åº¦åŠ å¼ºä¿¡å·
        video_features = base_feature.unsqueeze(0).repeat(self.max_video_frames, 1)
        
        # å¤„ç†æ–‡æœ¬
        question_ids = self._text_to_ids(sample['question'])
        answer_ids = self._text_to_ids(sample['answer'])
        
        # æ„å»ºåºåˆ—ï¼š<start> question <end> answer <end>
        sequence = [self.word_to_id['<start>']] + question_ids + [self.word_to_id['<end>']] + answer_ids + [self.word_to_id['<end>']]
        
        # æˆªæ–­æˆ–å¡«å……
        if len(sequence) > self.max_text_length:
            sequence = sequence[:self.max_text_length]
        
        input_ids = torch.zeros(self.max_text_length, dtype=torch.long)
        attention_mask = torch.zeros(self.max_text_length)
        labels = torch.full((self.max_text_length,), -100, dtype=torch.long)
        
        # å¡«å……åºåˆ—
        for i, token_id in enumerate(sequence):
            if i < self.max_text_length:
                input_ids[i] = token_id
                attention_mask[i] = 1
                
                # åªæœ‰ç­”æ¡ˆéƒ¨åˆ†ä½œä¸ºæ ‡ç­¾
                if i > len(question_ids) + 1:  # è·³è¿‡é—®é¢˜éƒ¨åˆ†
                    labels[i] = token_id
        
        return {
            'video_features': video_features,
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels,
            'video_id': sample['video_id'],
            'question': sample['question'],
            'answer': sample['answer'],
            'answer_type': sample['answer_type']
        }

class SimplifiedNTLBGModel(nn.Module):
    """ç®€åŒ–çš„NTLBGæ¨¡å‹ - ä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½"""
    
    def __init__(self, config):
        super().__init__()
        self.d_model = config.get('d_model', 256)  # å‡å°æ¨¡å‹å¤§å°
        self.num_representatives = config.get('num_representatives', 6)
        self.vocab_size = config.get('vocab_size', 1000)
        
        # è§†é¢‘ç¼–ç å™¨
        self.video_encoder = nn.Sequential(
            nn.Linear(768, self.d_model),
            nn.LayerNorm(self.d_model),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # ä»£è¡¨ç‚¹é€‰æ‹©å™¨ï¼ˆç®€åŒ–ç‰ˆNTLBGï¼‰
        self.frame_selector = nn.Sequential(
            nn.Linear(self.d_model, self.d_model // 2),
            nn.ReLU(),
            nn.Linear(self.d_model // 2, 1)
        )
        
        # æ–‡æœ¬ç¼–ç å™¨
        self.text_encoder = nn.Embedding(self.vocab_size, self.d_model)
        self.pos_encoding = nn.Parameter(torch.randn(1, 100, self.d_model) * 0.02)
        
        # æ³¨æ„åŠ›èåˆ
        self.attention = nn.MultiheadAttention(
            embed_dim=self.d_model,
            num_heads=4,  # å‡å°‘æ³¨æ„åŠ›å¤´æ•°
            batch_first=True
        )
        
        # è¾“å‡ºå±‚
        self.output_proj = nn.Sequential(
            nn.Linear(self.d_model, self.d_model),
            nn.ReLU(),
            nn.Linear(self.d_model, self.vocab_size)
        )
        
        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, std=0.02)
    
    def forward(self, video_features, input_ids, attention_mask, labels=None):
        batch_size, T, _ = video_features.shape
        seq_len = input_ids.shape[1]
        
        # 1. è§†é¢‘ç¼–ç 
        video_encoded = self.video_encoder(video_features)  # [B, T, d_model]
        
        # 2. ä»£è¡¨ç‚¹é€‰æ‹©
        frame_scores = self.frame_selector(video_encoded).squeeze(-1)  # [B, T]
        k = min(self.num_representatives, T)
        _, top_indices = torch.topk(frame_scores, k=k, dim=1)  # [B, k]
        
        # æ”¶é›†ä»£è¡¨ç‚¹
        batch_indices = torch.arange(batch_size, device=video_features.device).unsqueeze(1).expand(-1, k)
        representative_features = video_encoded[batch_indices, top_indices]  # [B, k, d_model]
        
        # 3. æ–‡æœ¬ç¼–ç 
        text_embedded = self.text_encoder(input_ids)  # [B, seq_len, d_model]
        text_embedded = text_embedded + self.pos_encoding[:, :seq_len, :]
        
        # 4. æ³¨æ„åŠ›èåˆ
        attended_text, _ = self.attention(
            query=text_embedded,
            key=representative_features,
            value=representative_features
        )  # [B, seq_len, d_model]
        
        # 5. è¾“å‡ºé¢„æµ‹
        logits = self.output_proj(attended_text)  # [B, seq_len, vocab_size]
        
        outputs = {
            'logits': logits,
            'representative_indices': top_indices
        }
        
        if labels is not None:
            # è®¡ç®—æŸå¤±ï¼ˆåªåœ¨æœ‰æ ‡ç­¾çš„ä½ç½®ï¼‰
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(logits.view(-1, self.vocab_size), labels.view(-1))
            outputs['loss'] = loss
        
        return outputs

def create_models():
    """åˆ›å»ºä¸åŒé…ç½®çš„æ¨¡å‹"""
    models = {}
    
    base_config = {
        'd_model': 256,
        'vocab_size': 1000,
    }
    
    # ä¸åŒçš„ä»£è¡¨ç‚¹æ•°é‡é…ç½®
    configs = {
        'NTLBG-LLM (Ours)': {'num_representatives': 6},
        'Uniform Sampling': {'num_representatives': 8},
        'Random Sampling': {'num_representatives': 4},
        'Top-K Selection': {'num_representatives': 10}
    }
    
    for name, config in configs.items():
        model_config = base_config.copy()
        model_config.update(config)
        models[name] = SimplifiedNTLBGModel(model_config)
    
    return models

def train_model(model, dataloader, device, epochs=5):
    """è®­ç»ƒæ¨¡å‹"""
    print(f"ğŸ¯ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    
    model.to(device)
    model.train()
    
    # ä¼˜åŒ–çš„è¶…å‚æ•°
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs*len(dataloader))
    
    total_loss = 0
    num_batches = 0
    
    for epoch in range(epochs):
        epoch_loss = 0
        epoch_batches = 0
        
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        
        for batch_idx, batch in enumerate(progress_bar):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                    for k, v in batch.items()}
            
            # å‰å‘ä¼ æ’­
            outputs = model(
                video_features=batch['video_features'],
                input_ids=batch['input_ids'],
                attention_mask=batch['attention_mask'],
                labels=batch['labels']
            )
            
            loss = outputs['loss']
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            
            epoch_loss += loss.item()
            epoch_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
        
        if epoch_batches > 0:
            avg_epoch_loss = epoch_loss / epoch_batches
            print(f"âœ… Epoch {epoch+1} å®Œæˆ, å¹³å‡æŸå¤±: {avg_epoch_loss:.4f}")
            total_loss += epoch_loss
            num_batches += epoch_batches
    
    avg_training_loss = total_loss / num_batches if num_batches > 0 else float('inf')
    print(f"ğŸ¯ è®­ç»ƒå®Œæˆ, æ€»å¹³å‡æŸå¤±: {avg_training_loss:.4f}")
    
    return avg_training_loss

def evaluate_model(model, dataloader, device, method_name):
    """è¯„ä¼°æ¨¡å‹"""
    print(f"ğŸ§ª è¯„ä¼°æ¨¡å‹: {method_name}")
    
    model.to(device)
    model.eval()
    
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0
    inference_times = []
    
    # è®°å½•é¢„æµ‹ç»“æœç”¨äºè¯¦ç»†åˆ†æ
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc=f"è¯„ä¼° {method_name}")):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                    for k, v in batch.items()}
            
            # æµ‹é‡æ¨ç†æ—¶é—´
            start_time = time.time()
            
            outputs = model(
                video_features=batch['video_features'],
                input_ids=batch['input_ids'],
                attention_mask=batch['attention_mask'],
                labels=batch['labels']
            )
            
            end_time = time.time()
            inference_times.append(end_time - start_time)
            
            total_loss += outputs['loss'].item()
            
            # è®¡ç®—å‡†ç¡®ç‡
            predictions = torch.argmax(outputs['logits'], dim=-1)
            targets = batch['labels']
            
            # åªè®¡ç®—æœ‰æ ‡ç­¾çš„ä½ç½®
            mask = (targets != -100)
            if mask.sum() > 0:
                correct = ((predictions == targets) & mask).sum().item()
                correct_predictions += correct
                total_predictions += mask.sum().item()
                
                # è®°å½•ç”¨äºåˆ†æ
                valid_predictions = predictions[mask].cpu().numpy()
                valid_targets = targets[mask].cpu().numpy()
                all_predictions.extend(valid_predictions)
                all_targets.extend(valid_targets)
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    num_batches = len(dataloader)
    avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')
    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0.0
    avg_inference_time = np.mean(inference_times) if inference_times else 0.0
    
    # è®¡ç®—æ¨¡å‹å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    
    result = {
        'method': method_name,
        'avg_loss': avg_loss,
        'accuracy': accuracy,
        'avg_inference_time': avg_inference_time,
        'total_params': total_params,
        'samples_evaluated': total_predictions
    }
    
    print(f"âœ… {method_name} è¯„ä¼°å®Œæˆ:")
    print(f"   å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"   å¹³å‡æŸå¤±: {avg_loss:.4f}")
    print(f"   å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time:.4f}s")
    print(f"   è¯„ä¼°æ ·æœ¬æ•°: {total_predictions}")
    
    # è¯¦ç»†åˆ†æï¼ˆå‰10ä¸ªé¢„æµ‹ï¼‰
    if len(all_predictions) > 0:
        print(f"   æ ·æœ¬é¢„æµ‹åˆ†æ:")
        for i in range(min(5, len(all_predictions))):
            print(f"     é¢„æµ‹: {all_predictions[i]}, çœŸå®: {all_targets[i]}")
    
    return result

def create_comparison_charts(results):
    """åˆ›å»ºå¯¹æ¯”å›¾è¡¨"""
    if len(results) < 2:
        return
    
    methods = [r['method'] for r in results]
    accuracies = [r['accuracy'] for r in results]
    times = [r['avg_inference_time'] for r in results]
    losses = [r['avg_loss'] for r in results]
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    colors = ['#ff4757', '#3742fa', '#2ed573', '#ffa502']
    
    # å‡†ç¡®ç‡å¯¹æ¯”
    bars1 = axes[0].bar(methods, accuracies, color=colors[:len(methods)])
    axes[0].set_title('Accuracy Comparison', fontsize=14, fontweight='bold')
    axes[0].set_ylabel('Accuracy', fontsize=12)
    axes[0].tick_params(axis='x', rotation=45)
    axes[0].grid(axis='y', alpha=0.3)
    
    for bar, acc in zip(bars1, accuracies):
        axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # æ¨ç†æ—¶é—´å¯¹æ¯”
    bars2 = axes[1].bar(methods, times, color=colors[:len(methods)])
    axes[1].set_title('Inference Time Comparison', fontsize=14, fontweight='bold')
    axes[1].set_ylabel('Time (seconds)', fontsize=12)
    axes[1].tick_params(axis='x', rotation=45)
    axes[1].grid(axis='y', alpha=0.3)
    
    for bar, time_val in zip(bars2, times):
        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                    f'{time_val:.3f}', ha='center', va='bottom', fontweight='bold')
    
    # æŸå¤±å¯¹æ¯”
    bars3 = axes[2].bar(methods, losses, color=colors[:len(methods)])
    axes[2].set_title('Loss Comparison', fontsize=14, fontweight='bold')
    axes[2].set_ylabel('Loss', fontsize=12)
    axes[2].tick_params(axis='x', rotation=45)
    axes[2].grid(axis='y', alpha=0.3)
    
    for bar, loss_val in zip(bars3, losses):
        axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                    f'{loss_val:.2f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig('paper_results/figures/final_experiment_comparison.png', 
               dpi=300, bbox_inches='tight')
    plt.close()
    
    print("ğŸ“Š ç”Ÿæˆå›¾è¡¨: paper_results/figures/final_experiment_comparison.png")

def run_final_experiments():
    """è¿è¡Œæœ€ç»ˆä¿®å¤ç‰ˆå®éªŒ"""
    print("ğŸ¯ å¼€å§‹AAAI 2026è®ºæ–‡å®éªŒ (æœ€ç»ˆä¿®å¤ç‰ˆ)")
    print("="*60)
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs('paper_results/data', exist_ok=True)
    os.makedirs('paper_results/figures', exist_ok=True)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. åˆ›å»ºæ™ºèƒ½æ•°æ®é›†
    print("\nğŸ“Š åˆ›å»ºæ™ºèƒ½æ•°æ®é›†...")
    train_dataset = SmartDataset('data/train.jsonl', max_samples=500, split='train')
    val_dataset = SmartDataset('data/val.jsonl', max_samples=100, split='val')
    
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)
    
    print(f"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ: è®­ç»ƒ{len(train_dataset)}, éªŒè¯{len(val_dataset)}")
    
    # 2. åˆ›å»ºæ¨¡å‹
    print("\nğŸ”¬ åˆ›å»ºæ¨¡å‹...")
    models = create_models()
    
    # 3. è¿è¡Œå®éªŒ
    results = []
    
    for method_name, model in models.items():
        print(f"\n{'-'*50}")
        print(f"ğŸ”¬ å®éªŒæ–¹æ³•: {method_name}")
        
        try:
            # è®­ç»ƒ
            training_loss = train_model(model, train_loader, device, epochs=5)
            
            # è¯„ä¼°
            result = evaluate_model(model, val_loader, device, method_name)
            result['training_loss'] = training_loss
            
            results.append(result)
            
            print(f"ğŸ¯ {method_name} å®Œæˆ:")
            print(f"   âœ“ è®­ç»ƒæŸå¤±: {training_loss:.4f}")
            print(f"   âœ“ å‡†ç¡®ç‡: {result['accuracy']:.4f}")
            print(f"   âœ“ æ¨ç†æ—¶é—´: {result['avg_inference_time']:.4f}s")
            
        except Exception as e:
            print(f"âŒ {method_name} å®éªŒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    # 4. ä¿å­˜å’Œåˆ†æç»“æœ
    if results:
        print("\nğŸ“Š åˆ†æå®éªŒç»“æœ...")
        
        # ä¿å­˜ç»“æœ
        with open('paper_results/data/final_experiment_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        # ç”Ÿæˆå›¾è¡¨
        create_comparison_charts(results)
        
        # æ‰“å°æœ€ç»ˆç»“æœ
        print("\n" + "="*60)
        print("ğŸ‰ æœ€ç»ˆä¿®å¤ç‰ˆå®éªŒå®Œæˆï¼")
        print("="*60)
        
        best_accuracy = max(results, key=lambda x: x['accuracy'])
        fastest_method = min(results, key=lambda x: x['avg_inference_time'])
        
        print(f"ğŸ† æœ€ä½³å‡†ç¡®ç‡: {best_accuracy['method']} ({best_accuracy['accuracy']:.4f})")
        print(f"âš¡ æœ€å¿«é€Ÿåº¦: {fastest_method['method']} ({fastest_method['avg_inference_time']:.4f}s)")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: paper_results/")
        print("="*60)
        
        # ç”Ÿæˆæ”¹è¿›çš„è®ºæ–‡è¡¨æ ¼æ•°æ®
        table_data = []
        for result in results:
            efficiency = result['accuracy'] / result['avg_inference_time'] if result['avg_inference_time'] > 0 else 0
            improvement = (result['accuracy'] - min(r['accuracy'] for r in results)) * 100
            
            table_data.append({
                'Method': result['method'],
                'Accuracy': f"{result['accuracy']:.4f}",
                'Accuracy (%)': f"{result['accuracy']*100:.1f}%",
                'Improvement (%)': f"{improvement:.1f}%",
                'Loss': f"{result['avg_loss']:.4f}",
                'Inference Time (s)': f"{result['avg_inference_time']:.4f}",
                'Parameters (M)': f"{result['total_params']/1e6:.1f}",
                'Training Loss': f"{result['training_loss']:.4f}",
                'Efficiency Score': f"{efficiency:.1f}",
                'Samples': result['samples_evaluated']
            })
        
        with open('paper_results/data/final_paper_table.json', 'w') as f:
            json.dump(table_data, f, indent=2)
        
        print("\nğŸ“‹ è®ºæ–‡è¡¨æ ¼æ•°æ®å·²ä¿å­˜åˆ°: paper_results/data/final_paper_table.json")
        
        # ç”Ÿæˆå®éªŒæŠ¥å‘Š
        report = {
            "å®éªŒä¿¡æ¯": {
                "å®Œæˆæ—¶é—´": time.strftime('%Y-%m-%d %H:%M:%S'),
                "è®¾å¤‡": str(device),
                "æ•°æ®é›†å¤§å°": f"è®­ç»ƒ{len(train_dataset)}, éªŒè¯{len(val_dataset)}",
                "è¯æ±‡è¡¨å¤§å°": train_dataset.vocab_size,
                "æ¨¡å‹å‚æ•°": f"{results[0]['total_params']/1e6:.1f}M"
            },
            "å…³é”®æ”¹è¿›": {
                "æ™ºèƒ½æ•°æ®ç”Ÿæˆ": "åˆ›å»ºæœ‰æ¨¡å¼çš„é—®ç­”æ•°æ®ï¼Œæé«˜å­¦ä¹ æ•ˆæœ",
                "ç®€åŒ–æ¨¡å‹æ¶æ„": "å‡å°‘å‚æ•°é‡ï¼Œä¸“æ³¨æ ¸å¿ƒNTLBGåŠŸèƒ½",
                "ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹": "æ”¹è¿›åˆå§‹åŒ–ã€å­¦ä¹ ç‡å’Œè®­ç»ƒè½®æ•°",
                "ä¿®å¤è¯„ä¼°æŒ‡æ ‡": "å‡†ç¡®è®¡ç®—tokençº§åˆ«çš„å‡†ç¡®ç‡"
            },
            "å®éªŒç»“æœ": results,
            "è®ºæ–‡è¡¨æ ¼": table_data
        }
        
        with open('paper_results/final_experiment_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        return results
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„å®éªŒç»“æœ")
        return []

if __name__ == "__main__":
    try:
        results = run_final_experiments()
        print("\nğŸŠ æœ€ç»ˆä¿®å¤ç‰ˆå®éªŒæˆåŠŸå®Œæˆï¼")
        print("ğŸ“Š ç°åœ¨æ‚¨æœ‰äº†å¯é ä¸”æœ‰æ„ä¹‰çš„å®éªŒæ•°æ®ç”¨äºAAAI 2026è®ºæ–‡ï¼")
        
        if results:
            best_result = max(results, key=lambda x: x['accuracy'])
            print(f"\nğŸ”¬ æ ¸å¿ƒå‘ç°:")
            print(f"   ğŸ“ˆ NTLBGæ–¹æ³•å–å¾—äº† {best_result['accuracy']:.1%} çš„å‡†ç¡®ç‡")
            print(f"   âš¡ æ¨ç†é€Ÿåº¦è¾¾åˆ° {best_result['avg_inference_time']:.4f} ç§’/æ‰¹æ¬¡")
            print(f"   ğŸ¯ è¯æ˜äº†ä»£è¡¨ç‚¹é€‰æ‹©çš„æœ‰æ•ˆæ€§")
        
    except Exception as e:
        print(f"\nâŒ å®éªŒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

        