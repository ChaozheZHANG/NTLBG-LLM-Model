import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import os
import yaml
import json
from tqdm import tqdm

class SimpleVideoQADataset(Dataset):
    def __init__(self, data_dirs, max_samples=1000):
        self.samples = []
        
        # æ‰«ææ•°æ®é›†
        for data_dir in data_dirs:
            if os.path.exists(data_dir):
                # æŸ¥æ‰¾JSONæ–‡ä»¶
                for root, dirs, files in os.walk(data_dir):
                    for file in files:
                        if file.endswith('.json') and len(self.samples) < max_samples:
                            try:
                                with open(os.path.join(root, file), 'r') as f:
                                    data = json.load(f)
                                    if isinstance(data, list):
                                        self.samples.extend(data[:10])  # æ¯ä¸ªæ–‡ä»¶å–10ä¸ªæ ·æœ¬
                                    elif isinstance(data, dict):
                                        self.samples.append(data)
                            except:
                                continue
        
        print(f"âœ… åŠ è½½äº† {len(self.samples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        # æ¨¡æ‹Ÿæ•°æ®
        return {
            'video_features': torch.randn(64, 768),  # 64å¸§ï¼Œ768ç»´ç‰¹å¾
            'input_ids': torch.randint(1, 1000, (128,)),  # æ–‡æœ¬token
            'attention_mask': torch.ones(128),
            'labels': torch.randint(1, 1000, (128,))
        }

class SimpleNTLBGModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.video_encoder = nn.Linear(768, 512)
        self.frame_selector = nn.Linear(512, 1)  # NTLBGé€‰æ‹©å™¨
        self.text_encoder = nn.Embedding(1000, 512)
        self.fusion = nn.MultiheadAttention(512, 8, batch_first=True)
        self.classifier = nn.Linear(512, 1000)
        
    def forward(self, video_features, input_ids, attention_mask, labels=None):
        # è§†é¢‘ç¼–ç 
        video_encoded = torch.relu(self.video_encoder(video_features))  # [B, T, 512]
        
        # NTLBGä»£è¡¨ç‚¹é€‰æ‹©ï¼ˆç®€åŒ–ç‰ˆï¼‰
        frame_scores = self.frame_selector(video_encoded).squeeze(-1)  # [B, T]
        _, top_indices = torch.topk(frame_scores, k=6, dim=1)  # é€‰æ‹©6ä¸ªä»£è¡¨ç‚¹
        
        # æ”¶é›†ä»£è¡¨ç‚¹
        batch_size = video_features.size(0)
        batch_indices = torch.arange(batch_size).unsqueeze(1).expand(-1, 6)
        representatives = video_encoded[batch_indices, top_indices]  # [B, 6, 512]
        
        # æ–‡æœ¬ç¼–ç 
        text_encoded = self.text_encoder(input_ids)  # [B, L, 512]
        
        # å¤šæ¨¡æ€èåˆ
        fused, _ = self.fusion(text_encoded, representatives, representatives)
        
        # åˆ†ç±»
        logits = self.classifier(fused)  # [B, L, 1000]
        
        outputs = {'logits': logits, 'representatives': representatives}
        
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(logits.view(-1, 1000), labels.view(-1))
            outputs['loss'] = loss
            
        return outputs

def train():
    print("ğŸš€ å¼€å§‹NTLBG-LLMè®­ç»ƒ")
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ•°æ®é›†
    data_dirs = ['data/longvideobench', 'data/video_mme', 'data/mlvu']
    dataset = SimpleVideoQADataset(data_dirs)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
    
    # æ¨¡å‹
    model = SimpleNTLBGModel().to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
    
    print(f"ğŸ¤– æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M")
    
    # è®­ç»ƒå¾ªç¯
    model.train()
    for epoch in range(3):  # 3ä¸ªepochçš„æ¼”ç¤º
        epoch_loss = 0
        num_batches = 0
        
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/3")
        
        for batch in progress_bar:
            # ç§»åŠ¨åˆ°è®¾å¤‡
            batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            outputs = model(**batch)
            loss = outputs['loss']
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            # è®°å½•
            epoch_loss += loss.item()
            num_batches += 1
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'loss': f"{loss.item():.4f}",
                'reps': outputs['representatives'].shape[1]
            })
        
        avg_loss = epoch_loss / num_batches
        print(f"âœ… Epoch {epoch+1} å®Œæˆï¼Œå¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        os.makedirs('outputs/checkpoints', exist_ok=True)
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': avg_loss
        }, f'outputs/checkpoints/checkpoint_epoch_{epoch+1}.pt')
    
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("ğŸ“Š æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ° outputs/checkpoints/")

if __name__ == "__main__":
    train()
